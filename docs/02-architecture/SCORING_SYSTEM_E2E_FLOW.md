# Scoring System End-to-End Flow

**Complete data flow from generation â†’ evaluation â†’ scoring â†’ storage â†’ learning**

Last Updated: November 16, 2025

---

## ğŸ¯ Overview

The scoring system captures comprehensive quality metrics throughout the generation pipeline, stores them in a linked database, and uses them for continuous learning and parameter optimization.

**Three Quality Dimensions**:
1. **Grok humanness Score** (0-100) - AI detection avoidance
2. **Subjective Score** (0-10) - Human-like quality assessment  
3. **Readability Score** (0-100) - Text clarity and flow

**Composite Quality Score** = Grok (60%) + Subjective (30%) + Readability (10%)

---

## ğŸ“Š Complete Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    1. PRE-GENERATION PHASE                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

UnifiedOrchestrator.generate()
    â†“
_get_adaptive_parameters()
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Query: Best Previous Parameters      â”‚
â”‚                                      â”‚
â”‚ SELECT p.* FROM generation_parametersâ”‚
â”‚ JOIN detection_results r             â”‚
â”‚ WHERE material = ? AND component = ? â”‚
â”‚ AND r.success = 1                    â”‚
â”‚ ORDER BY r.human_score DESC          â”‚
â”‚ LIMIT 1                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Parameters Retrieved:
  â€¢ temperature
  â€¢ frequency_penalty
  â€¢ presence_penalty
  â€¢ voice_params (11 parameters)
  â€¢ enrichment_params (3 parameters)
    â†“
Apply to generation config
    â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    2. GENERATION PHASE                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Build prompt with enrichment
    â†“
Call API with parameters
    â†“
Generated text returned
    â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    3. GROK EVALUATION                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

GrokHumannessRuntimeEvaluator.evaluate()
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Grok API Call                     â”‚
â”‚                                      â”‚
â”‚ POST /api/detect                     â”‚
â”‚ Body: { text: generated_text }      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Grok Response:
  {
    "ai_score": 0.15,        # 0-1 scale
    "human_score": 0.85,     # 0-1 scale (inverted)
    "readability_score": 72, # 0-100 scale
    "sentences": [           # Per-sentence analysis
      {"text": "...", "score": 0.92},
      {"text": "...", "score": 0.78}
    ]
  }
    â†“
Convert to 0-100 scale:
  â€¢ winston_score = ai_score * 100    # 15
  â€¢ human_score = human_score * 100   # 85
  â€¢ readability_score = 72            # Already 0-100
    â†“
WinstonFeedbackDatabase.log_detection()
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INSERT INTO detection_results        â”‚
â”‚                                      â”‚
â”‚ Fields:                              â”‚
â”‚   - material                         â”‚
â”‚   - component_type                   â”‚
â”‚   - generated_text                   â”‚
â”‚   - human_score (0-1)                â”‚
â”‚   - ai_score (0-1)                   â”‚
â”‚   - readability_score (0-100)        â”‚
â”‚   - composite_quality_score (NULL)   â”‚ â† NOT YET CALCULATED
â”‚   - subjective_evaluation_id (NULL)  â”‚ â† NOT YET LINKED
â”‚   - success (boolean)                â”‚
â”‚   - temperature                      â”‚
â”‚   - attempt_number                   â”‚
â”‚   - timestamp                        â”‚
â”‚                                      â”‚
â”‚ RETURNS: detection_id                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
detection_id = 42
    â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    4. PARAMETER LOGGING                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

WinstonFeedbackDatabase.log_parameters()
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INSERT INTO generation_parameters    â”‚
â”‚                                      â”‚
â”‚ Fields (20+ parameters):             â”‚
â”‚   - detection_result_id (FK)         â”‚ â† Links to detection_results
â”‚   - temperature                      â”‚
â”‚   - frequency_penalty                â”‚
â”‚   - presence_penalty                 â”‚
â”‚   - trait_frequency                  â”‚
â”‚   - opinion_rate                     â”‚
â”‚   - reader_address_rate              â”‚
â”‚   - colloquialism_frequency          â”‚
â”‚   - structural_predictability        â”‚
â”‚   - emotional_tone                   â”‚
â”‚   - imperfection_tolerance           â”‚
â”‚   - sentence_rhythm_variation        â”‚
â”‚   - technical_intensity              â”‚
â”‚   - context_detail_level             â”‚
â”‚   - engagement_level                 â”‚
â”‚   - detection_threshold              â”‚
â”‚   - readability_min                  â”‚
â”‚   - readability_max                  â”‚
â”‚   - grammar_strictness               â”‚
â”‚   - confidence_high                  â”‚
â”‚   - confidence_medium                â”‚
â”‚   - param_hash (for deduplication)   â”‚
â”‚   - timestamp                        â”‚
â”‚                                      â”‚
â”‚ RETURNS: param_id                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
param_id = 123
    â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    5. SUBJECTIVE EVALUATION (Optional)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SubjectiveEvaluator.evaluate()
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Claude/Grok API Call                 â”‚
â”‚                                      â”‚
â”‚ Evaluate across 6 dimensions:        â”‚
â”‚   1. Clarity                         â”‚
â”‚   2. Professionalism                 â”‚
â”‚   3. Technical Accuracy              â”‚
â”‚   4. Human-likeness                  â”‚
â”‚   5. Engagement                      â”‚
â”‚   6. Jargon-free                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
SubjectiveEvaluationResult:
  {
    "overall_score": 8.2,              # 0-10 scale
    "dimension_scores": [
      {"dimension": "clarity", "score": 9.0},
      {"dimension": "professionalism", "score": 8.5},
      ...
    ],
    "strengths": ["Clear", "Engaging"],
    "weaknesses": ["Too technical"],
    "recommendations": ["Simplify terms"]
  }
    â†“
WinstonFeedbackDatabase.log_subjective_evaluation()
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INSERT INTO subjective_evaluations   â”‚
â”‚                                      â”‚
â”‚ Fields:                              â”‚
â”‚   - topic (material name)            â”‚
â”‚   - component_type                   â”‚
â”‚   - generated_text                   â”‚
â”‚   - overall_score (0-10)             â”‚
â”‚   - clarity_score                    â”‚
â”‚   - professionalism_score            â”‚
â”‚   - technical_accuracy_score         â”‚
â”‚   - human_likeness_score             â”‚
â”‚   - engagement_score                 â”‚
â”‚   - jargon_free_score                â”‚
â”‚   - passes_quality_gate              â”‚
â”‚   - strengths (JSON)                 â”‚
â”‚   - weaknesses (JSON)                â”‚
â”‚   - recommendations (JSON)           â”‚
â”‚   - generation_parameters_id (FK)    â”‚ â† Links to generation_parameters
â”‚   - timestamp                        â”‚
â”‚                                      â”‚
â”‚ RETURNS: subjective_eval_id          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
subjective_eval_id = 67
    â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    6. COMPOSITE SCORING (NEW)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CompositeScorer.calculate()
    â†“
INPUTS:
  â€¢ winston_score = 85 (from detection_results.human_score * 100)
  â€¢ subjective_score = 8.2 (from subjective_evaluations.overall_score)
  â€¢ readability_score = 72 (from detection_results.readability_score)
    â†“
CALCULATION:
  composite_score = (
      winston_score * 0.6 +           # 85 * 0.6 = 51.0
      subjective_score * 10 * 0.3 +   # 8.2 * 10 * 0.3 = 24.6
      readability_score * 0.1          # 72 * 0.1 = 7.2
  )
  = 51.0 + 24.6 + 7.2 = 82.8
    â†“
composite_score = 82.8 (0-100 scale)
    â†“
WEIGHT REDISTRIBUTION (if dimensions missing):
  If subjective_score is None:
    # Redistribute 30% weight to Grok (60% â†’ 75%)
    composite_score = (
        winston_score * 0.75 +
        readability_score * 0.25
    )
    â†“
UPDATE detection_results:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ UPDATE detection_results             â”‚
â”‚ SET composite_quality_score = 82.8,  â”‚
â”‚     subjective_evaluation_id = 67    â”‚
â”‚ WHERE id = 42                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
UPDATE subjective_evaluations:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ UPDATE subjective_evaluations        â”‚
â”‚ SET generation_parameters_id = 123   â”‚
â”‚ WHERE id = 67                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    7. SWEET SPOT ANALYSIS                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SweetSpotAnalyzer.update_sweet_spot()
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Query Recent Successful Generations  â”‚
â”‚                                      â”‚
â”‚ SELECT AVG(temperature),             â”‚
â”‚        AVG(frequency_penalty),       â”‚
â”‚        AVG(presence_penalty),        â”‚
â”‚        COUNT(*) as sample_count,     â”‚
â”‚        AVG(human_score)              â”‚
â”‚ FROM generation_parameters p         â”‚
â”‚ JOIN detection_results r             â”‚
â”‚   ON p.detection_result_id = r.id   â”‚
â”‚ WHERE r.success = 1                  â”‚
â”‚   AND r.material = '*'               â”‚ â† Global scope
â”‚   AND r.component_type = '*'         â”‚
â”‚   AND r.timestamp > (NOW - 30 days) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Statistical Analysis:
  â€¢ Calculate mean, median, std dev for each parameter
  â€¢ Determine confidence level based on sample size
  â€¢ Identify optimal parameter ranges
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INSERT/UPDATE sweet_spot_recommendations â”‚
â”‚                                      â”‚
â”‚ Fields:                              â”‚
â”‚   - material = '*'                   â”‚ â† Global sweet spot
â”‚   - component_type = '*'             â”‚
â”‚   - optimal_temperature              â”‚
â”‚   - optimal_frequency_penalty        â”‚
â”‚   - optimal_presence_penalty         â”‚
â”‚   - sample_count                     â”‚
â”‚   - confidence_level ('high'/'medium'/'low') â”‚
â”‚   - avg_human_score                  â”‚
â”‚   - std_dev_human_score              â”‚
â”‚   - last_updated                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    8. POST-GENERATION INTEGRITY CHECKS              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

IntegrityChecker.run_post_generation_checks()
    â†“
Check 1: Detection Result Logged?
  âœ… Query detection_results for recent entry
    â†“
Check 2: Parameters Logged?
  âœ… Query generation_parameters with detection_result_id FK
    â†“
Check 3: Sweet Spot Updated?
  âœ… Query sweet_spot_recommendations for global entry
    â†“
Check 4: Subjective Evaluation Logged?
  âš ï¸  Query subjective_evaluations (optional)
    â†“
RESULTS:
  âœ… Detection Logged (ID: 42, human: 85%, AI: 15%)
  âœ… Parameters Logged (ID: 123, temp: 0.8)
  âœ… Sweet Spot Updated (10 samples, high confidence)
  âš ï¸  Subjective Evaluation Logged (ID: 67, score: 8.2/10)
    â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    9. CONTINUOUS LEARNING (Future Use)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

GranularParameterCorrelator.analyze_all_parameters()
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ For Each of 20+ Parameters:         â”‚
â”‚                                      â”‚
â”‚ 1. Calculate Spearman correlation    â”‚
â”‚    between parameter value and       â”‚
â”‚    composite_quality_score           â”‚
â”‚                                      â”‚
â”‚ 2. Determine statistical significanceâ”‚
â”‚    (p-value < 0.05)                  â”‚
â”‚                                      â”‚
â”‚ 3. Calculate confidence interval     â”‚
â”‚    (bootstrap method)                â”‚
â”‚                                      â”‚
â”‚ 4. Detect relationship type          â”‚
â”‚    (linear, polynomial, logarithmic) â”‚
â”‚                                      â”‚
â”‚ 5. Find optimal range                â”‚
â”‚    (where scores consistently high)  â”‚
â”‚                                      â”‚
â”‚ 6. Calculate sensitivity             â”‚
â”‚    (score change per 1% param change)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Results (example):
  {
    "temperature": ParameterCorrelation(
      correlation_coefficient = 0.65,     # Strong positive
      p_value = 0.003,                    # Significant
      strength = "strong",
      direction = "positive",
      sample_count = 50,
      confidence_interval = (0.45, 0.80),
      relationship_type = "linear",
      optimal_range = (0.8, 1.2),
      sensitivity = 1.5  # +1.5 points per 1% increase
    ),
    "frequency_penalty": ParameterCorrelation(
      correlation_coefficient = -0.52,    # Moderate negative
      p_value = 0.012,
      strength = "moderate",
      direction = "negative",
      ...
    )
  }
    â†“
analyze_interactions()
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Detect Parameter Synergies           â”‚
â”‚                                      â”‚
â”‚ For top parameter pairs:             â”‚
â”‚   Calculate interaction_term =       â”‚
â”‚     param1_value * param2_value      â”‚
â”‚                                      â”‚
â”‚   Correlate with composite_score     â”‚
â”‚                                      â”‚
â”‚   Identify synergistic combinations  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Results (example):
  [
    ParameterInteraction(
      parameters = ["temperature", "trait_frequency"],
      interaction_strength = 0.48,
      combined_effect = 0.15,  # Synergy effect
      optimal_combination = {
        "temperature": 1.1,
        "trait_frequency": 0.7
      },
      sample_count = 45
    )
  ]
    â†“
generate_adjustment_recommendations()
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Calculate Optimal Adjustments        â”‚
â”‚                                      â”‚
â”‚ For each parameter:                  â”‚
â”‚   current_value = get_current()      â”‚
â”‚   optimal_value = calculate_from(    â”‚
â”‚     correlation,                     â”‚
â”‚     sensitivity,                     â”‚
â”‚     optimal_range,                   â”‚
â”‚     target_improvement = 5.0         â”‚
â”‚   )                                  â”‚
â”‚                                      â”‚
â”‚   expected_impact =                  â”‚
â”‚     abs(optimal - current) *         â”‚
â”‚     sensitivity                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Recommendations (example):
  [
    {
      "parameter": "temperature",
      "current_value": 0.8,
      "recommended_value": 1.0,
      "change": +0.2,
      "expected_impact": +3.0,  # Points improvement
      "confidence": 0.997,      # 1 - p_value
      "correlation_strength": "strong",
      "optimal_range": (0.8, 1.2),
      "reasoning": "Increase temperature (strong +0.65 correlation). Expected: 3.0 points per 1% change."
    },
    {
      "parameter": "frequency_penalty",
      "current_value": 0.3,
      "recommended_value": 0.2,
      "change": -0.1,
      "expected_impact": +2.5,
      "confidence": 0.988,
      "correlation_strength": "moderate",
      "reasoning": "Decrease frequency_penalty (moderate -0.52 correlation)..."
    }
  ]
    â†“
Apply recommendations in next generation cycle
    â†“
```

---

## ğŸ—„ï¸ Database Schema Relationships

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   detection_results              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  PK: id                          â”‚
â”‚  material                        â”‚
â”‚  component_type                  â”‚
â”‚  generated_text                  â”‚
â”‚  human_score (0-1)               â”‚
â”‚  ai_score (0-1)                  â”‚
â”‚  readability_score (0-100)       â”‚
â”‚  composite_quality_score (0-100) â”‚ â† NEW: Unified metric
â”‚  subjective_evaluation_id (FK)   â”‚ â† NEW: Links to subjective eval
â”‚  success                         â”‚
â”‚  temperature                     â”‚
â”‚  attempt_number                  â”‚
â”‚  timestamp                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†‘                 â†“
          â”‚                 â”‚
          â”‚ FK              â”‚ FK
          â”‚                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ generation_parameters  â”‚  â””â”€â”€â”‚ subjective_evaluations           â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ PK: id                 â”‚     â”‚ PK: id                           â”‚
â”‚ detection_result_id â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”‚ generation_parameters_id (FK) â”€â”€â”€â”¼â”€â”
â”‚ temperature            â”‚     â”‚ topic                            â”‚ â”‚
â”‚ frequency_penalty      â”‚     â”‚ component_type                   â”‚ â”‚
â”‚ presence_penalty       â”‚     â”‚ generated_text                   â”‚ â”‚
â”‚ trait_frequency        â”‚     â”‚ overall_score (0-10)             â”‚ â”‚
â”‚ opinion_rate           â”‚     â”‚ clarity_score                    â”‚ â”‚
â”‚ ... (20+ parameters)   â”‚     â”‚ professionalism_score            â”‚ â”‚
â”‚ param_hash             â”‚     â”‚ technical_accuracy_score         â”‚ â”‚
â”‚ timestamp              â”‚     â”‚ human_likeness_score             â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ engagement_score                 â”‚ â”‚
                               â”‚ jargon_free_score                â”‚ â”‚
                               â”‚ passes_quality_gate              â”‚ â”‚
                               â”‚ strengths (JSON)                 â”‚ â”‚
                               â”‚ weaknesses (JSON)                â”‚ â”‚
                               â”‚ recommendations (JSON)           â”‚ â”‚
                               â”‚ timestamp                        â”‚ â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                                                                     â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ sweet_spot_recommendations       â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ PK: (material, component_type)   â”‚
â”‚ material = '*'                   â”‚ â† Global scope
â”‚ component_type = '*'             â”‚
â”‚ optimal_temperature              â”‚
â”‚ optimal_frequency_penalty        â”‚
â”‚ optimal_presence_penalty         â”‚
â”‚ sample_count                     â”‚
â”‚ confidence_level                 â”‚
â”‚ avg_human_score                  â”‚
â”‚ std_dev_human_score              â”‚
â”‚ last_updated                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¥ Inputs and Outputs

### 1. UnifiedOrchestrator.generate()
**Inputs**:
- `identifier`: Material name (e.g., "Aluminum")
- `component_type`: Component type (e.g., "micro")
- `api_client`: Grok API client

**Outputs**:
```python
{
    'success': True,
    'content': "Generated micro text",
    'text': "Full generated text",
    'attempts': 1,
    'ai_score': 0.15,        # 0-1 scale
    'human_score': 0.85,     # 0-1 scale
    'readability': {
        'status': 'readable',
        'is_readable': True
    }
}
```

### 2. WinstonIntegration.detect_and_log()
**Inputs**:
- `text`: Generated content
- `material`: Material name
- `component_type`: Component type
- `temperature`: Generation temperature
- `attempt`: Attempt number
- `max_attempts`: Maximum attempts
- `ai_threshold`: Success threshold

**Outputs**:
```python
{
    'ai_score': 0.15,              # 0-1 scale
    'detection': {                 # Full Grok response
        'ai_score': 0.15,
        'human_score': 0.85,
        'readability_score': 72,
        'sentences': [...]
    },
    'detection_id': 42,            # Database ID
    'failure_analysis': {...},     # If failed
    'method': 'grok'            # Or 'pattern_only'
}
```

### 3. WinstonFeedbackDatabase.log_detection()
**Inputs**:
- `material`: Material name
- `component_type`: Component type
- `generated_text`: Full text
- `winston_result`: Grok API response
- `temperature`: Temperature used
- `attempt`: Attempt number
- `success`: Boolean
- `failure_analysis`: Optional analysis

**Outputs**:
- Returns `detection_id` (integer)

### 4. WinstonFeedbackDatabase.log_parameters() *(Not shown in current flow - needs integration)*
**Inputs**:
- `detection_result_id`: FK to detection_results
- All 20+ generation parameters

**Outputs**:
- Returns `param_id` (integer)

### 5. SubjectiveEvaluator.evaluate()
**Inputs**:
- `content`: Generated text
- `material_name`: Material name
- `component_type`: Component type
- `context`: Optional additional context

**Outputs**:
```python
SubjectiveEvaluationResult(
    overall_score = 8.2,           # 0-10
    dimension_scores = [
        SubjectiveScore(
            dimension = EvaluationDimension.CLARITY,
            score = 9.0,
            feedback = "Very clear",
            suggestions = [...]
        ),
        ...
    ],
    strengths = ["Clear", "Engaging"],
    weaknesses = ["Too technical"],
    recommendations = ["Simplify"],
    passes_quality_gate = True,
    evaluation_time_ms = 1500,
    raw_response = "..."
)
```

### 6. WinstonFeedbackDatabase.log_subjective_evaluation()
**Inputs**:
- `topic`: Material name
- `component_type`: Component type
- `generated_text`: Full text
- `evaluation_result`: SubjectiveEvaluationResult object
- `domain`: "materials"
- `author_id`: Optional
- `attempt_number`: Optional

**Outputs**:
- Returns `subjective_eval_id` (integer)

### 7. CompositeScorer.calculate()
**Inputs**:
- `winston_score`: 0-100 (from human_score * 100)
- `subjective_score`: 0-10 (from overall_score)
- `readability_score`: 0-100

**Outputs**:
- Returns `composite_score` (0-100 float)

### 8. GranularParameterCorrelator.analyze_all_parameters()
**Inputs**:
- `target_metric`: 'composite_quality_score'
- `min_score`: Minimum score threshold (e.g., 50.0)

**Outputs**:
```python
{
    "temperature": ParameterCorrelation(
        parameter_name = "temperature",
        correlation_coefficient = 0.65,
        p_value = 0.003,
        strength = "strong",
        direction = "positive",
        sample_count = 50,
        confidence_interval = (0.45, 0.80),
        relationship_type = "linear",
        optimal_range = (0.8, 1.2),
        sensitivity = 1.5
    ),
    ...
}
```

### 9. GranularParameterCorrelator.generate_adjustment_recommendations()
**Inputs**:
- `correlations`: Dict of ParameterCorrelation objects
- `current_params`: Dict of current parameter values
- `target_improvement`: Target score increase (e.g., 5.0)

**Outputs**:
```python
[
    {
        'parameter': 'temperature',
        'current_value': 0.8,
        'recommended_value': 1.0,
        'change': +0.2,
        'expected_impact': +3.0,
        'confidence': 0.997,
        'correlation_strength': 'strong',
        'optimal_range': (0.8, 1.2),
        'reasoning': "Increase temperature..."
    },
    ...
]
```

---

## ğŸ”„ Current Integration Status

### âœ… **IMPLEMENTED**
1. **Grok Detection** - Full integration with database logging
2. **Parameter Logging** - All 20+ parameters stored per generation
3. **Sweet Spot Analysis** - Global parameter optimization
4. **Subjective Evaluation** - Human-like quality assessment
5. **CompositeScorer** - Unified quality metric calculation
6. **GranularParameterCorrelator** - Fine-grained parameter analysis
7. **Database Schema** - Foreign keys linking all tables
8. **Post-Generation Checks** - Integrity verification

### â³ **PENDING INTEGRATION**
1. **Automatic Composite Score Calculation** - Need to call `CompositeScorer.calculate()` after detection + subjective eval
2. **Automatic Foreign Key Updates** - Need to link `subjective_evaluation_id` in detection_results
3. **Parameter Correlation in Pipeline** - Need to integrate `GranularParameterCorrelator` into orchestrator
4. **Automated Recommendations** - Need to apply correlation findings to parameter selection

---

## ğŸš€ Next Steps for Full Integration

### Phase 2A: Integrate Composite Scoring into Pipeline

**File**: `generation/core/evaluated_generator.py`

```python
# After Grok detection (line ~360)
if readability['is_readable']:
    # Calculate composite score if subjective eval available
    from postprocessing.evaluation import CompositeScorer
    
    composite_scorer = CompositeScorer()
    
    # Get subjective score if available
    subjective_score = None
    if self.config.get('enable_subjective_evaluation', False):
        from postprocessing.evaluation import SubjectiveEvaluator
        evaluator = SubjectiveEvaluator(api_client=self.api_client)
        
        subjective_result = evaluator.evaluate(
            content=text,
            material_name=identifier,
            component_type=component_type
        )
        
        # Log subjective evaluation
        subjective_eval_id = self.grok.feedback_db.log_subjective_evaluation(
            topic=identifier,
            component_type=component_type,
            generated_text=text,
            evaluation_result=subjective_result,
            domain="materials"
        )
        
        subjective_score = subjective_result.overall_score
    else:
        subjective_eval_id = None
    
    # Calculate composite score
    composite_score = composite_scorer.calculate(
        winston_score=human_score * 100,  # Convert 0-1 to 0-100
        subjective_score=subjective_score,
        readability_score=readability['score']
    )
    
    # Update detection result with composite score and subjective link
    self.grok.feedback_db.update_detection_composite(
        detection_id=detection_result['detection_id'],
        composite_quality_score=composite_score,
        subjective_evaluation_id=subjective_eval_id
    )
    
    logger.info(f"ğŸ“Š Composite quality score: {composite_score:.1f}/100")
```

### Phase 2B: Add Database Update Methods

**File**: `postprocessing/detection/winston_feedback_db.py`

```python
def update_detection_composite(
    self,
    detection_id: int,
    composite_quality_score: float,
    subjective_evaluation_id: Optional[int] = None
) -> None:
    """Update detection result with composite score and subjective eval link."""
    with sqlite3.connect(self.db_path) as conn:
        cursor = conn.cursor()
        
        cursor.execute("""
            UPDATE detection_results
            SET composite_quality_score = ?,
                subjective_evaluation_id = ?
            WHERE id = ?
        """, (composite_quality_score, subjective_evaluation_id, detection_id))
        
        conn.commit()
    
    logger.info(f"ğŸ“Š Updated detection #{detection_id} with composite score {composite_quality_score:.1f}")
```

### Phase 3: Integrate Correlation Analysis

Add periodic correlation analysis to optimize parameters:

```python
# In UnifiedOrchestrator or separate script
def analyze_and_optimize_parameters(self):
    """Run correlation analysis and generate recommendations."""
    from learning.validation_winston_correlator import ValidationWinstonCorrelator
    
    correlator = GranularParameterCorrelator(
        db_path='data/z-beam.db',
        min_samples=30,
        significance_level=0.05
    )
    
    # Analyze all parameters
    correlations = correlator.analyze_all_parameters(
        target_metric='composite_quality_score',
        min_score=60.0
    )
    
    # Get current parameters
    current_params = self.dynamic_config.get_all_generation_params('micro')
    
    # Generate recommendations
    recommendations = correlator.generate_adjustment_recommendations(
        correlations,
        current_params,
        target_improvement=5.0
    )
    
    # Log recommendations
    for rec in recommendations[:5]:  # Top 5
        logger.info(
            f"ğŸ’¡ {rec['parameter']}: {rec['current_value']} â†’ {rec['recommended_value']} "
            f"(expected: +{rec['expected_impact']:.1f} points)"
        )
    
    return recommendations
```

---

## ğŸ“Š Key Metrics Tracked

| Metric | Scale | Source | Usage |
|--------|-------|--------|-------|
| **Grok humanness Score** | 0-100 | Grok API (ai_score * 100) | AI detection avoidance |
| **Grok Human Score** | 0-100 | Grok API (human_score * 100) | Human-like quality |
| **Readability Score** | 0-100 | Grok API | Text clarity |
| **Subjective Overall** | 0-10 | SubjectiveEvaluator | Comprehensive quality |
| **Composite Score** | 0-100 | CompositeScorer | Unified optimization target |
| **Parameter Correlations** | -1 to +1 | GranularParameterCorrelator | Parameter tuning |

---

## ğŸ¯ Benefits of Complete Integration

1. **Holistic Optimization** - Not just AI detection, but overall quality
2. **Precise Tuning** - 0.01 increment adjustments based on statistical analysis
3. **Relationship Discovery** - Non-linear patterns and parameter synergies
4. **Statistical Confidence** - P-values and confidence intervals prevent false signals
5. **Actionable Recommendations** - Automated suggestions with expected impact
6. **Full Traceability** - Every quality score links to exact parameters used
7. **Continuous Learning** - System improves with every generation

---

**Last Updated**: November 16, 2025  
**Status**: Phase 1-2 Complete (Database + Scoring), Integration Pending
