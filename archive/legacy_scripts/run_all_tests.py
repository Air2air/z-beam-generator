#!/usr/bin/env python3
"""
Comprehensive Test Runner for Z-Beam Service Architecture

This script runs all tests for the Z-Beam service architecture and provides
detailed reporting on test coverage, performance, and results.
"""

import json
import os
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List


class TestRunner:
    """Comprehensive test runner for the Z-Beam service architecture."""

    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.test_results = {}
        self.start_time = None
        self.end_time = None

    def run_all_tests(self) -> Dict[str, Any]:
        """Run all test suites and return comprehensive results."""
        self.start_time = datetime.now()

        print("üöÄ Starting Z-Beam Service Architecture Test Suite")
        print("=" * 60)

        # Test suites to run
        test_suites = [
            {
                "name": "AI Detection Optimization Tests",
                "path": "services/ai_detection_optimization/test_ai_detection_optimization.py",
                "description": "Tests for AI detection scoring and optimization",
            },
            {
                "name": "Iterative Workflow Tests",
                "path": "services/iterative_workflow/test_iterative_workflow.py",
                "description": "Tests for iterative content improvement workflows",
            },
            {
                "name": "Dynamic Evolution Tests",
                "path": "services/dynamic_evolution/test_dynamic_evolution.py",
                "description": "Tests for dynamic evolution and A/B testing",
            },
            {
                "name": "Quality Assessment Tests",
                "path": "services/quality_assessment/test_quality_assessment.py",
                "description": "Tests for content quality assessment and benchmarking",
            },
            {
                "name": "Configuration Optimization Tests",
                "path": "services/configuration_optimizer/test_configuration_optimization.py",
                "description": "Tests for configuration optimization and parameter tuning",
            },
            {
                "name": "Service Integration Tests",
                "path": "tests/test_service_integration.py",
                "description": "Integration tests for complete service architecture",
            },
        ]

        all_results = {}

        for suite in test_suites:
            print(f"\nüìã Running {suite['name']}")
            print(f"   {suite['description']}")
            print("-" * 40)

            result = self._run_test_suite(suite)
            all_results[suite["name"]] = result

            # Print immediate results
            self._print_suite_results(suite["name"], result)

        self.end_time = datetime.now()

        # Generate comprehensive report
        final_report = self._generate_final_report(all_results)

        return final_report

    def _run_test_suite(self, suite: Dict[str, str]) -> Dict[str, Any]:
        """Run a specific test suite and return results."""
        try:
            # Change to project root
            os.chdir(self.project_root)

            # Prepare pytest command
            if suite["path"].endswith("*.py"):
                # Run all test files matching pattern
                cmd = [
                    "python3",
                    "-m",
                    "pytest",
                    suite["path"],
                    "-v",
                    "--tb=short",
                    "--json-report",
                    "--json-report-file=/tmp/test_results.json",
                ]
            else:
                # Run specific test file
                test_file = self.project_root / suite["path"]
                print(f"DEBUG: Looking for test file: {test_file}")
                print(f"DEBUG: File exists: {test_file.exists()}")
                if not test_file.exists():
                    return {
                        "status": "error",
                        "error": f"Test file not found: {test_file}",
                        "passed": 0,
                        "failed": 0,
                        "total": 0,
                        "duration": 0,
                    }

                cmd = [
                    "python3",
                    "-m",
                    "pytest",
                    str(test_file),
                    "-v",
                    "--tb=short",
                    "--json-report",
                    "--json-report-file=/tmp/test_results.json",
                ]

            # Run the test
            start_time = time.time()
            print(f"DEBUG: Running command: {' '.join(cmd)}")
            result = subprocess.run(
                cmd, capture_output=True, text=True, timeout=300  # 5 minute timeout
            )
            end_time = time.time()

            duration = end_time - start_time
            print(f"DEBUG: Command completed with return code: {result.returncode}")
            print(f"DEBUG: STDOUT length: {len(result.stdout)}")
            print(f"DEBUG: STDERR length: {len(result.stderr)}")
            if result.stderr:
                print(
                    f"DEBUG: STDERR content: {result.stderr[:500]}"
                )  # Print first 500 chars of stderr

            # Try to parse JSON results
            json_results = {}
            json_file = Path("/tmp/test_results.json")
            if json_file.exists():
                try:
                    with open(json_file, "r") as f:
                        json_results = json.load(f)
                    print(f"DEBUG: JSON results loaded: {len(json_results)} keys")
                except Exception as e:
                    print(f"DEBUG: Failed to parse JSON: {e}")
                finally:
                    json_file.unlink(missing_ok=True)
            else:
                print("DEBUG: JSON report file not found")

            # Parse results
            passed = 0
            failed = 0
            total = 0

            if json_results and "tests" in json_results:
                tests = json_results["tests"]
                total = len(tests)
                for test in tests:
                    outcome = test.get("outcome", "unknown")
                    if outcome == "passed":
                        passed += 1
                    elif outcome in ["failed", "error"]:
                        failed += 1
                # Note: skipped tests are not counted as failed
            else:
                # Fallback: parse from stdout
                stdout_lines = result.stdout.split("\n")
                for line in stdout_lines:
                    if "passed" in line and "failed" in line:
                        # Try to extract numbers
                        try:
                            parts = line.split(",")
                            for part in parts:
                                part = part.strip()
                                if part.endswith("passed"):
                                    passed = int(part.split()[0])
                                elif part.endswith("failed"):
                                    failed = int(part.split()[0])
                        except:
                            pass
                        break

                total = passed + failed

            # Determine status based on whether we have results
            if total > 0:
                status = "completed"  # We have test results
            elif result.returncode == 0:
                status = "completed"  # No tests but successful
            else:
                status = "failed"  # No results and failed

            return {
                "status": status,
                "passed": passed,
                "failed": failed,
                "total": total,
                "duration": duration,
                "return_code": result.returncode,
                "stdout": result.stdout[-1000:]
                if len(result.stdout) > 1000
                else result.stdout,
                "stderr": result.stderr[-1000:]
                if len(result.stderr) > 1000
                else result.stderr,
            }

        except subprocess.TimeoutExpired:
            return {
                "status": "timeout",
                "error": "Test suite timed out after 5 minutes",
                "passed": 0,
                "failed": 0,
                "total": 0,
                "duration": 300,
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "passed": 0,
                "failed": 0,
                "total": 0,
                "duration": 0,
            }

    def _print_suite_results(self, suite_name: str, result: Dict[str, Any]):
        """Print results for a test suite."""
        status = result["status"]
        passed = result["passed"]
        failed = result["failed"]
        total = result["total"]
        duration = result["duration"]

        if status == "completed":
            if failed == 0:
                print(f"‚úÖ PASSED: {passed}/{total} tests passed in {duration:.2f}s")
            else:
                print(
                    f"‚ùå FAILED: {passed}/{total} passed, {failed} failed in {duration:.2f}s"
                )
        elif status == "timeout":
            print(f"‚è∞ TIMEOUT: Test suite timed out after {duration:.2f}s")
        else:
            print(f"üí• ERROR: {result.get('error', 'Unknown error')}")

    def _generate_final_report(
        self, all_results: Dict[str, Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Generate comprehensive final test report."""
        total_passed = 0
        total_failed = 0
        total_tests = 0
        total_duration = 0
        suite_count = len(all_results)
        successful_suites = 0
        failed_suites = 0

        suite_details = []

        for suite_name, result in all_results.items():
            passed = result["passed"]
            failed = result["failed"]
            total = result["total"]
            duration = result["duration"]
            status = result["status"]

            total_passed += passed
            total_failed += failed
            total_tests += total
            total_duration += duration

            suite_detail = {
                "name": suite_name,
                "status": status,
                "passed": passed,
                "failed": failed,
                "total": total,
                "duration": duration,
                "success_rate": (passed / total * 100) if total > 0 else 0,
            }
            suite_details.append(suite_detail)

            if status == "completed" and failed == 0:
                successful_suites += 1
            elif status in ["failed", "timeout", "error"]:
                failed_suites += 1

        # Calculate overall metrics
        overall_success_rate = (
            (total_passed / total_tests * 100) if total_tests > 0 else 0
        )
        total_runtime = (self.end_time - self.start_time).total_seconds()

        report = {
            "test_run": {
                "timestamp": self.start_time.isoformat(),
                "duration": total_runtime,
                "total_suites": suite_count,
                "successful_suites": successful_suites,
                "failed_suites": failed_suites,
            },
            "test_results": {
                "total_tests": total_tests,
                "passed": total_passed,
                "failed": total_failed,
                "success_rate": overall_success_rate,
                "total_duration": total_duration,
            },
            "suite_details": suite_details,
            "coverage": {
                "services_covered": 5,  # AI, Workflow, Evolution, Quality, Configuration
                "integration_tests": True,
                "unit_tests": True,
                "performance_tests": True,
                "error_handling_tests": True,
            },
            "recommendations": self._generate_recommendations(all_results),
        }

        return report

    def _generate_recommendations(
        self, all_results: Dict[str, Dict[str, Any]]
    ) -> List[str]:
        """Generate recommendations based on test results."""
        recommendations = []

        failed_suites = []
        slow_suites = []

        for suite_name, result in all_results.items():
            if result["status"] in ["failed", "timeout", "error"]:
                failed_suites.append(suite_name)
            elif result["duration"] > 60:  # More than 1 minute
                slow_suites.append(suite_name)

        if failed_suites:
            recommendations.append(
                f"Fix failing test suites: {', '.join(failed_suites)}"
            )

        if slow_suites:
            recommendations.append(
                f"Optimize slow test suites: {', '.join(slow_suites)}"
            )

        total_passed = sum(r["passed"] for r in all_results.values())
        total_tests = sum(r["total"] for r in all_results.values())

        if total_tests > 0 and (total_passed / total_tests) < 0.95:
            recommendations.append("Improve overall test success rate above 95%")

        if len(all_results) < 6:
            recommendations.append(
                "Add more comprehensive test coverage for all services"
            )

        return recommendations

    def print_final_report(self, report: Dict[str, Any]):
        """Print the final comprehensive test report."""
        print("\n" + "=" * 80)
        print("üìä Z-BEAM SERVICE ARCHITECTURE TEST REPORT")
        print("=" * 80)

        # Test run summary
        test_run = report["test_run"]
        print("\nüèÉ Test Run Summary:")
        print(f"   Timestamp: {test_run['timestamp']}")
        print(f"   Duration: {test_run['duration']:.2f} seconds")
        print(f"   Total Suites: {test_run['total_suites']}")
        print(f"   Successful: {test_run['successful_suites']}")
        print(f"   Failed: {test_run['failed_suites']}")

        # Test results
        results = report["test_results"]
        print("\nüìà Test Results:")
        print(f"   Total Tests: {results['total_tests']}")
        print(f"   Passed: {results['passed']}")
        print(f"   Failed: {results['failed']}")
        print(f"   Success Rate: {results['success_rate']:.1f}%")
        print(f"   Total Duration: {results['total_duration']:.2f}s")

        # Coverage
        coverage = report["coverage"]
        print("\nüéØ Test Coverage:")
        print(f"   Services Covered: {coverage['services_covered']}/5")
        print(f"   Integration Tests: {'‚úÖ' if coverage['integration_tests'] else '‚ùå'}")
        print(f"   Unit Tests: {'‚úÖ' if coverage['unit_tests'] else '‚ùå'}")
        print(f"   Performance Tests: {'‚úÖ' if coverage['performance_tests'] else '‚ùå'}")
        print(
            f"   Error Handling Tests: {'‚úÖ' if coverage['error_handling_tests'] else '‚ùå'}"
        )

        # Suite details
        print("\nüìã Suite Details:")
        for suite in report["suite_details"]:
            status_icon = (
                "‚úÖ" if suite["status"] == "completed" and suite["failed"] == 0 else "‚ùå"
            )
            print(
                f"   {status_icon} {suite['name']}: {suite['passed']}/{suite['total']} "
                f"({suite['success_rate']:.1f}%) in {suite['duration']:.2f}s"
            )

        # Recommendations
        recommendations = report["recommendations"]
        if recommendations:
            print("\nüí° Recommendations:")
            for rec in recommendations:
                print(f"   ‚Ä¢ {rec}")

        # Final status
        success_rate = results["success_rate"]
        if success_rate >= 95:
            print("\nüéâ OVERALL STATUS: EXCELLENT (95%+ success rate)")
        elif success_rate >= 80:
            print("\nüëç OVERALL STATUS: GOOD (80-95% success rate)")
        else:
            print("\n‚ö†Ô∏è  OVERALL STATUS: NEEDS IMPROVEMENT (<80% success rate)")
        print("=" * 80)


def main():
    """Main entry point for the test runner."""
    # Get project root - ensure we're in the z-beam-generator directory
    script_dir = Path(__file__).parent
    if script_dir.name == "z-beam-generator":
        project_root = script_dir
    else:
        project_root = script_dir.parent
        # If parent is not z-beam-generator, look for it
        if project_root.name != "z-beam-generator":
            for parent in script_dir.parents:
                if parent.name == "z-beam-generator":
                    project_root = parent
                    break

    print(f"DEBUG: Script dir: {script_dir}")
    print(f"DEBUG: Project root: {project_root}")

    # Initialize test runner
    runner = TestRunner(str(project_root))

    try:
        # Run all tests
        report = runner.run_all_tests()

        # Print final report
        runner.print_final_report(report)

        # Save report to file
        report_file = project_root / "test_report.json"
        with open(report_file, "w") as f:
            json.dump(report, f, indent=2, default=str)

        print(f"\nüìÑ Detailed report saved to: {report_file}")

        # Exit with appropriate code
        success_rate = report["test_results"]["success_rate"]
        if success_rate >= 80:
            sys.exit(0)  # Success
        else:
            sys.exit(1)  # Failure

    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Test run interrupted by user")
        sys.exit(130)
    except Exception as e:
        print(f"\nüí• Error running tests: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
