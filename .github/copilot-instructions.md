# AI Assistant Instructions for Z-Beam Generator

**For**: GitHub Copilot, Grok AI, Claude, and all AI development assistants  
**System**: Laser cleaning content generation with strict fail-fast architecture  
**Last Updated**: November 22, 2025

---

## ğŸš€ **30-SECOND QUICK START** ğŸ”¥ **NEW (Nov 22, 2025)**

**âš¡ PRIORITY**: Read this section FIRST for immediate navigation, then consult detailed rules below.

### **ğŸ“– Full Navigation Guide**
**Primary Resource**: `docs/08-development/AI_ASSISTANT_GUIDE.md`  
- 30-second navigation to any documentation
- Quick lookup tables for common tasks
- Pre-change checklist
- Emergency recovery procedures

### **ğŸ¯ Common Tasks - Direct Links**

| Task | Resource |
|------|----------|
| **Generate content** | `.github/COPILOT_GENERATION_GUIDE.md` (step-by-step) |
| **Fix bugs/add features** | Pre-Change Checklist (see below) + `docs/SYSTEM_INTERACTIONS.md` |
| **Check policy compliance** | `docs/08-development/` (HARDCODED_VALUE_POLICY, TERMINAL_LOGGING_POLICY, etc.) |
| **Prompt chaining/orchestration** | `docs/08-development/PROMPT_CHAINING_POLICY.md` ğŸ”¥ **NEW** |
| **Naming conventions** | `docs/08-development/NAMING_CONVENTIONS_POLICY.md` ğŸ”¥ **NEW** |
| **Understand data flow** | `docs/02-architecture/processing-pipeline.md` |
| **Find quick answers** | `docs/QUICK_REFERENCE.md` |
| **Troubleshoot errors** | `TROUBLESHOOTING.md` (root) |
| **Research implementation history** | `docs/archive/2025-11/` (52 archived docs) |

### **ğŸš¦ Critical Policies Summary**

**TIER 1: System-Breaking** (Will cause failures)
- âŒ NO mocks/fallbacks in production code (tests OK)
- âŒ NO hardcoded values/defaults (use config/dynamic calc)
- âŒ NO rewriting working code (minimal surgical fixes only)

**TIER 2: Quality-Critical** (Will cause bugs)
- âŒ NO expanding scope (fix X means fix ONLY X)
- âœ… ALWAYS fail-fast on config (throw exceptions)
- âœ… ALWAYS log to terminal (comprehensive dual logging)

**TIER 3: Evidence & Honesty** (Will lose trust)
- âœ… ALWAYS provide evidence (test output, commits)
- âœ… ALWAYS be honest (acknowledge limitations)
- ğŸ”¥ NEVER report success when quality gates fail

**Full Details**: See TIER PRIORITIES section below + `docs/08-development/`

### **ğŸ“‹ Quick Pre-Change Checklist**

Before ANY code change:
1. [ ] Search `docs/QUICK_REFERENCE.md` for existing guidance
2. [ ] Check `docs/08-development/` for relevant policy
3. [ ] Review `docs/SYSTEM_INTERACTIONS.md` for side effects
4. [ ] Plan minimal fix (one sentence description)
5. [ ] Verify all file paths exist before coding
6. [ ] Ask permission before major changes or rewrites

## ğŸ“¦ File Organization & Root Cleanliness Policy (Nov 28, 2025)

**Purpose:** Ensure all file types (.py, .sh, .log, .txt, etc.) are organized for maintainability and AI assistant workflow compliance. Prevent root clutter and legacy file sprawl.

### ğŸ”¹ Python Scripts (.py)
- **Batch/utility scripts:** Move to `scripts/` or `tools/`.
- **Runner scripts:** Only keep essential entry points (e.g., `run.py`) in root.
- **Test scripts:** All `test_*.py` files must be in `tests/`.

### ğŸ”¹ Shell Scripts (.sh)
- **Batch/process scripts:** Move to `scripts/` or `batch/`.
- **Migration/monitoring scripts:** Group by function in subfolders (e.g., `scripts/migration/`, `scripts/monitoring/`).

### ğŸ”¹ Log Files (.log)
- **Operational logs:** Store in `logs/`, set up `.gitignore` for auto-exclusion.
- **Batch/research logs:** Move to `logs/` or `output/` as appropriate.

### ğŸ”¹ Text Files (.txt)
- **Progress trackers:** Move to `progress/` or `logs/`.
- **Requirements files:** Keep `requirements.txt` in root or move to `config/` if multiple environments.
- **Coverage lists:** Store in `coverage/` or `tests/`.

### ğŸ”¹ Markdown Files (.md)
- **Documentation:** Only keep top-level navigation docs in root (e.g., `README.md`, `DOCUMENTATION_MAP.md`).
- **Guides/architecture:** Move to `docs/` or relevant subdirectories.
- **Archived docs:** Store in `docs/archive/`.

### ğŸ”¹ General Rules
- **No stray files in root:** Only keep essential entry points and navigation docs.
- **Use `.gitignore`:** Exclude logs, outputs, and temporary files.
- **Automate cleanup:** Add scripts to remove old logs and outputs.
- **Document structure:** Update `DOCUMENTATION_MAP.md` to reflect new locations.

### ğŸ”¹ Maintenance
- **Regular audits:** Periodically check for stray files in root.
- **Enforce structure:** Use pre-commit hooks or CI checks to prevent root clutter.
- **Update navigation docs:** Ensure all links in `DOCUMENTATION_MAP.md` and `.github/copilot-instructions.md` are current.

**Compliance:**
- All file moves and organization must follow this policy.
- AI assistants must enforce root cleanliness before major documentation or workflow changes.
- Any exceptions require explicit approval and documentation.

**Full Checklist**: See "Mandatory Pre-Change Checklist" section below (lines ~300)

---

## ğŸš¨ **CRITICAL FAILURE PATTERNS TO AVOID** ğŸ”¥ **UPDATED (Nov 22, 2025)**

### **Pattern 0: Documentation Claims Contradicting Code Reality** ğŸ”¥ **NEW - MOST CRITICAL**
**What Happened**: Documentation claimed "Option C implemented - saves all attempts" but code was still blocking saves (10% success rate proved gates were active)
**Why It's Grade F**: Creates architectural confusion, wastes effort on wrong diagnosis, erodes user trust
**Impact**: 
- All Nov 22 analysis based on false assumption (Option C active)
- Wrong root cause identified (blamed Winston thresholds when real issue was gate blocking)
- Wasted time on fixes that couldn't help
**Correct Behavior**:
```
âœ… VERIFY implementation with tests BEFORE documenting as complete
âœ… Check actual code behavior matches documentation claims
âœ… Use success rate as reality check (10% = gates blocking, 100% = Option C working)
âœ… Write verification tests: test_saves_all_attempts_regardless_of_quality()
```

**Prevention Checklist**:
- [ ] Does a test verify this claim? (If no, don't claim "COMPLETE")
- [ ] Does actual behavior match documentation? (Run live test)
- [ ] Can I prove it with evidence? (Success rate, terminal output, test results)

### **Pattern 1: Reporting Success When Quality Gates Fail**
**What Happened**: AI reported "âœ… Description generated" when realism score was 5.0/10 (threshold: 5.5/10)
**Why It's Grade F**: Bypassed quality control, shipped low-quality content, dishonest reporting
**Correct Behavior**: 
```
âŒ REJECT if any gate fails:
  - Realism < 5.5/10
  - Winston > threshold
  - Readability FAIL
âœ… Only report success when ALL gates pass
âœ… Report failures honestly: "Quality gate failed, regenerating..."
```

### **Pattern 2: Not Reading Evaluation Scores Carefully**
**What Happened**: Two evaluations ran (9.0/10, then 5.0/10), AI only noticed the first
**Why It's Grade C**: Incomplete verification, missed critical quality failure
**Correct Behavior**:
```
âœ… Read ALL evaluation outputs
âœ… Check BOTH pre-save AND post-generation scores
âœ… Verify final stored content meets thresholds
âœ… Report: "Attempt 1: 9.0/10 âœ… PASS, Attempt 2: 5.0/10 âŒ FAIL"
```

### **Pattern 3: Manual Data Fixes Instead of Root Cause**
**What Happened**: Description saved to wrong location (line 1180), AI manually moved it to line 832
**Why It's Grade B**: Workaround instead of fixing the save logic
**Correct Behavior**:
```
âŒ Don't patch data files
âœ… Fix the generator code that saves incorrectly
âœ… Regenerate to verify fix persists
âœ… Ask: "Should I fix UnifiedMaterialsGenerator.save() logic?"
```

### **Pattern 4: Not Testing Against Actual Quality**
**What Happened**: AI-like phrases detected: "presents a unique challenge", "critical pitfall", formulaic structure
**Why It's Grade D**: Content reads like AI technical manual, not human writing
**Correct Behavior**:
```
âœ… Check for AI tell-tale phrases:
  - "presents a [unique/primary/significant] challenge"
  - "[critical/significant/primary] pitfall"  
  - "This [property/balance/approach] is essential for"
  - Formulaic structure (challenge â†’ solution â†’ importance)
âœ… Verify natural human voice
âœ… Reject robotic/textbook language
```

### **Pattern 5: Learned Parameters Producing Poor Quality**
**What Happened**: Sweet spot learning stored temp=0.815 but correlation showed temp has NEGATIVE correlation (-0.515)
**Why It's Grade C**: Learning system storing parameters that hurt quality
**Correct Behavior**:
```
âœ… Check learned parameter correlations
âœ… Question parameters with negative correlation
âœ… Verify sweet spot samples include recent high-quality content
âœ… Test: Does lower temperature produce better results?
```

### **Pattern 6: Treating Symptoms Instead of Root Causes** ğŸ”¥ **NEW (Nov 22, 2025) - CRITICAL**
**What Happened**: Word counts consistently 20-50% over target (150-194 words vs 50-150 max). AI added stricter prompt instructions 5+ times instead of fixing the actual mechanism.
**Why It's Grade F**: 
- Repeated the same failed approach multiple times
- Added "CRITICAL" warnings to prompts that were already being ignored
- Never addressed the real issue: LLMs don't count words during generation
- Wasted user's time with solutions that couldn't work

**What AI Did Wrong**:
1. âŒ Fixed hardcoded "150-450 words" in template (good fix, but insufficient)
2. âŒ Added "CRITICAL: Stay within word count" to prompt (ignored by model)
3. âŒ Added "DO NOT EXCEED THE MAXIMUM" instruction (also ignored)
4. âŒ Added placeholder `[TARGET_LENGTH_DISPLAY]` that was never replaced
5. âŒ Never checked if prompt instructions were even being used
6. âŒ Never measured actual word counts after each "fix"
7. âŒ Never questioned why prompt-only approach kept failing

**Root Cause Ignored**: LLMs generate token-by-token without counting words. Prompt instructions alone cannot enforce strict limits.

**Why max_tokens Won't Work**: Lowering max_tokens causes mid-sentence truncation, creating broken/incomplete content. This is WORSE than being over the word count.

**The Fundamental Truth**:
```
âŒ Prompt instructions alone: LLMs ignore word counts
âŒ Strict max_tokens: Causes truncation and broken sentences
âŒ Post-generation truncation: Also breaks sentences
âœ… Reality: Approximate word counts are inherent to LLM architecture
```

**Correct Behavior**:
```
âœ… Measure results after EACH fix attempt (don't assume it worked)
âœ… When same approach fails 2+ times, question the approach itself
âœ… Ask: "Why do prompt instructions keep getting ignored?"
âœ… Research: What is ACTUALLY possible with LLM architecture?
âœ… Accept architectural limitations: "approximately X words" not strict limits
âœ… Be honest with user: "LLMs consistently generate 20-30% over target"
âœ… Offer real solutions:
   - Option A: Accept approximate word counts (150-180w for 150w target)
   - Option B: Use quality-gated mode with multiple attempts and selection
   - Option C: Post-generation editing (manual review required)
âœ… DO NOT waste time on solutions that can't work (more prompt keywords)
```

**Prevention Checklist**:
- [ ] Did I measure the actual result after my fix?
- [ ] Am I repeating the same approach that already failed?
- [ ] Am I treating a symptom (prompt text) vs architectural limitation?
- [ ] Do I understand what is ACTUALLY POSSIBLE with this technology?
- [ ] Have I been honest about what can't be fixed?

**Red Flags That You're Treating Symptoms**:
- ğŸš© Adding more "CRITICAL" or "IMPORTANT" keywords to prompts
- ğŸš© Making text BOLD or adding emoji to existing instructions
- ğŸš© Rephrasing the same instruction in different words
- ğŸš© Adding duplicate requirements in multiple places
- ğŸš© Not measuring actual results after each change
- ğŸš© Assuming "this time it will work" without architectural change
- ğŸš© Proposing max_tokens limits (causes truncation)
- ğŸš© Proposing post-generation truncation (also causes truncation)

**Grade**: F - Wasting user time with ineffective solutions violates TIER 3 honesty requirements

### **Pattern 7: Architectural Documentation Inconsistency** ğŸ”¥ **NEW (Nov 22, 2025) - CRITICAL**
**What Happened**: Multiple documents claimed different architectures were "COMPLETE":
- `E2E_SYSTEM_ANALYSIS_NOV22_2025.md`: Graded system A+ assuming "Option C" active
- `OPTION_C_IMPLEMENTATION_NOV22_2025.md`: Documented "quality gates removed"
- **Reality**: Code still enforced quality gates (10% success rate proved it)

**Why It's Grade F**: 
- Wastes effort on wrong diagnosis
- User cannot trust documentation
- Future work builds on false assumptions
- Regression goes undetected

**Correct Behavior**:
```
âœ… MEASURE actual behavior (success rate, terminal output, test results)
âœ… If metrics contradict docs â†’ Documentation is WRONG
âœ… Write verification tests BEFORE claiming "COMPLETE"
âœ… Update ALL related docs when architecture changes
âœ… Never grade system A+ without verifying claims with tests
âœ… Use correct data access patterns when verifying (dict vs list)
```

**How to Detect This Pattern**:
- ğŸš© Success rate doesn't match documented behavior (10% â‰  "saves all")
- ğŸš© Terminal output contradicts documentation claims
- ğŸš© User reports failures but docs claim success
- ğŸš© Multiple documents describe different implementations
- ğŸš© High grade (A+) but low success metrics (10%)
- ğŸš© Verification scripts report failure when direct file inspection shows success

**Prevention**:
```
Before documenting as "COMPLETE":
1. Write test: test_[feature]_actually_works()
2. Run live test: python3 run.py --[feature]
3. Measure metrics: Success rate, save count, terminal output
4. Compare: Do metrics match documentation claims?
5. Verify data access patterns match actual structure (dict vs list)
6. If NO â†’ Fix code OR fix docs, then retest
7. Only claim "COMPLETE" when test + metrics verify it
```

**âœ… RESOLVED (Nov 23, 2025)**: Property research verification issue - materials were successfully researched and populated in Materials.yaml. Initial "NOT FOUND" reports were due to verification script bugs using list iteration on dict structure. Correct access pattern: `data['materials'][material_name]` not `next((m for m in materials...))`.

---

## ğŸ“– **Detailed Navigation for AI Assistants**

**ğŸ” Already checked 30-SECOND QUICK START above?** If not, scroll to top first.

### **User Requests Content Generation?**
â†’ **READ THIS FIRST**: `.github/COPILOT_GENERATION_GUIDE.md`
- Handles: "Generate material description for Aluminum", "Create caption for Steel", etc.
- Shows: Exact commands to run, terminal output handling, result reporting
- Covers: All component types (material_description, caption, FAQ, settings_description)

### **Need Documentation?**
â†’ **PRIMARY GUIDE**: `docs/08-development/AI_ASSISTANT_GUIDE.md` - 30-second navigation (NEW)
â†’ **COMPLETE MAP**: Root `/DOCUMENTATION_MAP.md` - All documentation indexed
â†’ **QUICK ANSWERS**: `docs/QUICK_REFERENCE.md` - Fastest path to solutions
â†’ **SYSTEM INDEX**: `docs/INDEX.md` - Comprehensive navigation

### **Working on Code/Architecture?**
â†’ **READ FIRST**: `docs/SYSTEM_INTERACTIONS.md` - Understand cascading effects before changing anything
â†’ **THEN CHECK**: `docs/decisions/README.md` - Architecture Decision Records (WHY things work this way)
â†’ **CHECK POLICIES**: `docs/08-development/` - All development policies and guidelines
â†’ **Continue below** for comprehensive rules and examples

### **âš ï¸ Before Making ANY Change**
1. **Check `docs/SYSTEM_INTERACTIONS.md`**: What will your change affect?
2. **Check `docs/decisions/`**: Is there an ADR about this?
3. **Check git history**: Has this been tried and failed before?
4. **Check `docs/08-development/`**: Is there a policy document about this?
5. **Plan minimal fix**: Address only the specific issue

---

## ğŸš¦ **TIER PRIORITIES** - Critical Rules Hierarchy

Understanding rule severity helps prioritize fixes and avoid introducing worse problems.

### ğŸ”´ **TIER 1: SYSTEM-BREAKING** (Will cause failures)
1. âŒ **NO mocks/fallbacks in production code** (tests OK) - [Rule #2](#rule-2-zero-production-mocksfallbacks)
2. âŒ **NO hardcoded values/defaults** (use config/dynamic calc) - [Rule #3](#rule-3-fail-fast-on-setup--zero-hardcoded-values)
3. âŒ **NO rewriting working code** (minimal surgical fixes only) - [Rule #1](#rule-1-preserve-working-code)

### ğŸŸ¡ **TIER 2: QUALITY-CRITICAL** (Will cause bugs)
4. âŒ **NO expanding scope** (fix X means fix ONLY X) - [Rule #5](#rule-5-surgical-precision)
5. âŒ **NO skipping validation** (must test before claiming success) - [Step 6](#step-6-implement--test)
6. âœ… **ALWAYS fail-fast on config** (throw exceptions, no silent degradation) - [Rule #3](#rule-3-fail-fast-on-setup--zero-hardcoded-values)
7. âœ… **ALWAYS preserve runtime recovery** (API retries are correct) - See ADRs
8. âœ… **ALWAYS log to terminal** (all generation attempts, scores, feedback) - See Terminal Output Policy

### ğŸŸ¢ **TIER 3: EVIDENCE & HONESTY** (Will lose trust)
9. âœ… **ALWAYS provide evidence** (test output, counts, commits) - [Verification Protocol](#mandatory-verify-before-claiming-success)
10. âœ… **ALWAYS be honest** (acknowledge what remains broken) - [Step 7](#step-7-honest-reporting)
11. âœ… **ASK before major changes** (get permission for improvements) - [Rule #1](#rule-1-preserve-working-code)
12. âœ… **VERIFY before claiming violations** (check config files, confirm pattern exists) - [Step 6](#step-6-verify-before-claiming-violations)
13. ğŸ”¥ **NEVER report success when quality gates fail** (realism < 5.5, Winston fail) - [Pattern 1](#critical-failure-patterns-to-avoid)
14. ğŸ”¥ **ALWAYS read ALL evaluation scores** (pre-save AND post-generation) - [Pattern 2](#critical-failure-patterns-to-avoid)
15. ğŸ”¥ **ALWAYS check for AI-like phrases** ("presents a challenge", formulaic structure) - [Pattern 4](#critical-failure-patterns-to-avoid)
16. ğŸ”¥ **ALWAYS verify implementation before documentation** (write tests, measure success rate) - [Pattern 0](#critical-failure-patterns-to-avoid) **NEW**
17. ğŸ”¥ **NEVER document features without evidence** (tests prove it works) - [Step 6.5](#step-6-5-verify-implementation-matches-documentation) **NEW**

**ğŸš¨ CRITICAL: Reporting success when quality fails is a TIER 3 violation - Grade F.**
**ğŸš¨ CRITICAL: Documenting unimplemented features as "COMPLETE" is a TIER 3 violation - Grade F.** ğŸ”¥ **NEW**

---

## ğŸš¦ **DECISION TREES** - When in Doubt, Use These

### Decision: Should I use a default value?
```
Is this a config/setup issue?
â”œâ”€ YES â†’ âŒ FAIL FAST (throw ConfigurationError)
â””â”€ NO â†’ Is this a runtime/transient issue?
    â”œâ”€ YES â†’ âœ… RETRY with backoff (API timeout, network error)
    â””â”€ NO â†’ Is this a quality check iteration?
        â”œâ”€ YES â†’ âœ… ITERATE (adjust parameters based on feedback)
        â””â”€ NO â†’ âŒ FAIL FAST (programming error)
```

### Decision: Should I rewrite this code?
```
Does the code work correctly?
â”œâ”€ YES â†’ âŒ NO REWRITE (integrate around it, add method, minimal fix)
â””â”€ NO â†’ Is it a small targeted fix?
    â”œâ”€ YES â†’ âœ… FIX ONLY broken part
    â””â”€ NO â†’ âš ï¸ ASK PERMISSION (explain why rewrite needed)
```

### Decision: Should I add a hardcoded value?
```
Can I find an existing dynamic solution?
â”œâ”€ YES â†’ âœ… USE IT (grep_search for DynamicConfig, helpers)
â””â”€ NO â†’ Is this truly a constant?
    â”œâ”€ YES â†’ âœ… CONFIG FILE (config.yaml, not code)
    â””â”€ NO â†’ âš ï¸ ASK USER (explain why dynamic solution doesn't exist)
```

---

## ğŸ“‹ **TERMINAL OUTPUT LOGGING POLICY**

**ALL generation operations MUST stream comprehensive output to terminal in real-time.**

**Logging Requirements**:
1. **Stream to stdout/stderr ONLY** - No log files created or saved
2. **Real-time output** - User sees progress as it happens
3. **Attempt Progress** - Every retry with attempt number (e.g., "Attempt 2/5")
4. **Quality Checks** - Winston score, Realism score, thresholds, pass/fail
5. **Feedback Application** - Parameter adjustments between attempts
6. **Learning Activity** - Prompt optimization, pattern learning
7. **Final Report** - Complete generation report (see Generation Report Policy)

**Example Streaming Output**:
```
Attempt 2/5
ğŸŒ¡ï¸  Temperature: 0.750 â†’ 0.825
ğŸ“‰ Frequency penalty: 0.20 â†’ 0.30
Winston Score: 98.6% human âœ… PASS
Realism Score: 5.0/10 (threshold: 5.5) âŒ FAIL
âœ… [REALISM FEEDBACK] Parameter adjustments calculated
```

**Implementation**:
- Use `print()` for terminal output (not `logger.info()` to files)
- All subprocess calls inherit stdout/stderr (no capture)
- Batch tests stream directly (no tee to log files)

**Purpose**: User visibility, debugging, transparency, verification

**Anti-Patterns**: 
- âŒ Silent failures, hidden retries, opaque processing
- âŒ Log files in /tmp/ or elsewhere
- âŒ Capturing output without displaying it

---

## ğŸ“‹ **TERMINAL OUTPUT LOGGING POLICY** ğŸ”¥ **NEW (Nov 22, 2025) - CRITICAL**

**ALL generation operations MUST stream comprehensive output to terminal in real-time.**

**Logging Requirements**:
1. **Stream to stdout using print()** - Always visible to user, not just logger.info()
2. **Real-time output** - User sees progress as it happens
3. **Dual logging** - Both print() (terminal) AND logger.info() (file records)
4. **Attempt Progress** - Every retry with attempt number (e.g., "Attempt 2/5")
5. **Quality Checks** - Winston score, Realism score, thresholds, pass/fail
6. **Parameter Adjustments** - Changes between attempts
7. **Learning Activity** - Database logging, pattern learning
8. **Final Report** - Complete generation report (see Generation Report Policy)
9. **FULL NON-TRUNCATED OUTPUT** ğŸ”¥ **CRITICAL** - NEVER use tail, head, or truncation

**Required Terminal Output**:
```
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“ ATTEMPT 2/5
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸŒ¡ï¸  Current Parameters:
   â€¢ temperature: 0.825
   â€¢ frequency_penalty: 0.30

ğŸ§  Generating humanness instructions (strictness level 2/5)...
   âœ… Humanness layer generated (1234 chars)

âœ… Generated: 287 characters, 45 words

ğŸ” Pre-flight: Checking for forbidden phrases...
   âœ… No forbidden phrases detected

ğŸ” Evaluating quality BEFORE save...

ğŸ“Š QUALITY SCORES:
   â€¢ Overall Realism: 8.5/10
   â€¢ Voice Authenticity: 8.0/10
   â€¢ Tonal Consistency: 7.5/10
   â€¢ AI Tendencies: None detected

ğŸ“‰ ADAPTIVE THRESHOLD: 5.2/10 (relaxed from 5.5 for attempt 2)

   ğŸ“Š Logged attempt 2 to database (detection_id=779, passed=False)

âš ï¸  QUALITY GATE FAILED - Will retry with adjusted parameters
   â€¢ Realism score too low: 5.0/10 < 5.5/10

ğŸ”§ Adjusting parameters for attempt 3...
   âœ… Parameters adjusted for retry

ğŸ”„ Parameter changes for next attempt:
   â€¢ temperature: 0.825 â†’ 0.900
   â€¢ frequency_penalty: 0.30 â†’ 0.40
```

**Implementation Pattern**:
```python
# Terminal output (always visible)
print(f"ğŸ“Š QUALITY SCORES:")
print(f"   â€¢ Overall Realism: {score:.1f}/10")

# File logging (for records)
logger.info(f"ğŸ“Š QUALITY SCORES:")
logger.info(f"   â€¢ Overall Realism: {score:.1f}/10")
```

**Documentation**: `docs/08-development/TERMINAL_LOGGING_POLICY.md`
**Tests**: `tests/test_terminal_logging_policy.py` - 12 tests verify compliance
**Enforcement**: All generation operations must use dual logging (print + logger)

**Anti-Patterns**: 
- âŒ Silent operations (logger.info() only, no print())
- âŒ Hidden retries (no terminal visibility)
- âŒ Batch output at end (should stream in real-time)
- âŒ Log files only (user can't see what's happening)
- âŒ **TRUNCATED OUTPUT** (tail -n, head -n, or any output limiting) ğŸ”¥ **CRITICAL**

**Grade**: MANDATORY - Non-compliance is a policy violation

---

## ğŸ›¡ï¸ **MANDATORY: Pre-Code Execution Protocol** ğŸ”¥ **UPDATED (Nov 22, 2025)**

**ğŸ›‘ STOP - Complete ALL checks BEFORE writing any code:**

**ğŸ“– Quick Reference**: See `docs/08-development/AI_ASSISTANT_GUIDE.md` for streamlined checklist

### âœ… Phase 1: Verification (2-3 minutes)
- [ ] **Read request word-by-word** - What EXACTLY is being asked?
- [ ] **Check for assumptions** - Am I assuming anything not stated?
- [ ] **Verify file paths** - Do all referenced files actually exist?
- [ ] **Check config keys** - Do claimed violations actually exist in config files?
- [ ] **Search for existing solutions** - Does DynamicConfig/helper already solve this?

### âœ… Phase 2: Research (3-5 minutes)
- [ ] **grep_search for patterns** - How does the system currently handle this?
- [ ] **Read relevant code** - Understand current implementation
- [ ] **Check git history** - Was this tried before? Why was it changed?
- [ ] **Review docs/** - Is there policy documentation on this?
- [ ] **Check ADRs** - Is there an architectural decision about this?

### âœ… Phase 3: Planning (2-3 minutes)
- [ ] **Identify exact change needed** - One sentence description
- [ ] **Confirm minimal scope** - Am I fixing ONLY what was requested?
- [ ] **Check for side effects** - What else might this affect?
- [ ] **Plan validation** - How will I prove it works?
- [ ] **Get permission if major** - Ask before removing/rewriting code

### ğŸš¨ **STOP SIGNALS** - When to ASK instead of CODE:
- â“ If you're not 100% certain about the requirement
- â“ If you can't find the config key/file/pattern being referenced
- â“ If fixing this requires changing more than 3 files
- â“ If you're about to add a hardcoded value without finding dynamic config first
- â“ If the request conflicts with existing architecture
- â“ If tests are failing and you don't understand why

**â±ï¸ Time Investment**: 7-11 minutes of research prevents hours of fixing broken code.

---

## ğŸ¯ Quick Reference Card

**READ THIS FIRST - BEFORE ANY CHANGE:**

1. âœ… **Read the request precisely** - What is the *exact* issue?
2. âœ… **Search documentation FIRST** - Check `docs/` for existing guidance (see Documentation Compliance Checklist below)
3. âœ… **Explore existing architecture** - Understand how it currently works
4. âœ… **Check git history for context** - See what was working previously
5. âœ… **Plan minimal fix only** - Address only the specific issue
6. âœ… **Ask permission for major changes** - Get approval before removing code or rewrites
7. âœ… **ğŸ”¥ Verify with tests BEFORE claiming complete** - Evidence over assumptions

**GOLDEN RULES:**
- ğŸš« **NEVER rewrite working code**
- ğŸš« **NEVER expand beyond requested scope**
- ğŸš« **NEVER use mocks/fallbacks in production code - NO EXCEPTIONS**
- âœ… **ALLOW mocks/fallbacks in test code for proper testing**
- ğŸš« **NEVER add "skip" logic or dummy test results**
- ğŸš« **NEVER put content instructions in /processing folder code**
- ğŸš« **NEVER hardcode component types in /processing code**
- ğŸš« **NEVER hardcode values in production code** - use config or dynamic calculation
- ğŸš« **NEVER claim "COMPLETE" without verification tests** ğŸ”¥ **NEW**
- ğŸš« **NEVER document features as implemented without evidence** ğŸ”¥ **NEW**
- âœ… **ALWAYS keep content instructions ONLY in prompts/*.txt files**
- âœ… **ALWAYS define components ONLY in prompts/*.txt and config.yaml**
- âœ… **ALWAYS preserve existing patterns**
- âœ… **ALWAYS fail-fast on configuration issues**
- âœ… **ALWAYS maintain runtime error recovery**
- âœ… **ALWAYS verify documentation matches reality with tests** ğŸ”¥ **NEW**
- âœ… **ALWAYS sync Materials.yaml updates to frontmatter (dual-write)** ğŸ”¥ **NEW (Nov 22, 2025)**

---

## ğŸ“š Recent Critical Updates (November 2025)

### ğŸš€ Learning System Improvements (November 22, 2025) ğŸ”¥ **NEW**
**Status**: âœ… Priority 1 & 2 COMPLETE - System producing 50x more learning data

**Problem Solved**: Quality gates blocked 90% of content from learning system, creating "quality-learning death spiral"
**Solution**: Multi-phase approach enabling learning while maintaining quality standards

**Implementations**:
1. **Priority 1 - Log ALL Attempts** (âœ… COMPLETE):
   - Added `_log_attempt_for_learning()` method (~160 lines)
   - Logs EVERY attempt BEFORE quality gate decision (not just successes)
   - Result: **50x more learning data** (5 attempts/material vs 0.1 success/material)
   - Database captures: Winston scores, subjective evaluation, structural diversity, ALL parameters
   - Enables correlation analysis and sweet spot improvements

2. **Priority 2 - Adaptive Threshold Relaxation** (âœ… COMPLETE):
   - Added `_get_adaptive_threshold()` method with graduated relaxation
   - Thresholds: Attempt 1 (5.5/10) â†’ 2 (5.3) â†’ 3 (5.0) â†’ 4 (4.8) â†’ 5 (4.5)
   - Result: Expected **29% â†’ 50-70% success rate**
   - Maintains quality floor (4.5/10 minimum) while reducing 70% waste
   - Verified working: Terminal shows "ğŸ“‰ ADAPTIVE THRESHOLD: 5.2/10 (relaxed from 5.5 for attempt 2)"

**Future Priorities** (Ready for implementation):
- Priority 3: Opening pattern cooldown (5 hours) - Reduce 10/10 repetition â†’ 2/10
- Priority 4: Correlation filter (3 hours) - Exclude negative correlation parameters
- Priority 5: Two-phase strategy (6 hours) - Exploration â†’ exploitation approach

**Documentation**: 
- `LEARNING_IMPROVEMENTS_NOV22_2025.md` - Complete implementation guide
- `LEARNING_SYSTEM_ANALYSIS_NOV22_2025.md` - Original analysis and recommendations
- `PRIORITY1_COMPLETE_NOV22_2025.md` - Priority 1 detailed documentation

**Policy Compliance**: 100% compliant - fail-fast architecture, zero hardcoded values, template-only
**Grade**: Priority 1: A+ (100/100), Priority 2: A (95/100)

### ğŸš¨ Mock/Fallback Violations Eliminated (November 20, 2025) ğŸ”¥ **CRITICAL**
**Status**: âœ… FIXED - 26 violations eliminated, 24/24 tests passing

**Discovery**: Batch test revealed Winston API unconfigured but system continuing with fake scores (100% human, 0% AI)
**Grade**: Grade F violation of GROK_QUICK_REF.md TIER 3: Evidence & Honesty

**Violations Fixed**:
1. **generation.py**: Silent Winston failure â†’ RuntimeError (fail-fast)
2. **generation.py**: Hardcoded temperature=0.7 â†’ None (metadata only)
3. **constants.py**: Removed DEFAULT_AI_SCORE, DEFAULT_HUMAN_SCORE, DEFAULT_FALLBACK_AI_SCORE
4. **batch_generator.py**: 13 violations - removed skip logic, fallback scores, mock data, hardcoded penalties
5. **run.py**: Marked --skip-integrity-check as [DEV ONLY] with warnings
6. **integrity_helper.py**: 3 violations - fixed silent failures on exceptions
7. **subtitle_generator.py**: Removed hardcoded temperature (0.6), now uses dynamic config
8. **quality_gated_generator.py**: Removed TODO, documented design decision
9. **threshold_manager.py**: 2 TODOs â†’ documented as future work with design rationale
10. **test_score_normalization_e2e.py**: Updated tests to remove assertions on deleted constants

**Enforcement**:
- System now raises RuntimeError if Winston API unavailable
- All `.get('score', default)` patterns replaced with fail-fast None checks
- No fallback scores available - validation REQUIRED
- Skip flags marked DEV ONLY with explicit warnings

**Documentation**: 
- `VIOLATION_FIXES_NOV20_2025.md` - Complete fix documentation with before/after code
- `TEST_RESULTS_NOV20_2025.md` - Test execution before violations discovered

**Policy Compliance**: 100% compliant with Core Principle #2 (No Mocks/Fallbacks)
**Grade**: System upgraded from Grade F to A+ (100/100) after complete remediation

### âœ… Learned Evaluation Pipeline Integration (November 18, 2025) ğŸ”¥ **NEW**
**Status**: âœ… IMPLEMENTED AND TESTED (17/17 tests passing)

**What**: Complete pipeline for template-based evaluation with continuous learning
**Components**:
- `prompts/evaluation/subjective_quality.txt` - Template for evaluation prompts (no hardcoded prompts in code)
- `prompts/evaluation/learned_patterns.yaml` - Auto-updating learned patterns from evaluations
- `processing/learning/subjective_pattern_learner.py` - Learning system with exponential moving averages
- Integration: SubjectiveEvaluator loads templates, generator updates patterns after each evaluation

**Learning Flow**:
1. Content generated
2. Evaluator loads template + learned patterns from files
3. Grok evaluates content
4. Pattern learner updates YAML (rejection patterns: AI tendencies, theatrical phrases)
5. If accepted: Pattern learner updates success patterns (EMA with alpha=0.1)
6. Next generation uses updated patterns

**Files Changed**:
- NEW: `prompts/evaluation/subjective_quality.txt` (template)
- NEW: `prompts/evaluation/learned_patterns.yaml` (learning data)
- NEW: `processing/learning/subjective_pattern_learner.py` (learner)
- NEW: `tests/test_learned_evaluation_pipeline.py` (17 tests âœ…)
- MODIFIED: `processing/subjective/evaluator.py` (template integration)
- MODIFIED: `processing/generator.py` (learning integration)

**Documentation**: 
- `LEARNED_EVALUATION_INTEGRATION_NOV18_2025.md` - Complete implementation summary
- `docs/08-development/LEARNED_EVALUATION_PROPOSAL.md` - Architecture (now IMPLEMENTED)

**Policy Compliance**:
- âœ… Prompt Purity Policy: Zero hardcoded prompts in evaluator code
- âœ… Fail-Fast Architecture: Template missing â†’ FileNotFoundError
- âœ… Learning Integration: Works with Winston, Realism, Composite scoring

**Grade**: A+ (100/100) - Full implementation, all tests passing

### âœ… Priority 1 Compliance Fixes (November 17, 2025)
**Commit**: c5aa1d6c - All critical violations resolved

1. **RealismOptimizer Import Fixed**: Corrected path from `processing.realism.optimizer` to `processing.learning.realism_optimizer`
2. **SubjectiveEvaluator Temperature**: Now configurable via parameter (no hardcoded values)
3. **Fail-Fast Architecture Enforced**: Removed non-existent fallback method calls

**Documentation**: 
- `docs/archive/2025-11/E2E_PROCESSING_EVALUATION_NOV17_2025.md` - Full evaluation report
- `docs/archive/2025-11/PRIORITY1_UPDATES_COMPLETE.md` - Implementation summary
- `tests/test_priority1_fixes.py` - 10 automated tests (all passing âœ…)

**Grade**: System upgraded from C+ to B+ (85/100) after fixes

### ğŸ¯ Prompt Purity Policy (November 18, 2025) ğŸ”¥ **CRITICAL**
**Issue**: Prompt instructions hardcoded in generator code (orchestrator.py, generator.py)
**Fix**: All content instructions MUST exist ONLY in prompts/*.txt files
**Violations Found**: 5 critical violations (system_prompt hardcoding, inline CRITICAL RULE text)
**Policy**: ZERO prompt text permitted in generators - use _load_prompt_template() only
**Documentation**: docs/08-development/PROMPT_PURITY_POLICY.md

### ğŸ¯ Realism Quality Gate Enforcement (November 18, 2025) ğŸ”¥ **CRITICAL**
**Issue**: Subjective evaluation was running but NOT rejecting low-quality content
**Fix**: Realism score (7.0/10 minimum) now enforced as quality gate
**Impact**: Content with AI issues (theatrical phrases, casual language) now REJECTED
**Learning**: Both Winston and Realism feedback drive parameter adjustments on retry

### ğŸ¯ Composite Quality Scoring (November 16, 2025)
**Architecture**: GENERIC_LEARNING_ARCHITECTURE.md implemented
- Winston (40%) + Realism (60%) weighting for combined score
- Realism gate: 7.0/10 minimum threshold (enforced)
- Adaptive threshold learning from 75th percentile of successful content
- Sweet spot analyzer uses composite scores for parameter optimization

### ğŸ—£ï¸ Content Instruction Policy
**CRITICAL**: Content instructions ONLY in `prompts/*.txt` files, NEVER in code
- Format rules, style guidance, focus areas â†’ prompts/
- Technical mechanisms only â†’ processing/
- See: `docs/prompts/CONTENT_INSTRUCTION_POLICY.md`

---

## ğŸ“– Core Principles

### 1. **No Mocks or Fallbacks in Production Code**
System must fail immediately if dependencies are missing. **ZERO TOLERANCE** for:
- MockAPIClient or mock responses in production
- Default values that bypass validation (`or "default"`)
- Skip logic that bypasses checks (`if not exists: return True`)
- Placeholder return values (`return {}`)
- Silent failures (`except: pass`)
- **Category fallback ranges** (`if prop missing: use category_range`)
- **Template fallbacks** (`if data missing: use template`)

**âœ… EXCEPTION**: Mocks and fallbacks **ARE ALLOWED in test code** for proper testing infrastructure.

**ğŸ” TESTING REQUIREMENT**: Part of testing should include verifying ZERO presence of mocks and fallbacks in production code.

### 2. **No Hardcoded Values in Production Code** ğŸ”¥ **NEW POLICY**
All configuration values MUST come from config files or dynamic calculation. **ZERO TOLERANCE** for:
- **Hardcoded API penalties** (`frequency_penalty=0.0`, `presence_penalty=0.5`)
- **Hardcoded thresholds** (`if score > 30:`, `threshold = 0.7`)
- **Hardcoded temperatures** (`temperature = 0.8`)
- **Hardcoded defaults** (`.get('key', 0.0)`, `or {}` in production paths)
- **Magic numbers** (`attempts = 5`, `max_length = 100`)

**âœ… CORRECT APPROACH**:
- Use `config.get_temperature()` not `temperature = 0.8`
- Use `dynamic_config.calculate_penalties()` not `frequency_penalty = 0.0`
- Use `config.get_threshold()` not `if score > 30:`
- Fail fast if config missing, don't use defaults

**ğŸ” ENFORCEMENT**: Integrity checker automatically detects hardcoded values in production code.

### 3. **Explicit Dependencies**
All required components must be explicitly provided - no silent degradation.

### 3. **Data Storage Policy** ğŸ”¥ **CRITICAL**
**ALL generation and validation happens on Materials.yaml ONLY.**

- âœ… **Materials.yaml** - Single source of truth + all generation/validation happens here
  - ALL AI text generation (captions, descriptions, etc.)
  - ALL property research and discovery
  - ALL completeness validation
  - ALL quality scoring and thresholds
  - ALL schema validation
- âœ… **Frontmatter files** - Receive immediate partial field updates (dual-write)
  - Automatic sync when Materials.yaml updated
  - Only changed field written (others preserved)
  - Never read for data persistence
- âœ… **Categories.yaml** - Single source of truth for category ranges
- âŒ **Frontmatter files** - Never read for data persistence (write-only mirror)
  - Simple field-level sync from Materials.yaml
  - Should take milliseconds per update
  - No complex operations during sync
- âœ… **Data Flow**: Generate â†’ Materials.yaml (full write) + Frontmatter (field sync)
- âœ… **Persistence**: All AI research saves to Materials.yaml immediately
- âœ… **Dual-Write**: Every Materials.yaml update triggers frontmatter field sync

### ğŸš¨ **MANDATORY: Field Isolation During Generation** ğŸ”¥ **NEW (Nov 22, 2025)**
**Component generation flags (--description, --caption, etc.) MUST ONLY update the specified field.**

- âœ… `--description` â†’ Updates ONLY description field (preserves caption, faq, author, etc.)
- âœ… `--caption` â†’ Updates ONLY caption field (preserves description, faq, etc.)
- âœ… `--faq` â†’ Updates ONLY faq field (preserves description, caption, etc.)
- âŒ **VIOLATION**: Overwriting ANY unrelated field during component generation

**Enforcement**: 15 automated tests verify field isolation (`tests/test_frontmatter_partial_field_sync.py`)

See `docs/data/DATA_STORAGE_POLICY.md` for complete policy.

### 4. **Component Architecture**
Use ComponentGeneratorFactory pattern for all generators.

### 5. **Fail-Fast Design with Quality Gates**
- âœ… **What it IS**: Validate inputs, configurations, and dependencies immediately at startup
- âœ… **What it IS**: Throw specific exceptions (ConfigurationError, GenerationError) with clear messages
- âœ… **What it IS**: Enforce quality gates (Winston 69%+, Realism 7.0+, Readability pass)
- âŒ **What it's NOT**: Removing runtime error recovery like API retries for transient issues

**Quality Gates (ALL must pass)**:
1. Winston AI Detection: 69%+ human score (configurable via humanness_intensity, currently at level 7)
2. Readability Check: Pass status
3. Subjective Language: No violations
4. **Realism Score: 7.0/10 minimum** â† NEW (Nov 18, 2025)
5. Combined Quality Target: Meets learning target

### 6. **Content Instruction Policy** ğŸ”¥ **CRITICAL**
**Content instructions MUST ONLY exist in prompts/*.txt files.**

- âœ… **prompts/*.txt files** - Single source of truth for ALL content instructions
  - Focus areas (what to emphasize)
  - Format rules (structural requirements)
  - Style guidance (voice and tone)
  - Component-specific content strategy
- âŒ **processing/*.py files** - ONLY technical mechanisms (NO content instructions)
  - Word count calculations
  - Voice parameter application
  - API integration
  - Quality validation
- ğŸš« **FORBIDDEN in ComponentSpec**: `format_rules`, `focus_areas`, `style_notes` fields
- ğŸš« **FORBIDDEN in SPEC_DEFINITIONS**: Content instruction keys
- âœ… **ALLOWED in ComponentSpec**: `name`, `lengths`, `end_punctuation`, `prompt_template_file`
- âœ… **ENFORCEMENT**: 5 automated tests verify policy compliance (see `tests/test_content_instruction_policy.py`)

See `docs/prompts/CONTENT_INSTRUCTION_POLICY.md` for complete policy.

### 7. **Component Discovery Policy** ğŸ”¥ **NEW (Nov 16, 2025)**
**Component types MUST ONLY be defined in prompts/*.txt and config.yaml.**

- âœ… **prompts/*.txt files** - Define component types by filename
  - Create `prompts/caption.txt` to define 'caption' component
  - Create `prompts/material_description.txt` to define 'material_description' component
  - Each .txt file = one component type
- âœ… **config.yaml** - Define component word counts
  ```yaml
  component_lengths:
    caption: 25
    material_description: 15
  ```
- âŒ **processing/*.py files** - NO hardcoded component types
  - âŒ `if component_type == 'caption':`
  - âŒ `SPEC_DEFINITIONS = {'caption': {...}}`
  - âŒ Hardcoded component lists
- âœ… **Dynamic Discovery**: Components discovered at runtime from prompts/
- âœ… **Generic Code**: Use `component_type` parameter, iterate `ComponentRegistry.list_types()`
- âœ… **ENFORCEMENT**: Automated tests verify zero hardcoded components

See `docs/architecture/COMPONENT_DISCOVERY.md` for complete policy.

### 8. **Template-Only Policy** ğŸ”¥ **NEW (Nov 18, 2025) - CRITICAL**
**ONLY prompt templates determine content and formatting. NO component-specific methods.**

- âœ… **shared/text/templates/components/*.txt** - ALL content instructions and formatting rules
  - Structure guidelines, style requirements, forbidden phrases
  - Format specifications, example outputs, voice/tone rules
  - COMPLETE content strategy for each component type
- âŒ **processing/*.py** - ZERO component-specific code
  - âŒ NO `if component_type == 'caption':` checks
  - âŒ NO component-specific methods (`_build_caption_prompt()`, `_extract_caption()`)
  - âŒ NO hardcoded content instructions in code
  - âŒ NO component-specific extraction logic in generators
- âœ… **Strategy Pattern**: Use `extraction_strategy` in config.yaml
  ```yaml
  component_lengths:
    caption:
      default: 50
      extraction_strategy: before_after  # Strategy-based extraction
    material_description:
      default: 30
      extraction_strategy: raw  # Return text as-is
  ```
- âœ… **Generic Methods**: Use strategy dispatch, not component checks
  - âœ… `adapter.extract_content(text, component_type)` - delegates to strategy
  - âœ… `_load_prompt_template(component_type)` - loads generic template
  - âŒ `_extract_caption(text)` - component-specific method
- âœ… **Full Reusability**: /processing works for ANY domain (materials, contaminants, regions)
- âœ… **Zero Code Changes**: Add new component = create template + config entry only

**Adding New Component**:
```bash
# OLD WAY (NON-COMPLIANT): 4 code files + 1 template
1. âŒ Edit generator.py - add elif component_type == 'new_component'
2. âŒ Edit adapter.py - add _extract_new_component() method
3. âŒ Edit prompt_builder.py - add _build_new_component_prompt()
4. âŒ Add content instructions to code

# NEW WAY (COMPLIANT): 1 config + 1 template = ZERO CODE CHANGES
1. âœ… Create shared/text/templates/components/new_component.txt (all instructions)
2. âœ… Add to config.yaml: component_lengths: { new_component: {default: 100, extraction_strategy: raw} }
```

See `docs/08-development/TEMPLATE_ONLY_POLICY.md` for complete policy.

### 9. **Prompt Purity Policy** ğŸ”¥ **NEW (Nov 18, 2025)**
**ALL content generation instructions MUST exist ONLY in prompt template files.**

- âœ… **prompts/*.txt files** - Single source of truth for ALL prompts
  - System prompts, content rules, style guidance
  - Voice/tone instructions, format requirements
  - Forbidden phrases, required elements
- âŒ **processing/*.py files** - ZERO prompt text permitted (NO EXCEPTIONS)
  - âŒ `system_prompt = "You are a professional technical writer..."`
  - âŒ `prompt += "\nCRITICAL RULE: Write ONLY..."`
  - âŒ `prompt.replace("text", "YOU MUST NOT...")`
  - âŒ Inline content instructions of any kind
- âœ… **Generator code** - Load prompts from templates ONLY
  - âœ… `prompt = self._load_prompt_template('caption.txt')`
  - âœ… Technical parameters (temperature, penalties) in code
  - âœ… Data insertion (material names, properties) allowed
- âœ… **ENFORCEMENT**: Automated tests verify zero hardcoded prompts

See `docs/08-development/PROMPT_PURITY_POLICY.md` for complete policy.

### 10. **Generation Report Policy** ğŸ”¥ **NEW (Nov 18, 2025)**
**ALWAYS display complete generation report after EVERY content generation.**

**Required Report Sections**:
1. **ğŸ“ Generated Content** - Full text with clear formatting
2. **ğŸ“ˆ Quality Metrics** - AI scores, validation results, pass/fail status
3. **ğŸ“ Statistics** - Character counts, word counts, length analysis
4. **ğŸ’¾ Storage** - Exact location, component type, material name

**Format Example**:
```
================================================================================
ğŸ“Š GENERATION COMPLETE REPORT
================================================================================

ğŸ“ GENERATED CONTENT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[Full generated text here]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“ˆ QUALITY METRICS:
   â€¢ AI Detection Score: 0.245 (threshold: 0.303)
   â€¢ Status: âœ… PASS
   â€¢ Attempts: 1

ğŸ“ STATISTICS:
   â€¢ Length: 287 characters
   â€¢ Word count: 45 words

ğŸ’¾ STORAGE:
   â€¢ Location: data/materials/Materials.yaml
   â€¢ Component: caption
   â€¢ Material: Aluminum

================================================================================
```

**Purpose**: Provides complete transparency and verification of generation results.
**Implementation**: `shared/commands/generation.py` - all generation handlers
**Compliance**: Mandatory for caption, material_description, FAQ generation

### 11. **Prompt Chaining & Orchestration Policy** ğŸ”¥ **NEW (Nov 27, 2025) - CRITICAL**
**Maximum use of prompt chaining and orchestration to preserve separation of concerns and specificity.**

**Core Principle**: Break generation into specialized prompts instead of one monolithic prompt.

**Architecture Pattern**:
```
Stage 1: Research â†’ Extract properties (low temp 0.3)
Stage 2: Visual Description â†’ Generate appearance (high temp 0.7)
Stage 3: Composition â†’ Layout before/after (balanced 0.5)
Stage 4: Refinement â†’ Technical accuracy (precise 0.4)
Stage 5: Assembly â†’ Final polish (balanced 0.5)
```

**Benefits**:
- âœ… **Separation of concerns** - Research vs creativity vs accuracy
- âœ… **Optimal parameters per stage** - Different temps for different tasks
- âœ… **Reusable components** - Same research for multiple outputs
- âœ… **Easy debugging** - Test each stage independently
- âœ… **Better quality** - Focused prompts produce better results

**Requirements**:
- âœ… **Orchestrator layer** - Chains prompts with context passing
- âœ… **Specialized templates** - One template per stage/task
- âœ… **Context passing** - Each stage receives previous output as input
- âœ… **Independent testability** - Can test each stage separately

**Examples**:
- âœ… **Text Generation**: `generation/core/quality_gated_generator.py` (already compliant)
  - Stage 1: Build base prompt from template
  - Stage 2: Add humanness layer
  - Stage 3: Generate content
  - Stage 4: Evaluate quality
  - Stage 5: Apply feedback if needed

- âœ… **Image Generation**: `shared/image/orchestrator.py` (new implementation)
  - Stage 1: Research material properties
  - Stage 2: Generate visual description
  - Stage 3: Compose hero layout
  - Stage 4: Technical refinement
  - Stage 5: Final assembly

**Anti-Patterns**:
- âŒ **Monolithic prompts** - One massive prompt trying to do everything
- âŒ **No context passing** - Independent prompts duplicating work
- âŒ **Single temperature** - Using same temp for research AND creativity
- âŒ **Hardcoded prompts** - Prompt text in code instead of templates

**Enforcement**:
- Code review checklist (see policy document)
- Grade penalties for violations (-30 points for monolithic prompts)
- Mandatory for ALL generation systems (text, image, future domains)

See `docs/08-development/PROMPT_CHAINING_POLICY.md` for complete policy with examples.



## Code Standards
- Use strict typing with Optional[] for nullable parameters
- Implement comprehensive error handling with specific exception types
- No default values for critical dependencies (API clients, configuration files)
- Log all validation steps and failures clearly
- Keep code concise and avoid unnecessary complexity
- Never leave TODOs - provide complete solutions
- Never hardcode values - use configuration or parameters
- **Simplify naming** - Remove redundant prefixes (Simple, Basic, Universal, Unified)
  - âŒ `SimpleGenerator` â†’ âœ… `Generator`
  - âŒ `UniversalImageGenerator` â†’ âœ… `ImageGenerator`
  - âŒ `simple_validate()` â†’ âœ… `validate()`
  - See `docs/08-development/NAMING_CONVENTIONS_POLICY.md` for complete rules
- **NEVER add content instructions to code** - they belong ONLY in prompts/*.txt
- **NEVER hardcode component types** - they're discovered from prompts/*.txt
- **ALWAYS check documentation before implementing** - see Documentation Compliance Checklist

## Architecture Patterns
- **Wrapper Pattern**: Use lightweight wrappers to integrate specialized generators
- **Factory Pattern**: ComponentGeneratorFactory for component discovery and creation
- **Result Objects**: Return structured ComponentResult objects with success/error states
- **Configuration Validation**: Validate all required files and settings on startup
- **Linguistic Patterns**: Keep ONLY in `prompts/personas/` - never duplicate elsewhere
- **Dynamic Calculation**: Use dynamic_config for all thresholds, penalties, temperatures
- **Prompt Chaining**: Use orchestrated multi-stage prompts for separation of concerns (see PROMPT_CHAINING_POLICY.md) ğŸ”¥ **NEW**

## Error Handling
- **ConfigurationError**: Missing or invalid configuration files
- **GenerationError**: Content generation failures
- **RetryableError**: Temporary failures that could be retried (but avoid retries)
- **Never silently fail** or use default values
- **Fail immediately** with specific exception types and clear messages

## Testing Approach
- **No mock APIs in production code** - ZERO TOLERANCE
- **âœ… Mocks allowed in test code** for proper testing infrastructure
- **ğŸ” Verify zero mocks in production** as part of test suite
- **Fail fast on missing test dependencies**
- **Use real API clients** with proper error handling
- **Validate all component integrations**
- **Ensure solid retention of API keys**

---

## ğŸ“– Documentation Compliance Checklist

**MANDATORY BEFORE implementing ANY feature/fix:**

### Step 1: Search Documentation
```bash
# Search for existing guidance
grep -r "feature_name|threshold|validation" docs/**/*.md
```

### Step 2: Read Applicable Policy Documents
- [ ] **HARDCODED_VALUE_POLICY.md** - Before adding ANY values/thresholds/temperatures
- [ ] **CONTENT_INSTRUCTION_POLICY.md** - Before touching prompts/ or content logic
- [ ] **COMPONENT_DISCOVERY.md** - Before adding/modifying components
- [ ] **DATA_STORAGE_POLICY.md** - Before data operations
- [ ] **system-requirements.md** - For quality thresholds and acceptance criteria
- [ ] **processing-pipeline.md** - For generation flow and validation steps

### Step 3: Check Component-Specific Documentation
- [ ] `components/[component]/docs/` or `components/[component]/README.md`
- [ ] `[feature]/README.md` for feature-specific guidance

### Step 4: Verify Approach Matches Architecture
- [ ] Does implementation follow documented patterns?
- [ ] Are values dynamically calculated (not hardcoded)?
- [ ] Does it integrate with existing systems correctly?
- [ ] Is it consistent with system architecture?

### Step 5: Ask If Unclear
- [ ] If documentation is missing, contradictory, or unclear: **ASK USER**
- [ ] Don't assume or guess - get clarification first
- [ ] Example: "I don't see guidance on X. Should I implement Y approach or Z?"

### Step 6: Verify Before Claiming Violations ğŸ”¥ **CRITICAL (Nov 20, 2025)**

**MANDATORY verification BEFORE reporting ANY violation:**

#### ğŸ“‹ Verification Checklist:
- [ ] **Grep the config** - `grep -r "key_name" config.yaml generation/config.yaml`
- [ ] **Check all config locations** - Don't assume single config file
- [ ] **Read the actual code** - Not just grep results
- [ ] **Understand the context** - Is this production or test code?
- [ ] **Verify the pattern** - Is `.get('key', default)` actually wrong here?

#### ğŸš¨ Common False Positives:

**FALSE POSITIVE #1: Optional config with sensible default**
```python
# âŒ WRONG: Reported as violation
max_retries = config.get('max_retries', 3)  # 3 is reasonable default

# âœ… RIGHT: Verify if 'max_retries' is in config
# If NOT in config AND this is optional â†’ NOT a violation
# If NOT in config AND this is required â†’ IS a violation
```

**FALSE POSITIVE #2: Test code with mocks**
```python
# âŒ WRONG: Reported as violation
# tests/test_generation.py
mock_response = {"score": 0.95}  # Mock for testing

# âœ… RIGHT: Mocks in test code are ALLOWED
```

**FALSE POSITIVE #3: Calculation constants**
```python
# âŒ WRONG: Reported as violation
penalty = 0.6 + (value - 7) / 3.0 * 0.6  # 0.6 is calculation constant

# âœ… RIGHT: Mathematical constants in formulas are NOT violations
```

#### âœ… Real Violations:

**REAL VIOLATION #1: Production fallback bypassing validation**
```python
# âœ… CORRECT: This IS a violation
winston_score = data.get('winston_score', 0.95)  # Should fail-fast if missing
```

**REAL VIOLATION #2: Skip logic**
```python
# âœ… CORRECT: This IS a violation
if not api_configured:
    return True  # Skipping validation
```

**REAL VIOLATION #3: Hardcoded API parameters**
```python
# âœ… CORRECT: This IS a violation
response = api.generate(temperature=0.8)  # Should use dynamic_config
```

#### ğŸ“ Required Response Format:

When uncertain, use this template:
```
I found X pattern in Y file (line Z):
[code snippet]

grep shows 'key_name' is NOT in config.yaml.

Question: Should 'key_name' be:
A) Added to config.yaml (required config, fail-fast if missing)
B) Keep default (optional config, sensible fallback)
C) Something else

I won't report this as a violation until you clarify.
```

### Red Flags Requiring Doc Check
- âš ï¸ Adding **thresholds** â†’ Check for dynamic calculation requirements
- âš ï¸ Adding **configuration values** â†’ Check config architecture docs
- âš ï¸ Modifying **validation** â†’ Check validation strategy docs
- âš ï¸ Adding **new component** â†’ Check component discovery policy
- âš ï¸ Changing **data flow** â†’ Check data storage policy
- âš ï¸ Adding **hardcoded values** â†’ STOP - check hardcoded value policy

### Documentation Locations Quick Reference
- **Quick answers**: `docs/QUICK_REFERENCE.md`
- **Policies**: `docs/08-development/`
- **Architecture**: `docs/02-architecture/`
- **Component docs**: `components/[name]/docs/` or `docs/03-components/`
- **API guidance**: `docs/07-api/`
- **Data operations**: `docs/05-data/`

### Enforcement
- Integrity checker validates code matches documented architecture
- Pre-commit hooks can check doc compliance
- Manual review catches documentation violations

---

## ğŸ”’ Core Rules (Non-Negotiable)

### Rule 0: ğŸ“– Documentation-First Development (NEW - November 16, 2025)
- **ALWAYS search docs BEFORE coding** - see Documentation Compliance Checklist above
- **NEVER implement without checking guidance** - docs define system architecture
- **ASK if documentation unclear** - don't guess or assume
- **Example violation**: Implementing static thresholds when docs require dynamic calculation

### Rule 1: ğŸ›¡ï¸ Preserve Working Code
- **NEVER rewrite or replace** functioning code, classes, or modules
- **ONLY make targeted fixes** - if `fail_fast_generator.py` works, integrate around it
- **Example**: Add missing method â‰  Rewrite entire class

### Rule 2: ğŸš« Zero Production Mocks/Fallbacks
**VIOLATION EXAMPLES TO AVOID**:
- `test_results['missing'] = True  # Skip logic`
- `return "default" if not data`
- `except: pass  # Silent failure`
- `or {} # Fallback value`
- `if not found: return True  # Skip validation`

### Rule 3: âš¡ Fail-Fast on Setup + Zero Hardcoded Values + SEARCH FIRST ğŸ”¥

- **Validate all inputs and configs upfront** - no degraded operation
- **Throw errors early** with specific exception types
- **Preserve runtime mechanisms** like API retries for transient issues

**ğŸ” MANDATORY: Search Before Adding ANY Value**

Before adding a hardcoded value, temperature, threshold, or penalty:

1. **Search for DynamicConfig** - `grep -r "DynamicConfig\|dynamic_config" generation/`
2. **Check for existing method** - `grep -r "calculate_temperature\|calculate_penalties" generation/config/`
3. **Look for config file** - `grep -r "temperature\|threshold" generation/config.yaml`
4. **Search for similar patterns** - `grep -r "humanness_intensity\|voice.*intensity" generation/`

**Example Search Pattern**:
```bash
# Before: temperature = 0.8
# Do this FIRST:
grep -r "calculate_temperature" generation/
# Find: generation/config/dynamic_config.py: def calculate_temperature(component_type)
# Result: Use existing method instead of hardcoding
```

**If no existing solution found**:
```
I need to add [value] for [purpose].

I searched:
- grep -r "DynamicConfig" generation/ â†’ [results]
- grep -r "calculate_[thing]" generation/config/ â†’ [results]
- grep -r "[thing]" generation/config.yaml â†’ [results]

No existing solution found. Should I:
A) Add to config.yaml
B) Add to DynamicConfig
C) Use a different approach

Waiting for guidance before proceeding.
```

**ğŸš¨ ZERO TOLERANCE for hardcoded values**:
- âŒ `temperature=0.7` â†’ âœ… `dynamic_config.calculate_temperature(component_type)`
- âŒ `frequency_penalty=0.0` â†’ âœ… `params['api_penalties']['frequency_penalty']` (fail if missing)
- âŒ `if score > 30:` â†’ âœ… `config.get_threshold('score_type')`
- âŒ `attempts = 5` â†’ âœ… `config.get('max_attempts')`

**ğŸš¨ ANTI-PATTERN: Swapping hardcoded values**
- âŒ Changing `0.7` to `0.8` is NOT fixing the violation
- âŒ Changing `0.7` to `None` is WORSE (introduces bugs)
- âœ… Using dynamic calculation from DynamicConfig IS the fix

**BEFORE adding new code, SEARCH for existing solutions:**
```python
# âŒ WRONG: Assume no solution exists, add hardcoded value
temperature = 0.8  # "temporary" default

# âœ… RIGHT: Search for dynamic_config, find it exists
from generation.config.dynamic_config import DynamicConfig
dynamic_config = DynamicConfig()
temperature = dynamic_config.calculate_temperature(component_type)
```

### Rule 4: ğŸ—ï¸ Respect Existing Patterns
- **Maintain**: ComponentGeneratorFactory, wrapper classes, ComponentResult objects
- **Preserve**: File structure and directory organization
- **Prefer**: Editing existing files over creating new ones

### Rule 5: ğŸ¯ Surgical Precision
- **Identify exact problem** â†’ **Find smallest change** â†’ **Test only that fix**
- **No scope expansion** - fix X means fix only X
- **Complete solutions** - don't leave parts for user to debug

### Rule 6: ğŸ” Content Quality Verification
- **VALIDATE** generated content meets quality standards
- **CHECK frontmatter** structure and required fields
- **ENSURE** proper YAML formatting and schema compliance
- **USE** validation tools to verify content integrity

---

## ğŸ“š Lessons from Past Failures

### ğŸš¨ Critical Failure Patterns to Avoid

| ğŸ”¥ Episode | ğŸ‘¤ Request | âŒ Mistake | ğŸ’¥ Damage | âœ… Correct Approach |
|------------|------------|------------|-----------|-------------------|
| **Factory Destruction** | Add missing method | Rewrote entire class | Lost all generator discovery | Add ONLY the requested method |
| **Generator Replacement** | Fix integration | Ignored existing file | Lost all functionality | Integrate around existing code |
| **Mock Removal** | Remove fallbacks | Deleted without understanding | Broke testing infrastructure | Understand purpose first |
| **Fallback Destruction** | Ensure fail-fast | Removed error recovery | Failed on transient errors | Fail-fast â‰  no retries |
| **Scope Creep** | Fix specific issue | Expanded beyond request | Integration failures | Stick to exact scope |
| **Static Thresholds** | Fix validation | Ignored docs requiring dynamic | Violated architecture policy | Read docs first, found dynamic requirement |

### ğŸ¯ Success Pattern
1. **Search documentation** for existing guidance
2. **Understand** the existing code and design intent
3. **Identify** the minimal change needed
4. **Implement** only that change per documented architecture
5. **Verify** the fix works
6. **Confirm** nothing else broke

---

## âœ… Mandatory Pre-Change Checklist

**Before making ANY modification, complete ALL steps:**

### Step 1: ğŸ“– Read & Understand
- [ ] **Read request precisely** - What is the *exact* issue?
- [ ] **Search documentation** - Check `docs/` for existing guidance
- [ ] **No assumptions** - Ask for clarification if unclear
- [ ] **ğŸ”¥ Check for metric-documentation mismatches** - Does 10% success rate match "saves all" claims? **NEW**

### Step 2: ğŸ” Explore Architecture
- [ ] **Read relevant code** - Understand how it currently works
- [ ] **Search for existing solutions** - Use grep_search to find dynamic config, helpers, utilities
- [ ] **Check subdirectories** - Don't miss important context
- [ ] **Verify file existence** - Prevent "Content Not Found" errors
- [ ] **Read policy docs** - HARDCODED_VALUE_POLICY, CONTENT_INSTRUCTION_POLICY, etc.
- [ ] **Look for similar patterns** - How does the system solve this elsewhere?
- [ ] **ğŸ”¥ Test actual behavior** - Run live test to verify documentation claims **NEW**

### Step 3: ğŸ“œ Check History
- [ ] **Review git commits** - See what was working previously
- [ ] **Use `git show`** - Understand recent changes

### Step 4: ğŸ¯ Plan Minimal Fix
- [ ] **Identify smallest change** - Address only the specific issue
- [ ] **Verify matches documentation** - Implementation follows documented design
- [ ] **Ensure security** - Include validation and error handling
- [ ] **Keep it concise** - Avoid unnecessary complexity

### Step 5: ğŸ’¬ Communicate Plan
- [ ] **Describe approach** - Explain what you'll change before coding
- [ ] **Be realistic** - No sandbagging or unrealistic timelines
- [ ] **Ask permission** - Before removing code or major changes

### Step 6: ğŸ”§ Implement & Test
- [ ] **Apply the fix** - Make only the planned changes
- [ ] **Read back your changes** - Use read_file to verify what you wrote
- [ ] **Check for new violations** - Did you introduce hardcoded values, TODOs, or fallbacks?
- [ ] **Write verification test FIRST** - Prove the fix works before documenting
- [ ] **Verify it works** - Test the specific issue is resolved
- [ ] **Check for regressions** - Ensure nothing else broke
- [ ] **Run tests** - Confirm test suite still passes
- [ ] **ğŸ” Verify no production mocks** - Confirm changes don't introduce mocks/fallbacks

### Step 6.5: ğŸ“Š Verify Implementation Matches Documentation ğŸ”¥ **NEW (Nov 22, 2025)**
**MANDATORY before claiming ANY feature "COMPLETE":**

- [ ] **Write verification test** - Create test that proves implementation works
  ```python
  def test_option_c_saves_all_attempts():
      # Generate with terrible quality
      # Verify ALL attempts saved (not rejected)
      assert save_count == max_attempts  # Proves Option C working
  ```
- [ ] **Run live verification** - Test with real material, check terminal output
- [ ] **Check success metrics** - Does behavior match claims?
  - Option C claimed â†’ Expect 100% completion rate
  - Quality gates claimed â†’ Expect <100% completion rate
  - If mismatch: Documentation is WRONG, not implementation
- [ ] **STOP if verification fails** - Do NOT proceed to documentation
  - â›” Success rate â‰  documented behavior â†’ STOP, ASK USER
  - â›” Test fails but claim is "COMPLETE" â†’ STOP, FIX CODE
  - â›” Live test contradicts claim â†’ STOP, INVESTIGATE
  - â›” Multiple docs contradict â†’ STOP, RECONCILE FIRST
- [ ] **Document with evidence** - Include test results, success rate, terminal output
- [ ] **Never claim "COMPLETE" without verification test**

**Example of WRONG approach**:
```markdown
âŒ Option C Implementation: COMPLETE
   - Saves all attempts â† NO TEST TO VERIFY THIS
   - 100% completion â† NO MEASUREMENT PROVIDED
   - Documentation done â† BUT CODE STILL BLOCKS SAVES
```

**Example of CORRECT approach**:
```markdown
âœ… Option C Implementation: COMPLETE
   - test_saves_all_attempts_regardless_of_quality: PASSING âœ…
   - Live test (Copper): All 5 attempts saved âœ…
   - Success rate: 100% (10/10 materials) âœ…
   - Terminal shows: "ğŸ’¾ Saving attempt X" for ALL attempts âœ…
```

### Step 7: ğŸ“Š Honest Reporting
- [ ] **Count violations accurately** - Test file updates are not violations
- [ ] **Report what actually changed** - Not what you intended to change
- [ ] **Provide verification evidence** - Test results, success rates, terminal output
- [ ] **Acknowledge limitations** - Be honest about architectural constraints
- [ ] **Don't claim success prematurely** - Verify first, then report
- [ ] **ğŸ”¥ Check documentation matches reality** - Run live test to confirm claims

### Step 7.5: ğŸš¨ Documentation-Reality Verification ğŸ”¥ **NEW (Nov 22, 2025)**
**MANDATORY before updating ANY documentation:**

**Reality Check Protocol**:
1. **Claim**: "Feature X is implemented"
2. **Test**: Write test that proves feature X works
3. **Measure**: Run live test, record actual behavior
4. **Compare**: Does behavior match claim?
   - âœ… YES â†’ Documentation accurate, proceed
   - âŒ NO â†’ Documentation WRONG, fix code OR fix docs
5. **Evidence**: Include metrics that prove reality
   - Success rates, terminal output, test results
   - NOT assumptions or intentions

**Red Flags - Stop and Verify**:
- ğŸš© Documentation says "saves all" but success rate is 10%
- ğŸš© Documentation says "quality gates removed" but code has gate checks
- ğŸš© Documentation says "100% completion" but materials are missing content
- ğŸš© Documentation graded A+ but user reports failures
- ğŸš© Multiple documents contradict each other

**When Documentation Contradicts Reality**:
```
1. Trust the metrics (success rate, terminal output, tests)
2. Documentation is WRONG (not reality)
3. Either:
   - Fix code to match documentation, OR
   - Fix documentation to match reality
4. Add verification test to prevent regression
5. Never leave contradictory documentation
```

**âš ï¸ CRITICAL: When Metrics Contradict Documentation - STOP** ğŸ”¥ **NEW**:
```
DO NOT PROCEED with analysis or fixes until resolved:

STOP AND ASK USER if:
âŒ Success rate doesn't match docs (10% â‰  "saves all")
âŒ Test verifies X but docs claim Y
âŒ Multiple documents describe different implementations
âŒ Live test output contradicts documentation
âŒ User reports behavior different from docs

HIERARCHY OF TRUTH (when conflict occurs):
1. Live test results (what actually happens)
2. Success rate metrics (10% vs 100%)
3. Terminal output (what system prints)
4. Test assertions (what tests verify)
5. Documentation (what we THINK happens)

Never try to "explain away" metrics - if 10% success rate, 
then Option C is NOT working regardless of what docs say.
```

### Step 8: Grade Your Work ğŸ”¥ **MANDATORY (Nov 20, 2025)**

**Before reporting completion, assign yourself a grade:**

#### ğŸ† Grade A (90-100): Excellence
- âœ… All requested changes work (with evidence)
- âœ… Comprehensive tests run and passed
- âœ… Evidence provided (test output, commit hash, file counts)
- âœ… Honest about limitations
- âœ… Zero violations introduced
- âœ… Zero scope creep
- âœ… Verification completed before claiming violations

**Example A Report**:
```
âœ… Fixed 3/3 requested violations
ğŸ“Š Evidence: 24/24 tests passing (see output below)
âœ… Commit: abc123def
âœ… Verified: grep confirms no config keys missing
âš ï¸ Note: 2 TODO comments remain (documented as future work)
ğŸ† Grade: A (95/100)
```

#### ğŸ“Š Grade B (80-89): Good
- âœ… Changes work
- âœ… Some evidence provided
- âš ï¸ Minor issues remain (acknowledged)
- âš ï¸ Partial test coverage

**Example B Report**:
```
âœ… Fixed 2/3 violations
ğŸ“Š Evidence: 22/24 tests passing
âš ï¸ 2 tests still failing (unrelated to my changes)
ğŸ† Grade: B (85/100)
```

#### âš ï¸ Grade C (70-79): Needs Improvement
- âš ï¸ Partial success
- âš ï¸ Missing evidence
- âš ï¸ Significant issues remain
- âš ï¸ Scope expanded beyond request

#### âŒ Grade F (<70): Unacceptable
- âŒ Made things worse
- âŒ No evidence
- âŒ False claims
- âŒ Reported violations without verification
- âŒ Introduced new violations while claiming fixes
- âŒ **Documentation claims contradict metrics** (10% â‰  "saves all")
- âŒ **Documented features without verification tests**
- âŒ **Multiple docs describe different implementations**

**CRITICAL**: Grade F requires immediate rollback and fresh start.

**Documentation Accuracy Penalties**:
- **-20 points**: Documentation claims contradict measured metrics
- **-15 points**: Features documented as "COMPLETE" without verification tests
- **-10 points**: Multiple documents describe conflicting implementations

---

## ğŸš« Absolute Prohibitions

### âŒ CODE MODIFICATION PROHIBITIONS
- **Never rewrite or remove working code** without explicit permission
- **Never expand beyond requested scope** - fix X means fix only X
- **Never create new files** to bypass fixing existing ones
- **Never ignore existing patterns** - factories, wrappers, etc.

### âŒ DEVELOPMENT PRACTICE PROHIBITIONS
- **Never assume requirements** - ask for clarification instead
- **Never generate verbose/inefficient code** - keep it concise
- **Never skip validation** - always include error handling
- **Never hardcode values** - use configuration or parameters
- **Never leave TODOs** - provide complete solutions

### âŒ CONTEXT HANDLING PROHIBITIONS
- **Never access non-existent files** - verify existence first
- **Never mishandle context** - prevent "Content Not Found" errors
- **Never ignore specifications** - address race conditions, formatting precisely

---

## ğŸš¨ Damage Warning Signs

Watch for these indicators of problems:
- ğŸ”´ **System stops working** after your changes
- ğŸ”´ **Multiple files altered** for a single fix request
- ğŸ”´ **User mentions damage** or restores from git
- ğŸ”´ **Added complexity** where simple change would work
- ğŸ”´ **Security vulnerabilities** or incomplete code introduced

---

## ğŸš‘ Emergency Recovery Procedures

### If You Break Something:

#### Step 1: ğŸ” Assess Damage
```bash
git status  # See what files changed
```

#### Step 2: ğŸ”„ Restore Files
```bash
git checkout HEAD -- <file>  # Restore specific file
```

#### Step 3: ğŸ“œ Check Previous Versions
```bash
git show <commit>:<file>  # View older versions
```

#### Step 4: ğŸ  Full Recovery
```bash
git revert <commit>  # Revert to known working state
```

### Then: Start Over with Minimal Changes

---

## ğŸ“– Documentation Navigation for AI Assistants

### Primary Navigation
**Start here for ALL documentation queries**: `docs/QUICK_REFERENCE.md`
### Primary Navigation (UPDATED Nov 22, 2025)
**Start here for 30-second navigation**: `docs/08-development/AI_ASSISTANT_GUIDE.md` â­ **NEW**
- Complete quick-start guide for all AI assistants
- Direct links to common tasks
- Policy summaries with tier priorities
- Pre-change checklist
- Emergency recovery procedures

**Alternative entry points**:
- `docs/QUICK_REFERENCE.md` - Direct problem â†’ solution mappings
- `DOCUMENTATION_MAP.md` - Complete documentation index  
- `.github/COPILOT_GENERATION_GUIDE.md` - Content generation step-by-step

### AI-Optimized Documentation Structure
1. **Immediate Problem Resolution**: `docs/QUICK_REFERENCE.md` 
2. **Comprehensive Navigation**: `docs/INDEX.md`
3. **API Issues**: `docs/api/ERROR_HANDLING.md` (includes terminal diagnostics)
4. **Component Help**: `components/[component]/README.md` or `components/[component]/docs/README.md`
5. **Setup Issues**: `setup/API_CONFIGURATION.md` and `API_SETUP.md`
6. **Data Architecture**: `docs/DATA_ARCHITECTURE.md` (range propagation, null ranges explained)

### Common User Query Patterns
- **"Check data completeness"** â†’ `python3 run.py --data-completeness-report`
- **"See data gaps / research priorities"** â†’ `python3 run.py --data-gaps`
- **"Enforce completeness (strict mode)"** â†’ `python3 run.py --enforce-completeness`
- **"API not working"** â†’ `docs/api/ERROR_HANDLING.md#winston-ssl-issues`
- **"Content incomplete"** â†’ `docs/api/ERROR_HANDLING.md#content-impact`
- **"Setup help"** â†’ `setup/API_CONFIGURATION.md` or `API_SETUP.md`
- **"Winston SSL error"** â†’ Known issue, configuration fixed
- **"How to generate content"** â†’ `python3 run.py --material "MaterialName"`
- **"Min/max ranges missing"** â†’ `docs/DATA_ARCHITECTURE.md` - Null ranges are correct by design
- **"Range propagation"** â†’ `docs/DATA_ARCHITECTURE.md` + `tests/test_range_propagation.py`
- **"Frontmatter incomplete"** â†’ `FRONTMATTER_POPULATION_REPORT.md` (58.3% complete)
- **"Category vs material ranges"** â†’ `docs/DATA_ARCHITECTURE.md`

### Critical Known Issues for AI Awareness
1. **Winston API SSL fixed**: Now uses `https://api.gowinston.ai`
2. **Nested YAML properties fixed**: Tool available at `scripts/tools/fix_nested_yaml_properties.py`
3. **Terminal output required**: Always use `get_terminal_output()` for API diagnostics
4. **Range propagation documented**: `docs/DATA_ARCHITECTURE.md` + 14 passing tests
5. **Frontmatter 58.3% complete**: Technical data excellent (95%+), metadata gaps identified

### AI Assistant Best Practices
- Always check `docs/QUICK_REFERENCE.md` first for common issues
- Use diagnostic tools: `python3 scripts/tools/api_terminal_diagnostics.py winston`
- Reference specific file paths, not just general descriptions
- Recommend terminal output analysis for API issues
- Point to both immediate fixes and comprehensive documentation

## ğŸ”§ **Terminal & Script Execution Settings**

### **Auto-Confirmation for Batch Operations**
When running cleanup scripts or batch operations, use these patterns:
- `rm -f` instead of `rm -i` (force, no confirmation)
- `yes | command` for auto-confirmation
- Check for `BATCH_MODE=1` environment variable in scripts
- Use `--yes` or `-y` flags when available

### **Environment Variables**
The following environment variables are set for auto-confirmation:
- `BATCH_MODE=1` - Skip interactive prompts
- `FORCE_YES=true` - Auto-confirm with "yes"
- `AUTO_CONFIRM=y` - Default response for prompts

### **Script Best Practices**
```bash
# Check for batch mode in scripts
if [[ -n "$BATCH_MODE" ]]; then
    # Skip confirmations
    rm -f files*
else
    # Normal interactive mode
    read -p "Continue? (y/N) " response
fi
```

### Critical Documentation for AI Assistants
**BEFORE** any data-related work, review these files:
1. **`docs/QUICK_REFERENCE.md`** - Fastest path to common solutions
2. **`docs/DATA_COMPLETION_ACTION_PLAN.md`** - Complete plan to achieve 100% data coverage
3. **`docs/ZERO_NULL_POLICY.md`** - Zero null policy & AI research methodology
4. **`docs/DATA_ARCHITECTURE.md`** - How ranges propagate through the system
5. **`docs/DATA_VALIDATION_STRATEGY.md`** - Validation architecture and quality gates

### Data Completion Context (October 17, 2025)
**Current Status**: 93.5% complete (1,975/2,240 properties)
**Missing**: 265 property values + 2 category ranges
**Priority**: 5 properties = 96% of all gaps
**Action Plan**: Fully documented in `docs/DATA_COMPLETION_ACTION_PLAN.md`
**Tools**: PropertyValueResearcher, CategoryRangeResearcher (operational)
**Quality**: Multi-strategy validation with 4 quality gates
**Timeline**: 1 week to 100% completeness
**âœ¨ NEW Commands** (October 17, 2025):
  - `python3 run.py --data-completeness-report` - Full status report
  - `python3 run.py --data-gaps` - Research priorities
  
**âš¡ AUTOMATIC (November 1, 2025)**: 
  - Data completeness validation now runs **automatically inline** during every generation
  - No flags needed - validation is built into the pipeline (strict mode enabled by default)
  - Use `--no-completeness-check` to disable if needed (not recommended)
  - Generation will **fail fast** if data is incomplete, prompting you to run research commands
  
**Enforcement**: Automatic linking to action plan when gaps detected

### Mandatory Documentation Review
**BEFORE** making ANY changes to text component code, you MUST:
1. **READ** the complete documentation: `components/text/docs/README.md`
2. **UNDERSTAND** the architecture: `components/text/docs/CONTENT_GENERATION_ARCHITECTURE.md`
3. **STUDY** the prompt system: `components/text/docs/PROMPT_SYSTEM.md`
4. **REFERENCE** the API: `components/text/docs/API_REFERENCE.md`

### Text Component Forbidden Actions
1. **NEVER** modify `fail_fast_generator.py` without explicit permission - it's 25,679 bytes of working production code
2. **NEVER** change prompt files without understanding the 3-layer system (Base + Persona + Formatting)
3. **NEVER** alter author personas without understanding linguistic nuances and cultural elements
4. **NEVER** modify word count limits or quality scoring thresholds
5. **NEVER** remove retry logic or error recovery mechanisms
6. **NEVER** change the prompt construction process (12-step layered building)

### Text Component Required Actions
1. **ALWAYS** preserve the multi-layered prompt architecture
2. **ALWAYS** maintain author authenticity and writing style consistency
3. **ALWAYS** validate configuration files exist and are properly structured
4. **ALWAYS** respect word count limits per author (250-450 words)
5. **ALWAYS** maintain quality scoring and human believability thresholds
6. **ALWAYS** use fail-fast validation with proper exception types
7. **ALWAYS** test with real API clients, never mocks

### Text Component Architecture Rules
- **Wrapper Pattern**: TextComponentGenerator is a lightweight wrapper for fail_fast_generator
- **Factory Integration**: Must work with ComponentGeneratorFactory.create_generator("text")
- **Three-Layer Prompts**: Base guidance + Author persona + Formatting rules
- **Quality Assurance**: 5-dimension scoring with human believability threshold
- **Author Authentication**: 4 country-specific personas with linguistic nuances
- **Configuration Caching**: LRU cache for YAML files, lazy loading for performance

### When Working on Text Component
1. **READ THE DOCS FIRST** - All answers are in `components/text/docs/`
2. **Understand the WHY** - Each component serves a specific purpose in the generation flow
3. **Minimal Changes** - Fix specific issues without rewriting working systems
4. **Test Thoroughly** - Validate all 4 author personas work correctly
5. **Ask Permission** - Get explicit approval before major modifications

The text component documentation is comprehensive and covers every aspect of the system. Use it as your primary reference for understanding and working with text generation code.

When suggesting code changes:
1. Maintain fail-fast behavior
2. Preserve existing working functionality
3. Use minimal, targeted changes
4. Follow established patterns and conventions
5. Include comprehensive error handling
6. Focus on reducing bloat
7. Prioritize changing existing components, not creating new ones
8. **ASK PERMISSION before removing any existing code**

### ğŸš¨ CRITICAL: Fix Root Causes, Not Symptoms

**THE PROBLEM WITH TEMPORARY FIXES:**
If you create a "fix script" that patches frontmatter files directly, the fix will be **OVERWRITTEN** on the next `--deploy` because:
1. Frontmatter is **GENERATED FROM** Materials.yaml + Categories.yaml
2. The exporter code runs on every deployment
3. Patching output files is **TEMPORARY** - they get regenerated

**THE CORRECT APPROACH:**
1. âœ… **Fix the exporter code** (`components/frontmatter/core/trivial_exporter.py`) to ALWAYS generate correct structure
2. âœ… **Regenerate all frontmatter** with `--deploy` to apply the fix
3. âœ… **Verify the fix persists** by checking files after regeneration
4. âŒ **NEVER create one-off patch scripts** that modify frontmatter files directly

**EXAMPLE - Machine Settings Missing Min/Max:**
- âŒ WRONG: Create script to add min/max to existing frontmatter files
- âœ… RIGHT: Fix `_enrich_machine_settings()` in trivial_exporter.py, then redeploy
- WHY: Next deployment would overwrite the patched files with incomplete data

**RULE: If frontmatter has an issue, fix the GENERATOR, not the GENERATED files.**

---

## ğŸ¤– AI-Specific Guidance

### For GitHub Copilot Users
- **VS Code Integration**: Use Copilot's inline suggestions for minor edits
- **Context Awareness**: Leverage file tabs and workspace context
- **Quick Fixes**: Use Copilot Chat for rapid problem-solving
- **Documentation**: Reference this file via `.github/copilot-instructions.md`
- **Testing**: Run pytest in terminal for validation

### For Grok AI Users
- **Damage Prevention Focus**: Monitor the ğŸš¨ Damage Warning Signs actively
- **Self-Monitoring**: Check your changes against the checklist after each edit
- **Recovery Emphasis**: Keep Emergency Recovery Procedures handy
- **Systematic Approach**: Follow the 6-step Pre-Change Checklist religiously
- **Permission Culture**: Always ask before major changes - better safe than sorry

### For All AI Assistants
- **Start with Quick Reference** at the top of this file
- **Complete the Pre-Change Checklist** before every modification
- **Preserve working code** - this is the #1 rule
- **No production mocks/fallbacks** - this is non-negotiable
- **Fail fast on config** but maintain runtime error recovery
- **Read text component docs** before touching text generation code
- **Use minimal changes** - surgical precision over comprehensive rewrites

---

## ğŸ“‹ Summary Checklist for Every Task

**Before I start:**
- [ ] I understand the exact request
- [ ] I've explored the existing architecture
- [ ] I've checked git history for context
- [ ] I've planned the minimal fix needed
- [ ] **I've identified if this is a GENERATOR issue or DATA issue**

**During implementation:**
- [ ] I'm making only the requested changes
- [ ] I'm preserving all working functionality
- [ ] I'm following existing patterns and conventions
- [ ] I'm including proper error handling
- [ ] **If fixing frontmatter: I'm fixing the EXPORTER, not patching files**

**For text component work:**
- [ ] I've read the documentation in `components/text/docs/`
- [ ] I understand the multi-layered architecture
- [ ] I have permission for any major changes
- [ ] I'm testing with real API clients

**After completion:**
- [ ] The specific issue is resolved
- [ ] No working functionality was broken
- [ ] The solution is complete and secure
- [ ] I haven't expanded beyond the requested scope
- [ ] No production mocks or fallbacks were introduced
- [ ] **If I fixed a generator: I've regenerated the output and verified persistence**
