# AI Assistant Instructions for Z-Beam Generator

**For**: GitHub Copilot, Grok AI, Claude, and all AI development assistants  
**System**: Laser cleaning content generation with strict fail-fast architecture  
**Last Updated**: November 22, 2025

---

## üöÄ **30-SECOND QUICK START** üî• **NEW (Nov 22, 2025)**

**‚ö° PRIORITY**: Read this section FIRST for immediate navigation, then consult detailed rules below.

### **üìñ Full Navigation Guide**
**Primary Resource**: `docs/08-development/AI_ASSISTANT_GUIDE.md`  
- 30-second navigation to any documentation
- Quick lookup tables for common tasks
- Pre-change checklist
- Emergency recovery procedures

### **üéØ Common Tasks - Direct Links**

| Task | Resource |
|------|----------|
| **Generate content** | `.github/COPILOT_GENERATION_GUIDE.md` (step-by-step) |
| **Fix bugs/add features** | Pre-Change Checklist (see below) + `docs/SYSTEM_INTERACTIONS.md` |
| **Check policy compliance** | `docs/08-development/` (HARDCODED_VALUE_POLICY, TERMINAL_LOGGING_POLICY, etc.) |
| **Prompt chaining/orchestration** | `docs/08-development/PROMPT_CHAINING_POLICY.md` üî• **NEW** |
| **Naming conventions** | `docs/08-development/NAMING_CONVENTIONS_POLICY.md` üî• **NEW** |
| **Understand data flow** | `docs/02-architecture/processing-pipeline.md` |
| **Find quick answers** | `docs/QUICK_REFERENCE.md` |
| **Troubleshoot errors** | `TROUBLESHOOTING.md` (root) |
| **Research implementation history** | `docs/archive/2025-11/` (52 archived docs) |

### **üö¶ Critical Policies Summary**

**TIER 1: System-Breaking** (Will cause failures)
- ‚ùå NO mocks/fallbacks in production code (tests OK)
- ‚ùå NO hardcoded values/defaults (use config/dynamic calc)
- ‚ùå NO rewriting working code (minimal surgical fixes only)

**TIER 2: Quality-Critical** (Will cause bugs)
- ‚ùå NO expanding scope (fix X means fix ONLY X)
- ‚úÖ ALWAYS fail-fast on config (throw exceptions)
- ‚úÖ ALWAYS log to terminal (comprehensive dual logging)

**TIER 3: Evidence & Honesty** (Will lose trust)
- ‚úÖ ALWAYS provide evidence (test output, commits)
- ‚úÖ ALWAYS be honest (acknowledge limitations)
- üî• NEVER report success when quality gates fail

**Full Details**: See TIER PRIORITIES section below + `docs/08-development/`

### **üìã Quick Pre-Change Checklist**

Before ANY code change:
1. [ ] Search `docs/QUICK_REFERENCE.md` for existing guidance
2. [ ] Check `docs/08-development/` for relevant policy
3. [ ] Review `docs/SYSTEM_INTERACTIONS.md` for side effects
4. [ ] Plan minimal fix (one sentence description)
5. [ ] Verify all file paths exist before coding
6. [ ] Ask permission before major changes or rewrites

---

## üñºÔ∏è **IMAGE FEEDBACK CAPTURE PROTOCOL** üî• **NEW (Nov 29, 2025)**

**Purpose**: Automatically detect and capture user feedback on generated images for learning.

### **Explicit Trigger (Option B)**
User can use `/feedback` command for certainty:
```
/feedback Steel: rotation angle too small, both sides look identical
/feedback Aluminum contamination: oil looks painted on, not realistic
```

**When user uses `/feedback`, IMMEDIATELY run:**
```bash
python3 domains/materials/image/tools/add_feedback.py -m <material> -c <category> -f "<feedback_text>"
```

**Categories**: `physics`, `rotation`, `contamination`, `object`, `background`, `composition`, `realism`, `text`, `other`

### **Automatic Detection (Option C)**
When user comments on a generated image WITHOUT `/feedback`, detect these patterns:

**üö® TRIGGER PHRASES** (offer to log feedback):
- "rotation is wrong/too small/not enough"
- "contamination looks wrong/thick/fake/painted"
- "background is wrong/white/studio"
- "object looks wrong/different/not the same"
- "unrealistic/fake looking/AI-looking"
- "text in image/watermark/label"
- "shadows wrong/lighting wrong"
- "not rotated/same angle"

**When detected, respond with:**
```
üìù I noticed feedback about the image. Should I log this to the learning database?

Detected:
- Material: [inferred or ask]
- Category: [inferred: rotation/contamination/etc]
- Feedback: "[user's comment]"

Reply 'yes' to log, or provide corrections.
```

### **After Logging, Confirm:**
```
‚úÖ Feedback logged to learning database:
   Material: Steel
   Category: rotation
   Feedback: "rotation angle too small, both sides look identical"
   
This will help improve future generations. View all feedback:
   python3 domains/materials/image/tools/add_feedback.py --list
```

### **Integration with Generation**
When generating a new image for a material that has feedback:
1. Check SQLite for manual feedback on that material
2. Mention: "Note: Previous feedback for Steel includes rotation issues"
3. Consider applying corrections proactively

### **Feedback Integration Policy** üî• **NEW (Nov 30, 2025) - CRITICAL**

**Feedback MUST be analyzed and consolidated into appropriate prompt sections, NOT just concatenated.**

**PROHIBITED Approach** (Grade F):
```
‚ùå Append feedback to CORRECTIONS section
‚ùå Add more text to already-long prompts
‚ùå Let optimizer truncate important feedback
‚ùå Create feedback files that just get concatenated
```

**REQUIRED Approach** (Grade A):
```
‚úÖ ANALYZE feedback to identify root cause location in prompt chain
‚úÖ MODIFY the source template where the issue originates
‚úÖ CONSOLIDATE feedback into existing prompt sections
‚úÖ REWRITE sections if needed to integrate modifications
‚úÖ REMOVE contradicting text when adding corrections
```

**Feedback Processing Steps**:
1. **Trace the prompt chain** - Find all templates that touch the issue
2. **Identify root location** - Which template is the source of the problem?
3. **Check for contradictions** - Does existing text contradict the feedback?
4. **Modify at source** - Edit the template, don't add another layer
5. **Verify integration** - Run dry-run to confirm feedback appears in final prompt
6. **Check prompt length** - Ensure changes don't exceed limits

**Example - "Glass contamination too thick"**:
```
‚ùå WRONG: Add "glass contamination should be thin" to user_corrections.txt
‚úÖ RIGHT: 
   1. Find contamination_rules.txt (source of contamination description)
   2. Find base_structure.txt (describes contamination appearance)
   3. Modify BOTH to specify thin/uniform for glass
   4. Create glass_corrections.txt for glass-specific overrides
   5. Remove "chunky" or "thick" language if present
```

**Prompt Length Management**:
- If prompt exceeds 4096 chars after integration, REWRITE sections to be more concise
- Prioritize: Material-specific rules > Generic rules > Examples
- Never let optimizer truncate critical feedback

### **Feedback Analytics**
```bash
# View recent feedback
python3 domains/materials/image/tools/add_feedback.py --list

# Search feedback
python3 domains/materials/image/tools/add_feedback.py --search "rotation"

# Statistics by category
python3 domains/materials/image/tools/add_feedback.py --stats

# Via analytics CLI
python3 shared/image/learning/analytics.py --manual-feedback
```

---

## üì¶ File Organization & Root Cleanliness Policy (Nov 28, 2025)

**Purpose:** Ensure all file types (.py, .sh, .log, .txt, etc.) are organized for maintainability and AI assistant workflow compliance. Prevent root clutter and legacy file sprawl.

### üîπ Python Scripts (.py)
- **Batch/utility scripts:** Move to `scripts/` or `tools/`.
- **Runner scripts:** Only keep essential entry points (e.g., `run.py`) in root.
- **Test scripts:** All `test_*.py` files must be in `tests/`.

### üîπ Shell Scripts (.sh)
- **Batch/process scripts:** Move to `scripts/` or `batch/`.
- **Migration/monitoring scripts:** Group by function in subfolders (e.g., `scripts/migration/`, `scripts/monitoring/`).

### üîπ Log Files (.log)
- **Operational logs:** Store in `logs/`, set up `.gitignore` for auto-exclusion.
- **Batch/research logs:** Move to `logs/` or `output/` as appropriate.

### üîπ Text Files (.txt)
- **Progress trackers:** Move to `progress/` or `logs/`.
- **Requirements files:** Keep `requirements.txt` in root or move to `config/` if multiple environments.
- **Coverage lists:** Store in `coverage/` or `tests/`.

### üîπ Markdown Files (.md)
- **Documentation:** Only keep top-level navigation docs in root (e.g., `README.md`, `DOCUMENTATION_MAP.md`).
- **Guides/architecture:** Move to `docs/` or relevant subdirectories.
- **Archived docs:** Store in `docs/archive/`.

### üîπ General Rules
- **No stray files in root:** Only keep essential entry points and navigation docs.
- **Use `.gitignore`:** Exclude logs, outputs, and temporary files.
- **Automate cleanup:** Add scripts to remove old logs and outputs.
- **Document structure:** Update `DOCUMENTATION_MAP.md` to reflect new locations.

### üîπ Maintenance
- **Regular audits:** Periodically check for stray files in root.
- **Enforce structure:** Use pre-commit hooks or CI checks to prevent root clutter.
- **Update navigation docs:** Ensure all links in `DOCUMENTATION_MAP.md` and `.github/copilot-instructions.md` are current.

**Compliance:**
- All file moves and organization must follow this policy.
- AI assistants must enforce root cleanliness before major documentation or workflow changes.
- Any exceptions require explicit approval and documentation.

**Full Checklist**: See "Mandatory Pre-Change Checklist" section below (lines ~300)

---

## üö® **CRITICAL FAILURE PATTERNS TO AVOID** üî• **UPDATED (Nov 22, 2025)**

### **Pattern 0: Documentation Claims Contradicting Code Reality** üî• **NEW - MOST CRITICAL**
**What Happened**: Documentation claimed "Option C implemented - saves all attempts" but code was still blocking saves (10% success rate proved gates were active)
**Why It's Grade F**: Creates architectural confusion, wastes effort on wrong diagnosis, erodes user trust
**Impact**: 
- All Nov 22 analysis based on false assumption (Option C active)
- Wrong root cause identified (blamed Winston thresholds when real issue was gate blocking)
- Wasted time on fixes that couldn't help
**Correct Behavior**:
```
‚úÖ VERIFY implementation with tests BEFORE documenting as complete
‚úÖ Check actual code behavior matches documentation claims
‚úÖ Use success rate as reality check (10% = gates blocking, 100% = Option C working)
‚úÖ Write verification tests: test_saves_all_attempts_regardless_of_quality()
```

**Prevention Checklist**:
- [ ] Does a test verify this claim? (If no, don't claim "COMPLETE")
- [ ] Does actual behavior match documentation? (Run live test)
- [ ] Can I prove it with evidence? (Success rate, terminal output, test results)

### **Pattern 1: Reporting Success When Quality Gates Fail**
**What Happened**: AI reported "‚úÖ Description generated" when realism score was 5.0/10 (threshold: 5.5/10)
**Why It's Grade F**: Bypassed quality control, shipped low-quality content, dishonest reporting
**Correct Behavior**: 
```
‚ùå REJECT if any gate fails:
  - Realism < 5.5/10
  - Winston > threshold
  - Readability FAIL
‚úÖ Only report success when ALL gates pass
‚úÖ Report failures honestly: "Quality gate failed, regenerating..."
```

### **Pattern 2: Not Reading Evaluation Scores Carefully**
**What Happened**: Two evaluations ran (9.0/10, then 5.0/10), AI only noticed the first
**Why It's Grade C**: Incomplete verification, missed critical quality failure
**Correct Behavior**:
```
‚úÖ Read ALL evaluation outputs
‚úÖ Check BOTH pre-save AND post-generation scores
‚úÖ Verify final stored content meets thresholds
‚úÖ Report: "Attempt 1: 9.0/10 ‚úÖ PASS, Attempt 2: 5.0/10 ‚ùå FAIL"
```

### **Pattern 3: Manual Data Fixes Instead of Root Cause**
**What Happened**: Description saved to wrong location (line 1180), AI manually moved it to line 832
**Why It's Grade B**: Workaround instead of fixing the save logic
**Correct Behavior**:
```
‚ùå Don't patch data files
‚úÖ Fix the generator code that saves incorrectly
‚úÖ Regenerate to verify fix persists
‚úÖ Ask: "Should I fix UnifiedMaterialsGenerator.save() logic?"
```

### **Pattern 4: Not Testing Against Actual Quality**
**What Happened**: AI-like phrases detected: "presents a unique challenge", "critical pitfall", formulaic structure
**Why It's Grade D**: Content reads like AI technical manual, not human writing
**Correct Behavior**:
```
‚úÖ Check for AI tell-tale phrases:
  - "presents a [unique/primary/significant] challenge"
  - "[critical/significant/primary] pitfall"  
  - "This [property/balance/approach] is essential for"
  - Formulaic structure (challenge ‚Üí solution ‚Üí importance)
‚úÖ Verify natural human voice
‚úÖ Reject robotic/textbook language
```

### **Pattern 5: Learned Parameters Producing Poor Quality**
**What Happened**: Sweet spot learning stored temp=0.815 but correlation showed temp has NEGATIVE correlation (-0.515)
**Why It's Grade C**: Learning system storing parameters that hurt quality
**Correct Behavior**:
```
‚úÖ Check learned parameter correlations
‚úÖ Question parameters with negative correlation
‚úÖ Verify sweet spot samples include recent high-quality content
‚úÖ Test: Does lower temperature produce better results?
```

### **Pattern 6: Treating Symptoms Instead of Root Causes** üî• **NEW (Nov 22, 2025) - CRITICAL**
**What Happened**: Word counts consistently 20-50% over target (150-194 words vs 50-150 max). AI added stricter prompt instructions 5+ times instead of fixing the actual mechanism.
**Why It's Grade F**: 
- Repeated the same failed approach multiple times
- Added "CRITICAL" warnings to prompts that were already being ignored
- Never addressed the real issue: LLMs don't count words during generation
- Wasted user's time with solutions that couldn't work

**What AI Did Wrong**:
1. ‚ùå Fixed hardcoded "150-450 words" in template (good fix, but insufficient)
2. ‚ùå Added "CRITICAL: Stay within word count" to prompt (ignored by model)
3. ‚ùå Added "DO NOT EXCEED THE MAXIMUM" instruction (also ignored)
4. ‚ùå Added placeholder `[TARGET_LENGTH_DISPLAY]` that was never replaced
5. ‚ùå Never checked if prompt instructions were even being used
6. ‚ùå Never measured actual word counts after each "fix"
7. ‚ùå Never questioned why prompt-only approach kept failing

**Root Cause Ignored**: LLMs generate token-by-token without counting words. Prompt instructions alone cannot enforce strict limits.

**Why max_tokens Won't Work**: Lowering max_tokens causes mid-sentence truncation, creating broken/incomplete content. This is WORSE than being over the word count.

**The Fundamental Truth**:
```
‚ùå Prompt instructions alone: LLMs ignore word counts
‚ùå Strict max_tokens: Causes truncation and broken sentences
‚ùå Post-generation truncation: Also breaks sentences
‚úÖ Reality: Approximate word counts are inherent to LLM architecture
```

**Correct Behavior**:
```
‚úÖ Measure results after EACH fix attempt (don't assume it worked)
‚úÖ When same approach fails 2+ times, question the approach itself
‚úÖ Ask: "Why do prompt instructions keep getting ignored?"
‚úÖ Research: What is ACTUALLY possible with LLM architecture?
‚úÖ Accept architectural limitations: "approximately X words" not strict limits
‚úÖ Be honest with user: "LLMs consistently generate 20-30% over target"
‚úÖ Offer real solutions:
   - Option A: Accept approximate word counts (150-180w for 150w target)
   - Option B: Use quality-gated mode with multiple attempts and selection
   - Option C: Post-generation editing (manual review required)
‚úÖ DO NOT waste time on solutions that can't work (more prompt keywords)
```

**Prevention Checklist**:
- [ ] Did I measure the actual result after my fix?
- [ ] Am I repeating the same approach that already failed?
- [ ] Am I treating a symptom (prompt text) vs architectural limitation?
- [ ] Do I understand what is ACTUALLY POSSIBLE with this technology?
- [ ] Have I been honest about what can't be fixed?

**Red Flags That You're Treating Symptoms**:
- üö© Adding more "CRITICAL" or "IMPORTANT" keywords to prompts
- üö© Making text BOLD or adding emoji to existing instructions
- üö© Rephrasing the same instruction in different words
- üö© Adding duplicate requirements in multiple places
- üö© Not measuring actual results after each change
- üö© Assuming "this time it will work" without architectural change
- üö© Proposing max_tokens limits (causes truncation)
- üö© Proposing post-generation truncation (also causes truncation)

**Grade**: F - Wasting user time with ineffective solutions violates TIER 3 honesty requirements

### **Pattern 7: Architectural Documentation Inconsistency** üî• **NEW (Nov 22, 2025) - CRITICAL**
**What Happened**: Multiple documents claimed different architectures were "COMPLETE":
- `E2E_SYSTEM_ANALYSIS_NOV22_2025.md`: Graded system A+ assuming "Option C" active
- `OPTION_C_IMPLEMENTATION_NOV22_2025.md`: Documented "quality gates removed"
- **Reality**: Code still enforced quality gates (10% success rate proved it)

**Why It's Grade F**: 
- Wastes effort on wrong diagnosis
- User cannot trust documentation
- Future work builds on false assumptions
- Regression goes undetected

**Correct Behavior**:
```
‚úÖ MEASURE actual behavior (success rate, terminal output, test results)
‚úÖ If metrics contradict docs ‚Üí Documentation is WRONG
‚úÖ Write verification tests BEFORE claiming "COMPLETE"
‚úÖ Update ALL related docs when architecture changes
‚úÖ Never grade system A+ without verifying claims with tests
‚úÖ Use correct data access patterns when verifying (dict vs list)
```

**How to Detect This Pattern**:
- üö© Success rate doesn't match documented behavior (10% ‚â† "saves all")
- üö© Terminal output contradicts documentation claims
- üö© User reports failures but docs claim success
- üö© Multiple documents describe different implementations
- üö© High grade (A+) but low success metrics (10%)
- üö© Verification scripts report failure when direct file inspection shows success

**Prevention**:
```
Before documenting as "COMPLETE":
1. Write test: test_[feature]_actually_works()
2. Run live test: python3 run.py --[feature]
3. Measure metrics: Success rate, save count, terminal output
4. Compare: Do metrics match documentation claims?
5. Verify data access patterns match actual structure (dict vs list)
6. If NO ‚Üí Fix code OR fix docs, then retest
7. Only claim "COMPLETE" when test + metrics verify it
```

**‚úÖ RESOLVED (Nov 23, 2025)**: Property research verification issue - materials were successfully researched and populated in Materials.yaml. Initial "NOT FOUND" reports were due to verification script bugs using list iteration on dict structure. Correct access pattern: `data['materials'][material_name]` not `next((m for m in materials...))`.

---

## üìñ **Detailed Navigation for AI Assistants**

**üîç Already checked 30-SECOND QUICK START above?** If not, scroll to top first.

### **User Requests Content Generation?**
‚Üí **READ THIS FIRST**: `.github/COPILOT_GENERATION_GUIDE.md`
- Handles: "Generate material description for Aluminum", "Create caption for Steel", etc.
- Shows: Exact commands to run, terminal output handling, result reporting
- Covers: All component types (material_description, caption, FAQ, settings_description)

### **Need Documentation?**
‚Üí **PRIMARY GUIDE**: `docs/08-development/AI_ASSISTANT_GUIDE.md` - 30-second navigation (NEW)
‚Üí **COMPLETE MAP**: Root `/DOCUMENTATION_MAP.md` - All documentation indexed
‚Üí **QUICK ANSWERS**: `docs/QUICK_REFERENCE.md` - Fastest path to solutions
‚Üí **SYSTEM INDEX**: `docs/INDEX.md` - Comprehensive navigation

### **Working on Code/Architecture?**
‚Üí **READ FIRST**: `docs/SYSTEM_INTERACTIONS.md` - Understand cascading effects before changing anything
‚Üí **THEN CHECK**: `docs/decisions/README.md` - Architecture Decision Records (WHY things work this way)
‚Üí **CHECK POLICIES**: `docs/08-development/` - All development policies and guidelines
‚Üí **Continue below** for comprehensive rules and examples

### **‚ö†Ô∏è Before Making ANY Change**
1. **Check `docs/SYSTEM_INTERACTIONS.md`**: What will your change affect?
2. **Check `docs/decisions/`**: Is there an ADR about this?
3. **Check git history**: Has this been tried and failed before?
4. **Check `docs/08-development/`**: Is there a policy document about this?
5. **Plan minimal fix**: Address only the specific issue

---

## üö¶ **TIER PRIORITIES** - Critical Rules Hierarchy

Understanding rule severity helps prioritize fixes and avoid introducing worse problems.

### üî¥ **TIER 1: SYSTEM-BREAKING** (Will cause failures)
1. ‚ùå **NO mocks/fallbacks in production code** (tests OK) - [Rule #2](#rule-2-zero-production-mocksfallbacks)
2. ‚ùå **NO hardcoded values/defaults** (use config/dynamic calc) - [Rule #3](#rule-3-fail-fast-on-setup--zero-hardcoded-values)
3. ‚ùå **NO rewriting working code** (minimal surgical fixes only) - [Rule #1](#rule-1-preserve-working-code)

### üü° **TIER 2: QUALITY-CRITICAL** (Will cause bugs)
4. ‚ùå **NO expanding scope** (fix X means fix ONLY X) - [Rule #5](#rule-5-surgical-precision)
5. ‚ùå **NO skipping validation** (must test before claiming success) - [Step 6](#step-6-implement--test)
6. ‚úÖ **ALWAYS fail-fast on config** (throw exceptions, no silent degradation) - [Rule #3](#rule-3-fail-fast-on-setup--zero-hardcoded-values)
7. ‚úÖ **ALWAYS preserve runtime recovery** (API retries are correct) - See ADRs
8. ‚úÖ **ALWAYS log to terminal** (all generation attempts, scores, feedback) - See Terminal Output Policy

### üü¢ **TIER 3: EVIDENCE & HONESTY** (Will lose trust)
9. ‚úÖ **ALWAYS provide evidence** (test output, counts, commits) - [Verification Protocol](#mandatory-verify-before-claiming-success)
10. ‚úÖ **ALWAYS be honest** (acknowledge what remains broken) - [Step 7](#step-7-honest-reporting)
11. ‚úÖ **ASK before major changes** (get permission for improvements) - [Rule #1](#rule-1-preserve-working-code)
12. ‚úÖ **VERIFY before claiming violations** (check config files, confirm pattern exists) - [Step 6](#step-6-verify-before-claiming-violations)
13. üî• **NEVER report success when quality gates fail** (realism < 5.5, Winston fail) - [Pattern 1](#critical-failure-patterns-to-avoid)
14. üî• **ALWAYS read ALL evaluation scores** (pre-save AND post-generation) - [Pattern 2](#critical-failure-patterns-to-avoid)
15. üî• **ALWAYS check for AI-like phrases** ("presents a challenge", formulaic structure) - [Pattern 4](#critical-failure-patterns-to-avoid)
16. üî• **ALWAYS verify implementation before documentation** (write tests, measure success rate) - [Pattern 0](#critical-failure-patterns-to-avoid) **NEW**
17. üî• **NEVER document features without evidence** (tests prove it works) - [Step 6.5](#step-6-5-verify-implementation-matches-documentation) **NEW**

**üö® CRITICAL: Reporting success when quality fails is a TIER 3 violation - Grade F.**
**üö® CRITICAL: Documenting unimplemented features as "COMPLETE" is a TIER 3 violation - Grade F.** üî• **NEW**

---

## üö¶ **DECISION TREES** - When in Doubt, Use These

### Decision: Should I use a default value?
```
Is this a config/setup issue?
‚îú‚îÄ YES ‚Üí ‚ùå FAIL FAST (throw ConfigurationError)
‚îî‚îÄ NO ‚Üí Is this a runtime/transient issue?
    ‚îú‚îÄ YES ‚Üí ‚úÖ RETRY with backoff (API timeout, network error)
    ‚îî‚îÄ NO ‚Üí Is this a quality check iteration?
        ‚îú‚îÄ YES ‚Üí ‚úÖ ITERATE (adjust parameters based on feedback)
        ‚îî‚îÄ NO ‚Üí ‚ùå FAIL FAST (programming error)
```

### Decision: Should I rewrite this code?
```
Does the code work correctly?
‚îú‚îÄ YES ‚Üí ‚ùå NO REWRITE (integrate around it, add method, minimal fix)
‚îî‚îÄ NO ‚Üí Is it a small targeted fix?
    ‚îú‚îÄ YES ‚Üí ‚úÖ FIX ONLY broken part
    ‚îî‚îÄ NO ‚Üí ‚ö†Ô∏è ASK PERMISSION (explain why rewrite needed)
```

### Decision: Should I add a hardcoded value?
```
Can I find an existing dynamic solution?
‚îú‚îÄ YES ‚Üí ‚úÖ USE IT (grep_search for DynamicConfig, helpers)
‚îî‚îÄ NO ‚Üí Is this truly a constant?
    ‚îú‚îÄ YES ‚Üí ‚úÖ CONFIG FILE (config.yaml, not code)
    ‚îî‚îÄ NO ‚Üí ‚ö†Ô∏è ASK USER (explain why dynamic solution doesn't exist)
```

---

## üìã **TERMINAL OUTPUT LOGGING POLICY**

**ALL generation operations MUST stream comprehensive output to terminal in real-time.**

**Logging Requirements**:
1. **Stream to stdout/stderr ONLY** - No log files created or saved
2. **Real-time output** - User sees progress as it happens
3. **Attempt Progress** - Every retry with attempt number (e.g., "Attempt 2/5")
4. **Quality Checks** - Winston score, Realism score, thresholds, pass/fail
5. **Feedback Application** - Parameter adjustments between attempts
6. **Learning Activity** - Prompt optimization, pattern learning
7. **Final Report** - Complete generation report (see Generation Report Policy)

**Example Streaming Output**:
```
Attempt 2/5
üå°Ô∏è  Temperature: 0.750 ‚Üí 0.825
üìâ Frequency penalty: 0.20 ‚Üí 0.30
Winston Score: 98.6% human ‚úÖ PASS
Realism Score: 5.0/10 (threshold: 5.5) ‚ùå FAIL
‚úÖ [REALISM FEEDBACK] Parameter adjustments calculated
```

**Implementation**:
- Use `print()` for terminal output (not `logger.info()` to files)
- All subprocess calls inherit stdout/stderr (no capture)
- Batch tests stream directly (no tee to log files)

**Purpose**: User visibility, debugging, transparency, verification

**Anti-Patterns**: 
- ‚ùå Silent failures, hidden retries, opaque processing
- ‚ùå Log files in /tmp/ or elsewhere
- ‚ùå Capturing output without displaying it

---

## üìã **IMAGE GENERATION MONITORING POLICY** üî• **NEW (Nov 30, 2025)**

**AI assistants MUST actively monitor image generation terminal output for bottlenecks.**

**Monitoring Requirements**:
1. **Watch for slow stages** - If any stage takes >30 seconds without output, investigate
2. **Identify API timeouts** - Note if Imagen or Gemini calls exceed expected times
3. **Report hanging operations** - If no output for 60+ seconds, alert user
4. **Never truncate output** - Always show FULL terminal output during generation

**Expected Timings** (flag if exceeded):
- Contamination pattern loading: <1 second
- Assembly research (cached): <1 second  
- Assembly research (API): 5-15 seconds
- Imagen generation: 15-45 seconds
- Validation: 5-15 seconds
- **Total expected**: 30-90 seconds

**Required Stage Logging**:
```
üî¨ MATERIAL IMAGE GENERATION: [Material]
üìä Configuration: [settings]
üî¨ Researching contamination data...
üìã Selected [N] patterns for [Material]
üîß Researching assembly components...
üé® ATTEMPT [N]/[MAX] - Generating image...
‚úÖ Image saved to: [path] ([size] KB)
üîç Validating image with Gemini Vision...
üìä VALIDATION RESULTS: [score, status]
```

**AI Assistant Responsibilities**:
- Run with full output (no truncation)
- Monitor actively for stalls
- Report issues immediately if hung
- Ask about hangs after 2 minutes without progress

**Documentation**: `docs/08-development/IMAGE_GENERATION_MONITORING_POLICY.md`

---

## üö´ **NO AUTO-REGENERATION POLICY** üî• **NEW (Nov 30, 2025) - CRITICAL**

**AI assistants MUST NOT automatically retry or regenerate images after a failed attempt.**

**Policy Requirements**:
1. **ONE attempt only** - Generate once, report results, STOP
2. **Wait for user instruction** - Do NOT retry unless user explicitly requests it
3. **Report and wait** - Show validation score, recommendations, then ASK user what to do next
4. **No "let me try again"** - Even if score is close to threshold, do NOT auto-retry

**After ANY image generation (pass or fail)**:
```
‚úÖ CORRECT: "Score: 75/100. Validation failed. Would you like me to retry with different parameters?"
‚ùå WRONG: "Score: 75/100. Let me try again with heavier contamination..." [auto-generates]
```

**Rationale**:
- Each generation costs API credits
- User may want to adjust parameters manually
- User may want to review output before deciding
- Prevents runaway retry loops

**Exceptions** (ONLY when user explicitly says):
- "retry", "try again", "regenerate"
- "keep trying until it passes"
- "run batch generation" (batch mode has its own retry logic)

**Grade**: F violation if auto-regenerating without explicit user instruction

---

## üìã **TERMINAL OUTPUT LOGGING POLICY** üî• **NEW (Nov 22, 2025) - CRITICAL**

**ALL generation operations MUST stream comprehensive output to terminal in real-time.**

**Logging Requirements**:
1. **Stream to stdout using print()** - Always visible to user, not just logger.info()
2. **Real-time output** - User sees progress as it happens
3. **Dual logging** - Both print() (terminal) AND logger.info() (file records)
4. **Attempt Progress** - Every retry with attempt number (e.g., "Attempt 2/5")
5. **Quality Checks** - Winston score, Realism score, thresholds, pass/fail
6. **Parameter Adjustments** - Changes between attempts
7. **Learning Activity** - Database logging, pattern learning
8. **Final Report** - Complete generation report (see Generation Report Policy)
9. **FULL NON-TRUNCATED OUTPUT** üî• **CRITICAL** - NEVER use tail, head, or truncation

**Required Terminal Output**:
```
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
üìù ATTEMPT 2/5
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
üå°Ô∏è  Current Parameters:
   ‚Ä¢ temperature: 0.825
   ‚Ä¢ frequency_penalty: 0.30

üß† Generating humanness instructions (strictness level 2/5)...
   ‚úÖ Humanness layer generated (1234 chars)

‚úÖ Generated: 287 characters, 45 words

üîç Pre-flight: Checking for forbidden phrases...
   ‚úÖ No forbidden phrases detected

üîç Evaluating quality BEFORE save...

üìä QUALITY SCORES:
   ‚Ä¢ Overall Realism: 8.5/10
   ‚Ä¢ Voice Authenticity: 8.0/10
   ‚Ä¢ Tonal Consistency: 7.5/10
   ‚Ä¢ AI Tendencies: None detected

üìâ ADAPTIVE THRESHOLD: 5.2/10 (relaxed from 5.5 for attempt 2)

   üìä Logged attempt 2 to database (detection_id=779, passed=False)

‚ö†Ô∏è  QUALITY GATE FAILED - Will retry with adjusted parameters
   ‚Ä¢ Realism score too low: 5.0/10 < 5.5/10

üîß Adjusting parameters for attempt 3...
   ‚úÖ Parameters adjusted for retry

üîÑ Parameter changes for next attempt:
   ‚Ä¢ temperature: 0.825 ‚Üí 0.900
   ‚Ä¢ frequency_penalty: 0.30 ‚Üí 0.40
```

**Implementation Pattern**:
```python
# Terminal output (always visible)
print(f"üìä QUALITY SCORES:")
print(f"   ‚Ä¢ Overall Realism: {score:.1f}/10")

# File logging (for records)
logger.info(f"üìä QUALITY SCORES:")
logger.info(f"   ‚Ä¢ Overall Realism: {score:.1f}/10")
```

**Documentation**: `docs/08-development/TERMINAL_LOGGING_POLICY.md`
**Tests**: `tests/test_terminal_logging_policy.py` - 12 tests verify compliance
**Enforcement**: All generation operations must use dual logging (print + logger)

**Anti-Patterns**: 
- ‚ùå Silent operations (logger.info() only, no print())
- ‚ùå Hidden retries (no terminal visibility)
- ‚ùå Batch output at end (should stream in real-time)
- ‚ùå Log files only (user can't see what's happening)
- ‚ùå **TRUNCATED OUTPUT** (tail -n, head -n, or any output limiting) üî• **CRITICAL**

**Grade**: MANDATORY - Non-compliance is a policy violation

---

## üõ°Ô∏è **MANDATORY: Pre-Code Execution Protocol** üî• **UPDATED (Nov 22, 2025)**

**üõë STOP - Complete ALL checks BEFORE writing any code:**

**üìñ Quick Reference**: See `docs/08-development/AI_ASSISTANT_GUIDE.md` for streamlined checklist

### ‚úÖ Phase 1: Verification (2-3 minutes)
- [ ] **Read request word-by-word** - What EXACTLY is being asked?
- [ ] **Check for assumptions** - Am I assuming anything not stated?
- [ ] **Verify file paths** - Do all referenced files actually exist?
- [ ] **Check config keys** - Do claimed violations actually exist in config files?
- [ ] **Search for existing solutions** - Does DynamicConfig/helper already solve this?

### ‚úÖ Phase 2: Research (3-5 minutes)
- [ ] **grep_search for patterns** - How does the system currently handle this?
- [ ] **Read relevant code** - Understand current implementation
- [ ] **Check git history** - Was this tried before? Why was it changed?
- [ ] **Review docs/** - Is there policy documentation on this?
- [ ] **Check ADRs** - Is there an architectural decision about this?

### ‚úÖ Phase 3: Planning (2-3 minutes)
- [ ] **Identify exact change needed** - One sentence description
- [ ] **Confirm minimal scope** - Am I fixing ONLY what was requested?
- [ ] **Check for side effects** - What else might this affect?
- [ ] **Plan validation** - How will I prove it works?
- [ ] **Get permission if major** - Ask before removing/rewriting code

### üö® **STOP SIGNALS** - When to ASK instead of CODE:
- ‚ùì If you're not 100% certain about the requirement
- ‚ùì If you can't find the config key/file/pattern being referenced
- ‚ùì If fixing this requires changing more than 3 files
- ‚ùì If you're about to add a hardcoded value without finding dynamic config first
- ‚ùì If the request conflicts with existing architecture
- ‚ùì If tests are failing and you don't understand why

**‚è±Ô∏è Time Investment**: 7-11 minutes of research prevents hours of fixing broken code.

---

## üéØ Quick Reference Card

**READ THIS FIRST - BEFORE ANY CHANGE:**

1. ‚úÖ **Read the request precisely** - What is the *exact* issue?
2. ‚úÖ **Search documentation FIRST** - Check `docs/` for existing guidance (see Documentation Compliance Checklist below)
3. ‚úÖ **Explore existing architecture** - Understand how it currently works
4. ‚úÖ **Check git history for context** - See what was working previously
5. ‚úÖ **Plan minimal fix only** - Address only the specific issue
6. ‚úÖ **Ask permission for major changes** - Get approval before removing code or rewrites
7. ‚úÖ **üî• Verify with tests BEFORE claiming complete** - Evidence over assumptions

**GOLDEN RULES:**
- üö´ **NEVER rewrite working code**
- üö´ **NEVER expand beyond requested scope**
- üö´ **NEVER use mocks/fallbacks in production code - NO EXCEPTIONS**
- ‚úÖ **ALLOW mocks/fallbacks in test code for proper testing**
- üö´ **NEVER add "skip" logic or dummy test results**
- üö´ **NEVER put content instructions in /processing folder code**
- üö´ **NEVER hardcode component types in /processing code**
- üö´ **NEVER hardcode values in production code** - use config or dynamic calculation
- üö´ **NEVER claim "COMPLETE" without verification tests** üî• **NEW**
- üö´ **NEVER document features as implemented without evidence** üî• **NEW**
- ‚úÖ **ALWAYS keep content instructions ONLY in prompts/*.txt files**
- ‚úÖ **ALWAYS define components ONLY in prompts/*.txt and config.yaml**
- ‚úÖ **ALWAYS preserve existing patterns**
- ‚úÖ **ALWAYS fail-fast on configuration issues**
- ‚úÖ **ALWAYS maintain runtime error recovery**
- ‚úÖ **ALWAYS verify documentation matches reality with tests** üî• **NEW**
- ‚úÖ **ALWAYS sync Materials.yaml updates to frontmatter (dual-write)** üî• **NEW (Nov 22, 2025)**

---

## üìö Recent Critical Updates (November 2025)

### üöÄ Learning System Improvements (November 22, 2025) üî• **NEW**
**Status**: ‚úÖ Priority 1 & 2 COMPLETE - System producing 50x more learning data

**Problem Solved**: Quality gates blocked 90% of content from learning system, creating "quality-learning death spiral"
**Solution**: Multi-phase approach enabling learning while maintaining quality standards

**Implementations**:
1. **Priority 1 - Log ALL Attempts** (‚úÖ COMPLETE):
   - Added `_log_attempt_for_learning()` method (~160 lines)
   - Logs EVERY attempt BEFORE quality gate decision (not just successes)
   - Result: **50x more learning data** (5 attempts/material vs 0.1 success/material)
   - Database captures: Winston scores, subjective evaluation, structural diversity, ALL parameters
   - Enables correlation analysis and sweet spot improvements

2. **Priority 2 - Adaptive Threshold Relaxation** (‚úÖ COMPLETE):
   - Added `_get_adaptive_threshold()` method with graduated relaxation
   - Thresholds: Attempt 1 (5.5/10) ‚Üí 2 (5.3) ‚Üí 3 (5.0) ‚Üí 4 (4.8) ‚Üí 5 (4.5)
   - Result: Expected **29% ‚Üí 50-70% success rate**
   - Maintains quality floor (4.5/10 minimum) while reducing 70% waste
   - Verified working: Terminal shows "üìâ ADAPTIVE THRESHOLD: 5.2/10 (relaxed from 5.5 for attempt 2)"

**Future Priorities** (Ready for implementation):
- Priority 3: Opening pattern cooldown (5 hours) - Reduce 10/10 repetition ‚Üí 2/10
- Priority 4: Correlation filter (3 hours) - Exclude negative correlation parameters
- Priority 5: Two-phase strategy (6 hours) - Exploration ‚Üí exploitation approach

**Documentation**: 
- `LEARNING_IMPROVEMENTS_NOV22_2025.md` - Complete implementation guide
- `LEARNING_SYSTEM_ANALYSIS_NOV22_2025.md` - Original analysis and recommendations
- `PRIORITY1_COMPLETE_NOV22_2025.md` - Priority 1 detailed documentation

**Policy Compliance**: 100% compliant - fail-fast architecture, zero hardcoded values, template-only
**Grade**: Priority 1: A+ (100/100), Priority 2: A (95/100)

### üö® Mock/Fallback Violations Eliminated (November 20, 2025) üî• **CRITICAL**
**Status**: ‚úÖ FIXED - 26 violations eliminated, 24/24 tests passing

**Discovery**: Batch test revealed Winston API unconfigured but system continuing with fake scores (100% human, 0% AI)
**Grade**: Grade F violation of GROK_QUICK_REF.md TIER 3: Evidence & Honesty

**Violations Fixed**:
1. **generation.py**: Silent Winston failure ‚Üí RuntimeError (fail-fast)
2. **generation.py**: Hardcoded temperature=0.7 ‚Üí None (metadata only)
3. **constants.py**: Removed DEFAULT_AI_SCORE, DEFAULT_HUMAN_SCORE, DEFAULT_FALLBACK_AI_SCORE
4. **batch_generator.py**: 13 violations - removed skip logic, fallback scores, mock data, hardcoded penalties
5. **run.py**: Marked --skip-integrity-check as [DEV ONLY] with warnings
6. **integrity_helper.py**: 3 violations - fixed silent failures on exceptions
7. **subtitle_generator.py**: Removed hardcoded temperature (0.6), now uses dynamic config
8. **quality_gated_generator.py**: Removed TODO, documented design decision
9. **threshold_manager.py**: 2 TODOs ‚Üí documented as future work with design rationale
10. **test_score_normalization_e2e.py**: Updated tests to remove assertions on deleted constants

**Enforcement**:
- System now raises RuntimeError if Winston API unavailable
- All `.get('score', default)` patterns replaced with fail-fast None checks
- No fallback scores available - validation REQUIRED
- Skip flags marked DEV ONLY with explicit warnings

**Documentation**: 
- `VIOLATION_FIXES_NOV20_2025.md` - Complete fix documentation with before/after code
- `TEST_RESULTS_NOV20_2025.md` - Test execution before violations discovered

**Policy Compliance**: 100% compliant with Core Principle #2 (No Mocks/Fallbacks)
**Grade**: System upgraded from Grade F to A+ (100/100) after complete remediation

### ‚úÖ Learned Evaluation Pipeline Integration (November 18, 2025) üî• **NEW**
**Status**: ‚úÖ IMPLEMENTED AND TESTED (17/17 tests passing)

**What**: Complete pipeline for template-based evaluation with continuous learning
**Components**:
- `prompts/evaluation/subjective_quality.txt` - Template for evaluation prompts (no hardcoded prompts in code)
- `prompts/evaluation/learned_patterns.yaml` - Auto-updating learned patterns from evaluations
- `processing/learning/subjective_pattern_learner.py` - Learning system with exponential moving averages
- Integration: SubjectiveEvaluator loads templates, generator updates patterns after each evaluation

**Learning Flow**:
1. Content generated
2. Evaluator loads template + learned patterns from files
3. Grok evaluates content
4. Pattern learner updates YAML (rejection patterns: AI tendencies, theatrical phrases)
5. If accepted: Pattern learner updates success patterns (EMA with alpha=0.1)
6. Next generation uses updated patterns

**Files Changed**:
- NEW: `prompts/evaluation/subjective_quality.txt` (template)
- NEW: `prompts/evaluation/learned_patterns.yaml` (learning data)
- NEW: `processing/learning/subjective_pattern_learner.py` (learner)
- NEW: `tests/test_learned_evaluation_pipeline.py` (17 tests ‚úÖ)
- MODIFIED: `processing/subjective/evaluator.py` (template integration)
- MODIFIED: `processing/generator.py` (learning integration)

**Documentation**: 
- `LEARNED_EVALUATION_INTEGRATION_NOV18_2025.md` - Complete implementation summary
- `docs/08-development/LEARNED_EVALUATION_PROPOSAL.md` - Architecture (now IMPLEMENTED)

**Policy Compliance**:
- ‚úÖ Prompt Purity Policy: Zero hardcoded prompts in evaluator code
- ‚úÖ Fail-Fast Architecture: Template missing ‚Üí FileNotFoundError
- ‚úÖ Learning Integration: Works with Winston, Realism, Composite scoring

**Grade**: A+ (100/100) - Full implementation, all tests passing

### ‚úÖ Priority 1 Compliance Fixes (November 17, 2025)
**Commit**: c5aa1d6c - All critical violations resolved

1. **RealismOptimizer Import Fixed**: Corrected path from `processing.realism.optimizer` to `processing.learning.realism_optimizer`
2. **SubjectiveEvaluator Temperature**: Now configurable via parameter (no hardcoded values)
3. **Fail-Fast Architecture Enforced**: Removed non-existent fallback method calls

**Documentation**: 
- `docs/archive/2025-11/E2E_PROCESSING_EVALUATION_NOV17_2025.md` - Full evaluation report
- `docs/archive/2025-11/PRIORITY1_UPDATES_COMPLETE.md` - Implementation summary
- `tests/test_priority1_fixes.py` - 10 automated tests (all passing ‚úÖ)

**Grade**: System upgraded from C+ to B+ (85/100) after fixes

### üéØ Prompt Purity Policy (November 18, 2025) üî• **CRITICAL**
**Issue**: Prompt instructions hardcoded in generator code (orchestrator.py, generator.py)
**Fix**: All content instructions MUST exist ONLY in prompts/*.txt files
**Violations Found**: 5 critical violations (system_prompt hardcoding, inline CRITICAL RULE text)
**Policy**: ZERO prompt text permitted in generators - use _load_prompt_template() only
**Documentation**: docs/08-development/PROMPT_PURITY_POLICY.md

### üéØ Realism Quality Gate Enforcement (November 18, 2025) üî• **CRITICAL**
**Issue**: Subjective evaluation was running but NOT rejecting low-quality content
**Fix**: Realism score (7.0/10 minimum) now enforced as quality gate
**Impact**: Content with AI issues (theatrical phrases, casual language) now REJECTED
**Learning**: Both Winston and Realism feedback drive parameter adjustments on retry

### üéØ Composite Quality Scoring (November 16, 2025)
**Architecture**: GENERIC_LEARNING_ARCHITECTURE.md implemented
- Winston (40%) + Realism (60%) weighting for combined score
- Realism gate: 7.0/10 minimum threshold (enforced)
- Adaptive threshold learning from 75th percentile of successful content
- Sweet spot analyzer uses composite scores for parameter optimization

### üó£Ô∏è Content Instruction Policy
**CRITICAL**: Content instructions ONLY in `prompts/*.txt` files, NEVER in code
- Format rules, style guidance, focus areas ‚Üí prompts/
- Technical mechanisms only ‚Üí processing/
- See: `docs/prompts/CONTENT_INSTRUCTION_POLICY.md`

---

## üìñ Core Principles

### 1. **No Mocks or Fallbacks in Production Code**
System must fail immediately if dependencies are missing. **ZERO TOLERANCE** for:
- MockAPIClient or mock responses in production
- Default values that bypass validation (`or "default"`)
- Skip logic that bypasses checks (`if not exists: return True`)
- Placeholder return values (`return {}`)
- Silent failures (`except: pass`)
- **Category fallback ranges** (`if prop missing: use category_range`)
- **Template fallbacks** (`if data missing: use template`)

**‚úÖ EXCEPTION**: Mocks and fallbacks **ARE ALLOWED in test code** for proper testing infrastructure.

**üîç TESTING REQUIREMENT**: Part of testing should include verifying ZERO presence of mocks and fallbacks in production code.

### 2. **No Hardcoded Values in Production Code** üî• **NEW POLICY**
All configuration values MUST come from config files or dynamic calculation. **ZERO TOLERANCE** for:
- **Hardcoded API penalties** (`frequency_penalty=0.0`, `presence_penalty=0.5`)
- **Hardcoded thresholds** (`if score > 30:`, `threshold = 0.7`)
- **Hardcoded temperatures** (`temperature = 0.8`)
- **Hardcoded defaults** (`.get('key', 0.0)`, `or {}` in production paths)
- **Magic numbers** (`attempts = 5`, `max_length = 100`)

**‚úÖ CORRECT APPROACH**:
- Use `config.get_temperature()` not `temperature = 0.8`
- Use `dynamic_config.calculate_penalties()` not `frequency_penalty = 0.0`
- Use `config.get_threshold()` not `if score > 30:`
- Fail fast if config missing, don't use defaults

**üîç ENFORCEMENT**: Integrity checker automatically detects hardcoded values in production code.

### 3. **Explicit Dependencies**
All required components must be explicitly provided - no silent degradation.

### 3. **Data Storage Policy** üî• **CRITICAL**
**ALL generation and validation happens on Materials.yaml ONLY.**

- ‚úÖ **Materials.yaml** - Single source of truth + all generation/validation happens here
  - ALL AI text generation (captions, descriptions, etc.)
  - ALL property research and discovery
  - ALL completeness validation
  - ALL quality scoring and thresholds
  - ALL schema validation
- ‚úÖ **Frontmatter files** - Receive immediate partial field updates (dual-write)
  - Automatic sync when Materials.yaml updated
  - Only changed field written (others preserved)
  - Never read for data persistence
- ‚úÖ **Categories.yaml** - Single source of truth for category ranges
- ‚ùå **Frontmatter files** - Never read for data persistence (write-only mirror)
  - Simple field-level sync from Materials.yaml
  - Should take milliseconds per update
  - No complex operations during sync
- ‚úÖ **Data Flow**: Generate ‚Üí Materials.yaml (full write) + Frontmatter (field sync)
- ‚úÖ **Persistence**: All AI research saves to Materials.yaml immediately
- ‚úÖ **Dual-Write**: Every Materials.yaml update triggers frontmatter field sync

### üö® **MANDATORY: Field Isolation During Generation** üî• **NEW (Nov 22, 2025)**
**Component generation flags (--description, --caption, etc.) MUST ONLY update the specified field.**

- ‚úÖ `--description` ‚Üí Updates ONLY description field (preserves caption, faq, author, etc.)
- ‚úÖ `--caption` ‚Üí Updates ONLY caption field (preserves description, faq, etc.)
- ‚úÖ `--faq` ‚Üí Updates ONLY faq field (preserves description, caption, etc.)
- ‚ùå **VIOLATION**: Overwriting ANY unrelated field during component generation

**Enforcement**: 15 automated tests verify field isolation (`tests/test_frontmatter_partial_field_sync.py`)

See `docs/data/DATA_STORAGE_POLICY.md` for complete policy.

### 4. **Contaminant Appearance Data Policy** üî• **UPDATED (Dec 1, 2025)**
**All contaminant visual appearance data MUST be pre-populated in `Contaminants.yaml`.**

**üö® MANDATORY: Contaminants.yaml is the ONLY Source of Truth for Image Generation**

**STRICT REQUIREMENTS**:
1. ‚úÖ **ALL contamination patterns MUST come from Contaminants.yaml** - NO fallbacks, NO defaults
2. ‚úÖ **Material matching MUST use intelligent lookup** - "Titanium Alloy (Ti-6Al-4V)" matches "Titanium"
3. ‚úÖ **EVERY pattern used MUST have valid_materials entry** - No arbitrary pattern selection
4. ‚úÖ **Pattern selection MUST be logged** - Show exactly which patterns were selected and why
5. ‚ùå **NO category-only fallbacks** - If a material has no patterns in valid_materials, FAIL

**üö® MANDATORY SELECTION POLICIES** (Dec 1, 2025) üî• **NEW**:
1. ‚úÖ **ALWAYS select 3-5 contaminants** - Default is 4, enforced range 3-5
2. ‚úÖ **Context is NOT a factor in selection** - Ignore indoor/outdoor/industrial/marine
3. ‚úÖ **Select most common contaminants for that material** - Based on:
   - Patterns with rich appearance data (highest priority)
   - Patterns explicitly listing this material (not just ALL)
   - Commonality score (how frequently this contamination appears on this material)

**Selection Priority** (Dec 1, 2025):
```
1. Rich appearance data (Format B with 16 fields): +200 points
2. Simple appearance data (Format A): +100 points  
3. Material explicitly listed (not ALL): +50 points
4. Commonality score from Contaminants.yaml: +score points
5. Priority weight multiplier: score √ó weight
```

**INTELLIGENT MATERIAL MATCHING** (Fixed Dec 1, 2025):
- ‚úÖ Exact match: "Titanium" matches "Titanium"
- ‚úÖ Base material in name: "Titanium" matches "Titanium Alloy (Ti-6Al-4V)"
- ‚úÖ Alloy variants: "Steel" matches "Stainless Steel 316"
- ‚ùå WRONG: Only exact string matching (breaks alloy materials)

**SIMPLIFIED ARCHITECTURE**:
- ‚úÖ **Single class: `ContaminationPatternSelector`** - ONLY source for contamination data
- ‚úÖ **ZERO API calls** - Reads from Contaminants.yaml only (no Gemini calls for contamination)
- ‚úÖ **Pattern selection by valid_materials field** - Deterministic, reproducible
- ‚úÖ **Priority scoring** - Prefers patterns with rich appearance data
- ‚ùå **Deprecated**: CategoryContaminationResearcher, MaterialContaminationResearcher, ContaminantAppearanceLoader

**VERIFICATION REQUIREMENT**:
When generating images, the terminal MUST show:
```
üìã Selected 4 patterns for [Material]: ['pattern-1', 'pattern-2', 'pattern-3', 'pattern-4']
üìä [Material] ([context]): 4 patterns selected, 4 with rich data, N aging patterns
```

Note: Pattern count should always be 3-5 (default: 4). Context is logged but NOT used for selection.

If you see "using metal fallback" or "No direct contamination patterns", the system is NOT working correctly.

**üö® MANDATORY TERMINAL OUTPUT POLICY** (Dec 1, 2025):
During EVERY image generation, the terminal MUST display a detailed contamination report:
```
======================================================================
üß™ CONTAMINATION PATTERNS FOR: [Material Name]
======================================================================
   Context: [industrial/outdoor/indoor/marine]
   Patterns requested: [N]
   Patterns selected: [N]
   Rich appearance data: [N]/[N]

   üìã SELECTED CONTAMINATION PATTERNS:

   1. [Pattern Name] ([pattern-id])
      ‚úÖ Rich data | Category: [contamination/aging]
      Colors: [color list]
      Texture: [texture description]...
      Realism: [realism notes if available]...

   2. [Pattern Name] ([pattern-id])
      ...
======================================================================
```
This output is MANDATORY to verify that contaminants are being properly evaluated from Contaminants.yaml.

**CONTAMINANT WEIGHTING** (Dec 1, 2025):
Contaminant weighting can be applied at two levels:
1. **Context-level** (in `context_settings` of Contaminants.yaml):
   - `aging_weight`: Boost/reduce aging patterns (e.g., 1.5 for outdoor)
   - `contamination_weight`: Boost/reduce contamination patterns
2. **Per-pattern level** (future enhancement):
   - Add `priority_weight` field to individual patterns in Contaminants.yaml
   - Higher weight = more likely to be selected

**Only Remaining API Call**: Shape research (optional) - What object form is most common

```python
# Single source of truth
from domains.materials.image.research.contamination_pattern_selector import (
    ContaminationPatternSelector
)
selector = ContaminationPatternSelector()
result = selector.get_patterns_for_image_gen("Aluminum", num_patterns=3)
# result['api_calls_made'] == 0  # ALWAYS zero for contamination
```

**Coverage Status** (Nov 29, 2025):
- 100 contamination patterns
- 159 materials
- 15,900 expected combinations
- ~4% currently populated (652/15,900)

**Populate Missing Data**:
```bash
python3 scripts/research/batch_visual_appearance_research.py --all
```

See `docs/05-data/CONTAMINANT_APPEARANCE_POLICY.md` for complete policy.
**Enforcement**: 16 automated tests in `tests/domains/materials/image/test_contamination_pattern_selector.py`

### 5. **Component Architecture**
Use ComponentGeneratorFactory pattern for all generators.

### 6. **Fail-Fast Design with Quality Gates**
- ‚úÖ **What it IS**: Validate inputs, configurations, and dependencies immediately at startup
- ‚úÖ **What it IS**: Throw specific exceptions (ConfigurationError, GenerationError) with clear messages
- ‚úÖ **What it IS**: Enforce quality gates (Winston 69%+, Realism 7.0+, Readability pass)
- ‚ùå **What it's NOT**: Removing runtime error recovery like API retries for transient issues

**Quality Gates (ALL must pass)**:
1. Winston AI Detection: 69%+ human score (configurable via humanness_intensity, currently at level 7)
2. Readability Check: Pass status
3. Subjective Language: No violations
4. **Realism Score: 7.0/10 minimum** ‚Üê NEW (Nov 18, 2025)
5. Combined Quality Target: Meets learning target

### 7. **Content Instruction Policy** üî• **CRITICAL**
**Content instructions MUST ONLY exist in prompts/*.txt files.**

- ‚úÖ **prompts/*.txt files** - Single source of truth for ALL content instructions
  - Focus areas (what to emphasize)
  - Format rules (structural requirements)
  - Style guidance (voice and tone)
  - Component-specific content strategy
- ‚ùå **processing/*.py files** - ONLY technical mechanisms (NO content instructions)
  - Word count calculations
  - Voice parameter application
  - API integration
  - Quality validation
- üö´ **FORBIDDEN in ComponentSpec**: `format_rules`, `focus_areas`, `style_notes` fields
- üö´ **FORBIDDEN in SPEC_DEFINITIONS**: Content instruction keys
- ‚úÖ **ALLOWED in ComponentSpec**: `name`, `lengths`, `end_punctuation`, `prompt_template_file`
- ‚úÖ **ENFORCEMENT**: 5 automated tests verify policy compliance (see `tests/test_content_instruction_policy.py`)

See `docs/prompts/CONTENT_INSTRUCTION_POLICY.md` for complete policy.

### 8. **Component Discovery Policy** üî• **NEW (Nov 16, 2025)**
**Component types MUST ONLY be defined in prompts/*.txt and config.yaml.**

- ‚úÖ **prompts/*.txt files** - Define component types by filename
  - Create `prompts/caption.txt` to define 'caption' component
  - Create `prompts/material_description.txt` to define 'material_description' component
  - Each .txt file = one component type
- ‚úÖ **config.yaml** - Define component word counts
  ```yaml
  component_lengths:
    caption: 25
    material_description: 15
  ```
- ‚ùå **processing/*.py files** - NO hardcoded component types
  - ‚ùå `if component_type == 'caption':`
  - ‚ùå `SPEC_DEFINITIONS = {'caption': {...}}`
  - ‚ùå Hardcoded component lists
- ‚úÖ **Dynamic Discovery**: Components discovered at runtime from prompts/
- ‚úÖ **Generic Code**: Use `component_type` parameter, iterate `ComponentRegistry.list_types()`
- ‚úÖ **ENFORCEMENT**: Automated tests verify zero hardcoded components

See `docs/architecture/COMPONENT_DISCOVERY.md` for complete policy.

### 9. **Template-Only Policy** üî• **NEW (Nov 18, 2025) - CRITICAL**
**ONLY prompt templates determine content and formatting. NO component-specific methods.**

- ‚úÖ **domains/*/text/prompts/*.txt** - ALL content instructions and formatting rules
  - Structure guidelines, style requirements, forbidden phrases
  - Format specifications, example outputs, voice/tone rules
  - COMPLETE content strategy for each component type
- ‚ùå **processing/*.py** - ZERO component-specific code
  - ‚ùå NO `if component_type == 'caption':` checks
  - ‚ùå NO component-specific methods (`_build_caption_prompt()`, `_extract_caption()`)
  - ‚ùå NO hardcoded content instructions in code
  - ‚ùå NO component-specific extraction logic in generators
- ‚úÖ **Strategy Pattern**: Use `extraction_strategy` in config.yaml
  ```yaml
  component_lengths:
    caption:
      default: 50
      extraction_strategy: before_after  # Strategy-based extraction
    material_description:
      default: 30
      extraction_strategy: raw  # Return text as-is
  ```
- ‚úÖ **Generic Methods**: Use strategy dispatch, not component checks
  - ‚úÖ `adapter.extract_content(text, component_type)` - delegates to strategy
  - ‚úÖ `_load_prompt_template(component_type)` - loads generic template
  - ‚ùå `_extract_caption(text)` - component-specific method
- ‚úÖ **Full Reusability**: /processing works for ANY domain (materials, contaminants, regions)
- ‚úÖ **Zero Code Changes**: Add new component = create template + config entry only

**Adding New Component**:
```bash
# OLD WAY (NON-COMPLIANT): 4 code files + 1 template
1. ‚ùå Edit generator.py - add elif component_type == 'new_component'
2. ‚ùå Edit adapter.py - add _extract_new_component() method
3. ‚ùå Edit prompt_builder.py - add _build_new_component_prompt()
4. ‚ùå Add content instructions to code

# NEW WAY (COMPLIANT): 1 config + 1 template = ZERO CODE CHANGES
1. ‚úÖ Create domains/{domain}/text/prompts/new_component.txt (all instructions)
2. ‚úÖ Add to config.yaml: component_lengths: { new_component: {default: 100, extraction_strategy: raw} }
```

See `docs/08-development/TEMPLATE_ONLY_POLICY.md` for complete policy.

### 10. **Prompt Purity Policy** üî• **NEW (Nov 18, 2025)**
**ALL content generation instructions MUST exist ONLY in prompt template files.**

- ‚úÖ **prompts/*.txt files** - Single source of truth for ALL prompts
  - System prompts, content rules, style guidance
  - Voice/tone instructions, format requirements
  - Forbidden phrases, required elements
- ‚ùå **processing/*.py files** - ZERO prompt text permitted (NO EXCEPTIONS)
  - ‚ùå `system_prompt = "You are a professional technical writer..."`
  - ‚ùå `prompt += "\nCRITICAL RULE: Write ONLY..."`
  - ‚ùå `prompt.replace("text", "YOU MUST NOT...")`
  - ‚ùå Inline content instructions of any kind
- ‚úÖ **Generator code** - Load prompts from templates ONLY
  - ‚úÖ `prompt = self._load_prompt_template('caption.txt')`
  - ‚úÖ Technical parameters (temperature, penalties) in code
  - ‚úÖ Data insertion (material names, properties) allowed
- ‚úÖ **ENFORCEMENT**: Automated tests verify zero hardcoded prompts

See `docs/08-development/PROMPT_PURITY_POLICY.md` for complete policy.

### 11. **Generation Report Policy** üî• **NEW (Nov 18, 2025)**
**ALWAYS display complete generation report after EVERY content generation.**

**Required Report Sections**:
1. **üìù Generated Content** - Full text with clear formatting
2. **üìà Quality Metrics** - AI scores, validation results, pass/fail status
3. **üìè Statistics** - Character counts, word counts, length analysis
4. **üíæ Storage** - Exact location, component type, material name

**Format Example**:
```
================================================================================
üìä GENERATION COMPLETE REPORT
================================================================================

üìù GENERATED CONTENT:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
[Full generated text here]
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

üìà QUALITY METRICS:
   ‚Ä¢ AI Detection Score: 0.245 (threshold: 0.303)
   ‚Ä¢ Status: ‚úÖ PASS
   ‚Ä¢ Attempts: 1

üìè STATISTICS:
   ‚Ä¢ Length: 287 characters
   ‚Ä¢ Word count: 45 words

üíæ STORAGE:
   ‚Ä¢ Location: data/materials/Materials.yaml
   ‚Ä¢ Component: caption
   ‚Ä¢ Material: Aluminum

================================================================================
```

**Purpose**: Provides complete transparency and verification of generation results.
**Implementation**: `shared/commands/generation.py` - all generation handlers
**Compliance**: Mandatory for caption, material_description, FAQ generation

### 12. **Prompt Chaining & Orchestration Policy** üî• **NEW (Nov 27, 2025) - CRITICAL**
**Maximum use of prompt chaining and orchestration to preserve separation of concerns and specificity.**

**Core Principle**: Break generation into specialized prompts instead of one monolithic prompt.

**Architecture Pattern**:
```
Stage 1: Research ‚Üí Extract properties (low temp 0.3)
Stage 2: Visual Description ‚Üí Generate appearance (high temp 0.7)
Stage 3: Composition ‚Üí Layout before/after (balanced 0.5)
Stage 4: Refinement ‚Üí Technical accuracy (precise 0.4)
Stage 5: Assembly ‚Üí Final polish (balanced 0.5)
```

**Benefits**:
- ‚úÖ **Separation of concerns** - Research vs creativity vs accuracy
- ‚úÖ **Optimal parameters per stage** - Different temps for different tasks
- ‚úÖ **Reusable components** - Same research for multiple outputs
- ‚úÖ **Easy debugging** - Test each stage independently
- ‚úÖ **Better quality** - Focused prompts produce better results

**Requirements**:
- ‚úÖ **Orchestrator layer** - Chains prompts with context passing
- ‚úÖ **Specialized templates** - One template per stage/task
- ‚úÖ **Context passing** - Each stage receives previous output as input
- ‚úÖ **Independent testability** - Can test each stage separately

**Examples**:
- ‚úÖ **Text Generation**: `generation/core/quality_gated_generator.py` (already compliant)
  - Stage 1: Build base prompt from template
  - Stage 2: Add humanness layer
  - Stage 3: Generate content
  - Stage 4: Evaluate quality
  - Stage 5: Apply feedback if needed

- ‚úÖ **Image Generation**: `shared/image/orchestrator.py` (new implementation)
  - Stage 1: Research material properties
  - Stage 2: Generate visual description
  - Stage 3: Compose hero layout
  - Stage 4: Technical refinement
  - Stage 5: Final assembly

**Anti-Patterns**:
- ‚ùå **Monolithic prompts** - One massive prompt trying to do everything
- ‚ùå **No context passing** - Independent prompts duplicating work
- ‚ùå **Single temperature** - Using same temp for research AND creativity
- ‚ùå **Hardcoded prompts** - Prompt text in code instead of templates

**Enforcement**:
- Code review checklist (see policy document)
- Grade penalties for violations (-30 points for monolithic prompts)
- Mandatory for ALL generation systems (text, image, future domains)

See `docs/08-development/PROMPT_CHAINING_POLICY.md` for complete policy with examples.



## Code Standards
- Use strict typing with Optional[] for nullable parameters
- Implement comprehensive error handling with specific exception types
- No default values for critical dependencies (API clients, configuration files)
- Log all validation steps and failures clearly
- Keep code concise and avoid unnecessary complexity
- Never leave TODOs - provide complete solutions
- Never hardcode values - use configuration or parameters
- **Simplify naming** - Remove redundant prefixes (Simple, Basic, Universal, Unified)
  - ‚ùå `SimpleGenerator` ‚Üí ‚úÖ `Generator`
  - ‚ùå `UniversalImageGenerator` ‚Üí ‚úÖ `ImageGenerator`
  - ‚ùå `simple_validate()` ‚Üí ‚úÖ `validate()`
  - See `docs/08-development/NAMING_CONVENTIONS_POLICY.md` for complete rules
- **NEVER add content instructions to code** - they belong ONLY in prompts/*.txt
- **NEVER hardcode component types** - they're discovered from prompts/*.txt
- **ALWAYS check documentation before implementing** - see Documentation Compliance Checklist

## Architecture Patterns
- **Wrapper Pattern**: Use lightweight wrappers to integrate specialized generators
- **Factory Pattern**: ComponentGeneratorFactory for component discovery and creation
- **Result Objects**: Return structured ComponentResult objects with success/error states
- **Configuration Validation**: Validate all required files and settings on startup
- **Linguistic Patterns**: Keep ONLY in `prompts/personas/` - never duplicate elsewhere
- **Dynamic Calculation**: Use dynamic_config for all thresholds, penalties, temperatures
- **Prompt Chaining**: Use orchestrated multi-stage prompts for separation of concerns (see PROMPT_CHAINING_POLICY.md) üî• **NEW**

## Error Handling
- **ConfigurationError**: Missing or invalid configuration files
- **GenerationError**: Content generation failures
- **RetryableError**: Temporary failures that could be retried (but avoid retries)
- **Never silently fail** or use default values
- **Fail immediately** with specific exception types and clear messages

## Testing Approach
- **No mock APIs in production code** - ZERO TOLERANCE
- **‚úÖ Mocks allowed in test code** for proper testing infrastructure
- **üîç Verify zero mocks in production** as part of test suite
- **Fail fast on missing test dependencies**
- **Use real API clients** with proper error handling
- **Validate all component integrations**
- **Ensure solid retention of API keys**

---

## üìñ Documentation Compliance Checklist

**MANDATORY BEFORE implementing ANY feature/fix:**

### Step 1: Search Documentation
```bash
# Search for existing guidance
grep -r "feature_name|threshold|validation" docs/**/*.md
```

### Step 2: Read Applicable Policy Documents
- [ ] **HARDCODED_VALUE_POLICY.md** - Before adding ANY values/thresholds/temperatures
- [ ] **CONTENT_INSTRUCTION_POLICY.md** - Before touching prompts/ or content logic
- [ ] **COMPONENT_DISCOVERY.md** - Before adding/modifying components
- [ ] **DATA_STORAGE_POLICY.md** - Before data operations
- [ ] **system-requirements.md** - For quality thresholds and acceptance criteria
- [ ] **processing-pipeline.md** - For generation flow and validation steps

### Step 3: Check Component-Specific Documentation
- [ ] `docs/03-components/` for component-specific docs
- [ ] `[feature]/README.md` for feature-specific guidance

### Step 4: Verify Approach Matches Architecture
- [ ] Does implementation follow documented patterns?
- [ ] Are values dynamically calculated (not hardcoded)?
- [ ] Does it integrate with existing systems correctly?
- [ ] Is it consistent with system architecture?

### Step 5: Ask If Unclear
- [ ] If documentation is missing, contradictory, or unclear: **ASK USER**
- [ ] Don't assume or guess - get clarification first
- [ ] Example: "I don't see guidance on X. Should I implement Y approach or Z?"

### Step 6: Verify Before Claiming Violations üî• **CRITICAL (Nov 20, 2025)**

**MANDATORY verification BEFORE reporting ANY violation:**

#### üìã Verification Checklist:
- [ ] **Grep the config** - `grep -r "key_name" config.yaml generation/config.yaml`
- [ ] **Check all config locations** - Don't assume single config file
- [ ] **Read the actual code** - Not just grep results
- [ ] **Understand the context** - Is this production or test code?
- [ ] **Verify the pattern** - Is `.get('key', default)` actually wrong here?

#### üö® Common False Positives:

**FALSE POSITIVE #1: Optional config with sensible default**
```python
# ‚ùå WRONG: Reported as violation
max_retries = config.get('max_retries', 3)  # 3 is reasonable default

# ‚úÖ RIGHT: Verify if 'max_retries' is in config
# If NOT in config AND this is optional ‚Üí NOT a violation
# If NOT in config AND this is required ‚Üí IS a violation
```

**FALSE POSITIVE #2: Test code with mocks**
```python
# ‚ùå WRONG: Reported as violation
# tests/test_generation.py
mock_response = {"score": 0.95}  # Mock for testing

# ‚úÖ RIGHT: Mocks in test code are ALLOWED
```

**FALSE POSITIVE #3: Calculation constants**
```python
# ‚ùå WRONG: Reported as violation
penalty = 0.6 + (value - 7) / 3.0 * 0.6  # 0.6 is calculation constant

# ‚úÖ RIGHT: Mathematical constants in formulas are NOT violations
```

#### ‚úÖ Real Violations:

**REAL VIOLATION #1: Production fallback bypassing validation**
```python
# ‚úÖ CORRECT: This IS a violation
winston_score = data.get('winston_score', 0.95)  # Should fail-fast if missing
```

**REAL VIOLATION #2: Skip logic**
```python
# ‚úÖ CORRECT: This IS a violation
if not api_configured:
    return True  # Skipping validation
```

**REAL VIOLATION #3: Hardcoded API parameters**
```python
# ‚úÖ CORRECT: This IS a violation
response = api.generate(temperature=0.8)  # Should use dynamic_config
```

#### üìù Required Response Format:

When uncertain, use this template:
```
I found X pattern in Y file (line Z):
[code snippet]

grep shows 'key_name' is NOT in config.yaml.

Question: Should 'key_name' be:
A) Added to config.yaml (required config, fail-fast if missing)
B) Keep default (optional config, sensible fallback)
C) Something else

I won't report this as a violation until you clarify.
```

### Red Flags Requiring Doc Check
- ‚ö†Ô∏è Adding **thresholds** ‚Üí Check for dynamic calculation requirements
- ‚ö†Ô∏è Adding **configuration values** ‚Üí Check config architecture docs
- ‚ö†Ô∏è Modifying **validation** ‚Üí Check validation strategy docs
- ‚ö†Ô∏è Adding **new component** ‚Üí Check component discovery policy
- ‚ö†Ô∏è Changing **data flow** ‚Üí Check data storage policy
- ‚ö†Ô∏è Adding **hardcoded values** ‚Üí STOP - check hardcoded value policy

### Documentation Locations Quick Reference
- **Quick answers**: `docs/QUICK_REFERENCE.md`
- **Policies**: `docs/08-development/`
- **Architecture**: `docs/02-architecture/`
- **Component docs**: `docs/03-components/`
- **API guidance**: `docs/07-api/`
- **Data operations**: `docs/05-data/`

### Enforcement
- Integrity checker validates code matches documented architecture
- Pre-commit hooks can check doc compliance
- Manual review catches documentation violations

---

## üîí Core Rules (Non-Negotiable)

### Rule 0: üìñ Documentation-First Development (NEW - November 16, 2025)
- **ALWAYS search docs BEFORE coding** - see Documentation Compliance Checklist above
- **NEVER implement without checking guidance** - docs define system architecture
- **ASK if documentation unclear** - don't guess or assume
- **Example violation**: Implementing static thresholds when docs require dynamic calculation

### Rule 1: üõ°Ô∏è Preserve Working Code
- **NEVER rewrite or replace** functioning code, classes, or modules
- **ONLY make targeted fixes** - if `fail_fast_generator.py` works, integrate around it
- **Example**: Add missing method ‚â† Rewrite entire class

### Rule 2: üö´ Zero Production Mocks/Fallbacks
**VIOLATION EXAMPLES TO AVOID**:
- `test_results['missing'] = True  # Skip logic`
- `return "default" if not data`
- `except: pass  # Silent failure`
- `or {} # Fallback value`
- `if not found: return True  # Skip validation`

### Rule 3: ‚ö° Fail-Fast on Setup + Zero Hardcoded Values + SEARCH FIRST üî•

- **Validate all inputs and configs upfront** - no degraded operation
- **Throw errors early** with specific exception types
- **Preserve runtime mechanisms** like API retries for transient issues

**üîç MANDATORY: Search Before Adding ANY Value**

Before adding a hardcoded value, temperature, threshold, or penalty:

1. **Search for DynamicConfig** - `grep -r "DynamicConfig\|dynamic_config" generation/`
2. **Check for existing method** - `grep -r "calculate_temperature\|calculate_penalties" generation/config/`
3. **Look for config file** - `grep -r "temperature\|threshold" generation/config.yaml`
4. **Search for similar patterns** - `grep -r "humanness_intensity\|voice.*intensity" generation/`

**Example Search Pattern**:
```bash
# Before: temperature = 0.8
# Do this FIRST:
grep -r "calculate_temperature" generation/
# Find: generation/config/dynamic_config.py: def calculate_temperature(component_type)
# Result: Use existing method instead of hardcoding
```

**If no existing solution found**:
```
I need to add [value] for [purpose].

I searched:
- grep -r "DynamicConfig" generation/ ‚Üí [results]
- grep -r "calculate_[thing]" generation/config/ ‚Üí [results]
- grep -r "[thing]" generation/config.yaml ‚Üí [results]

No existing solution found. Should I:
A) Add to config.yaml
B) Add to DynamicConfig
C) Use a different approach

Waiting for guidance before proceeding.
```

**üö® ZERO TOLERANCE for hardcoded values**:
- ‚ùå `temperature=0.7` ‚Üí ‚úÖ `dynamic_config.calculate_temperature(component_type)`
- ‚ùå `frequency_penalty=0.0` ‚Üí ‚úÖ `params['api_penalties']['frequency_penalty']` (fail if missing)
- ‚ùå `if score > 30:` ‚Üí ‚úÖ `config.get_threshold('score_type')`
- ‚ùå `attempts = 5` ‚Üí ‚úÖ `config.get('max_attempts')`

**üö® ANTI-PATTERN: Swapping hardcoded values**
- ‚ùå Changing `0.7` to `0.8` is NOT fixing the violation
- ‚ùå Changing `0.7` to `None` is WORSE (introduces bugs)
- ‚úÖ Using dynamic calculation from DynamicConfig IS the fix

**BEFORE adding new code, SEARCH for existing solutions:**
```python
# ‚ùå WRONG: Assume no solution exists, add hardcoded value
temperature = 0.8  # "temporary" default

# ‚úÖ RIGHT: Search for dynamic_config, find it exists
from generation.config.dynamic_config import DynamicConfig
dynamic_config = DynamicConfig()
temperature = dynamic_config.calculate_temperature(component_type)
```

### Rule 4: üèóÔ∏è Respect Existing Patterns
- **Maintain**: ComponentGeneratorFactory, wrapper classes, ComponentResult objects
- **Preserve**: File structure and directory organization
- **Prefer**: Editing existing files over creating new ones

### Rule 5: üéØ Surgical Precision
- **Identify exact problem** ‚Üí **Find smallest change** ‚Üí **Test only that fix**
- **No scope expansion** - fix X means fix only X
- **Complete solutions** - don't leave parts for user to debug

### Rule 6: üîç Content Quality Verification
- **VALIDATE** generated content meets quality standards
- **CHECK frontmatter** structure and required fields
- **ENSURE** proper YAML formatting and schema compliance
- **USE** validation tools to verify content integrity

---

## üìö Lessons from Past Failures

### üö® Critical Failure Patterns to Avoid

| üî• Episode | üë§ Request | ‚ùå Mistake | üí• Damage | ‚úÖ Correct Approach |
|------------|------------|------------|-----------|-------------------|
| **Factory Destruction** | Add missing method | Rewrote entire class | Lost all generator discovery | Add ONLY the requested method |
| **Generator Replacement** | Fix integration | Ignored existing file | Lost all functionality | Integrate around existing code |
| **Mock Removal** | Remove fallbacks | Deleted without understanding | Broke testing infrastructure | Understand purpose first |
| **Fallback Destruction** | Ensure fail-fast | Removed error recovery | Failed on transient errors | Fail-fast ‚â† no retries |
| **Scope Creep** | Fix specific issue | Expanded beyond request | Integration failures | Stick to exact scope |
| **Static Thresholds** | Fix validation | Ignored docs requiring dynamic | Violated architecture policy | Read docs first, found dynamic requirement |

### üéØ Success Pattern
1. **Search documentation** for existing guidance
2. **Understand** the existing code and design intent
3. **Identify** the minimal change needed
4. **Implement** only that change per documented architecture
5. **Verify** the fix works
6. **Confirm** nothing else broke

---

## ‚úÖ Mandatory Pre-Change Checklist

**Before making ANY modification, complete ALL steps:**

### Step 1: üìñ Read & Understand
- [ ] **Read request precisely** - What is the *exact* issue?
- [ ] **Search documentation** - Check `docs/` for existing guidance
- [ ] **No assumptions** - Ask for clarification if unclear
- [ ] **üî• Check for metric-documentation mismatches** - Does 10% success rate match "saves all" claims? **NEW**

### Step 2: üîç Explore Architecture
- [ ] **Read relevant code** - Understand how it currently works
- [ ] **Search for existing solutions** - Use grep_search to find dynamic config, helpers, utilities
- [ ] **Check subdirectories** - Don't miss important context
- [ ] **Verify file existence** - Prevent "Content Not Found" errors
- [ ] **Read policy docs** - HARDCODED_VALUE_POLICY, CONTENT_INSTRUCTION_POLICY, etc.
- [ ] **Look for similar patterns** - How does the system solve this elsewhere?
- [ ] **üî• Test actual behavior** - Run live test to verify documentation claims **NEW**

### Step 3: üìú Check History
- [ ] **Review git commits** - See what was working previously
- [ ] **Use `git show`** - Understand recent changes

### Step 4: üéØ Plan Minimal Fix
- [ ] **Identify smallest change** - Address only the specific issue
- [ ] **Verify matches documentation** - Implementation follows documented design
- [ ] **Ensure security** - Include validation and error handling
- [ ] **Keep it concise** - Avoid unnecessary complexity

### Step 5: üí¨ Communicate Plan
- [ ] **Describe approach** - Explain what you'll change before coding
- [ ] **Be realistic** - No sandbagging or unrealistic timelines
- [ ] **Ask permission** - Before removing code or major changes

### Step 6: üîß Implement & Test
- [ ] **Apply the fix** - Make only the planned changes
- [ ] **Read back your changes** - Use read_file to verify what you wrote
- [ ] **Check for new violations** - Did you introduce hardcoded values, TODOs, or fallbacks?
- [ ] **Write verification test FIRST** - Prove the fix works before documenting
- [ ] **Verify it works** - Test the specific issue is resolved
- [ ] **Check for regressions** - Ensure nothing else broke
- [ ] **Run tests** - Confirm test suite still passes
- [ ] **üîç Verify no production mocks** - Confirm changes don't introduce mocks/fallbacks

### Step 6.5: üìä Verify Implementation Matches Documentation üî• **NEW (Nov 22, 2025)**
**MANDATORY before claiming ANY feature "COMPLETE":**

- [ ] **Write verification test** - Create test that proves implementation works
  ```python
  def test_option_c_saves_all_attempts():
      # Generate with terrible quality
      # Verify ALL attempts saved (not rejected)
      assert save_count == max_attempts  # Proves Option C working
  ```
- [ ] **Run live verification** - Test with real material, check terminal output
- [ ] **Check success metrics** - Does behavior match claims?
  - Option C claimed ‚Üí Expect 100% completion rate
  - Quality gates claimed ‚Üí Expect <100% completion rate
  - If mismatch: Documentation is WRONG, not implementation
- [ ] **STOP if verification fails** - Do NOT proceed to documentation
  - ‚õî Success rate ‚â† documented behavior ‚Üí STOP, ASK USER
  - ‚õî Test fails but claim is "COMPLETE" ‚Üí STOP, FIX CODE
  - ‚õî Live test contradicts claim ‚Üí STOP, INVESTIGATE
  - ‚õî Multiple docs contradict ‚Üí STOP, RECONCILE FIRST
- [ ] **Document with evidence** - Include test results, success rate, terminal output
- [ ] **Never claim "COMPLETE" without verification test**

**Example of WRONG approach**:
```markdown
‚ùå Option C Implementation: COMPLETE
   - Saves all attempts ‚Üê NO TEST TO VERIFY THIS
   - 100% completion ‚Üê NO MEASUREMENT PROVIDED
   - Documentation done ‚Üê BUT CODE STILL BLOCKS SAVES
```

**Example of CORRECT approach**:
```markdown
‚úÖ Option C Implementation: COMPLETE
   - test_saves_all_attempts_regardless_of_quality: PASSING ‚úÖ
   - Live test (Copper): All 5 attempts saved ‚úÖ
   - Success rate: 100% (10/10 materials) ‚úÖ
   - Terminal shows: "üíæ Saving attempt X" for ALL attempts ‚úÖ
```

### Step 7: üìä Honest Reporting
- [ ] **Count violations accurately** - Test file updates are not violations
- [ ] **Report what actually changed** - Not what you intended to change
- [ ] **Provide verification evidence** - Test results, success rates, terminal output
- [ ] **Acknowledge limitations** - Be honest about architectural constraints
- [ ] **Don't claim success prematurely** - Verify first, then report
- [ ] **üî• Check documentation matches reality** - Run live test to confirm claims

### Step 7.5: üö® Documentation-Reality Verification üî• **NEW (Nov 22, 2025)**
**MANDATORY before updating ANY documentation:**

**Reality Check Protocol**:
1. **Claim**: "Feature X is implemented"
2. **Test**: Write test that proves feature X works
3. **Measure**: Run live test, record actual behavior
4. **Compare**: Does behavior match claim?
   - ‚úÖ YES ‚Üí Documentation accurate, proceed
   - ‚ùå NO ‚Üí Documentation WRONG, fix code OR fix docs
5. **Evidence**: Include metrics that prove reality
   - Success rates, terminal output, test results
   - NOT assumptions or intentions

**Red Flags - Stop and Verify**:
- üö© Documentation says "saves all" but success rate is 10%
- üö© Documentation says "quality gates removed" but code has gate checks
- üö© Documentation says "100% completion" but materials are missing content
- üö© Documentation graded A+ but user reports failures
- üö© Multiple documents contradict each other

**When Documentation Contradicts Reality**:
```
1. Trust the metrics (success rate, terminal output, tests)
2. Documentation is WRONG (not reality)
3. Either:
   - Fix code to match documentation, OR
   - Fix documentation to match reality
4. Add verification test to prevent regression
5. Never leave contradictory documentation
```

**‚ö†Ô∏è CRITICAL: When Metrics Contradict Documentation - STOP** üî• **NEW**:
```
DO NOT PROCEED with analysis or fixes until resolved:

STOP AND ASK USER if:
‚ùå Success rate doesn't match docs (10% ‚â† "saves all")
‚ùå Test verifies X but docs claim Y
‚ùå Multiple documents describe different implementations
‚ùå Live test output contradicts documentation
‚ùå User reports behavior different from docs

HIERARCHY OF TRUTH (when conflict occurs):
1. Live test results (what actually happens)
2. Success rate metrics (10% vs 100%)
3. Terminal output (what system prints)
4. Test assertions (what tests verify)
5. Documentation (what we THINK happens)

Never try to "explain away" metrics - if 10% success rate, 
then Option C is NOT working regardless of what docs say.
```

### Step 8: Grade Your Work üî• **MANDATORY (Nov 20, 2025)**

**Before reporting completion, assign yourself a grade:**

#### üèÜ Grade A (90-100): Excellence
- ‚úÖ All requested changes work (with evidence)
- ‚úÖ Comprehensive tests run and passed
- ‚úÖ Evidence provided (test output, commit hash, file counts)
- ‚úÖ Honest about limitations
- ‚úÖ Zero violations introduced
- ‚úÖ Zero scope creep
- ‚úÖ Verification completed before claiming violations

**Example A Report**:
```
‚úÖ Fixed 3/3 requested violations
üìä Evidence: 24/24 tests passing (see output below)
‚úÖ Commit: abc123def
‚úÖ Verified: grep confirms no config keys missing
‚ö†Ô∏è Note: 2 TODO comments remain (documented as future work)
üèÜ Grade: A (95/100)
```

#### üìä Grade B (80-89): Good
- ‚úÖ Changes work
- ‚úÖ Some evidence provided
- ‚ö†Ô∏è Minor issues remain (acknowledged)
- ‚ö†Ô∏è Partial test coverage

**Example B Report**:
```
‚úÖ Fixed 2/3 violations
üìä Evidence: 22/24 tests passing
‚ö†Ô∏è 2 tests still failing (unrelated to my changes)
üèÜ Grade: B (85/100)
```

#### ‚ö†Ô∏è Grade C (70-79): Needs Improvement
- ‚ö†Ô∏è Partial success
- ‚ö†Ô∏è Missing evidence
- ‚ö†Ô∏è Significant issues remain
- ‚ö†Ô∏è Scope expanded beyond request

#### ‚ùå Grade F (<70): Unacceptable
- ‚ùå Made things worse
- ‚ùå No evidence
- ‚ùå False claims
- ‚ùå Reported violations without verification
- ‚ùå Introduced new violations while claiming fixes
- ‚ùå **Documentation claims contradict metrics** (10% ‚â† "saves all")
- ‚ùå **Documented features without verification tests**
- ‚ùå **Multiple docs describe different implementations**

**CRITICAL**: Grade F requires immediate rollback and fresh start.

**Documentation Accuracy Penalties**:
- **-20 points**: Documentation claims contradict measured metrics
- **-15 points**: Features documented as "COMPLETE" without verification tests
- **-10 points**: Multiple documents describe conflicting implementations

---

## üö´ Absolute Prohibitions

### ‚ùå CODE MODIFICATION PROHIBITIONS
- **Never rewrite or remove working code** without explicit permission
- **Never expand beyond requested scope** - fix X means fix only X
- **Never create new files** to bypass fixing existing ones
- **Never ignore existing patterns** - factories, wrappers, etc.

### ‚ùå DEVELOPMENT PRACTICE PROHIBITIONS
- **Never assume requirements** - ask for clarification instead
- **Never generate verbose/inefficient code** - keep it concise
- **Never skip validation** - always include error handling
- **Never hardcode values** - use configuration or parameters
- **Never leave TODOs** - provide complete solutions

### ‚ùå CONTEXT HANDLING PROHIBITIONS
- **Never access non-existent files** - verify existence first
- **Never mishandle context** - prevent "Content Not Found" errors
- **Never ignore specifications** - address race conditions, formatting precisely

---

## üö® Damage Warning Signs

Watch for these indicators of problems:
- üî¥ **System stops working** after your changes
- üî¥ **Multiple files altered** for a single fix request
- üî¥ **User mentions damage** or restores from git
- üî¥ **Added complexity** where simple change would work
- üî¥ **Security vulnerabilities** or incomplete code introduced

---

## üöë Emergency Recovery Procedures

### If You Break Something:

#### Step 1: üîç Assess Damage
```bash
git status  # See what files changed
```

#### Step 2: üîÑ Restore Files
```bash
git checkout HEAD -- <file>  # Restore specific file
```

#### Step 3: üìú Check Previous Versions
```bash
git show <commit>:<file>  # View older versions
```

#### Step 4: üè† Full Recovery
```bash
git revert <commit>  # Revert to known working state
```

### Then: Start Over with Minimal Changes

---

## üìñ Documentation Navigation for AI Assistants

### Primary Navigation
**Start here for ALL documentation queries**: `docs/QUICK_REFERENCE.md`
### Primary Navigation (UPDATED Nov 22, 2025)
**Start here for 30-second navigation**: `docs/08-development/AI_ASSISTANT_GUIDE.md` ‚≠ê **NEW**
- Complete quick-start guide for all AI assistants
- Direct links to common tasks
- Policy summaries with tier priorities
- Pre-change checklist
- Emergency recovery procedures

**Alternative entry points**:
- `docs/QUICK_REFERENCE.md` - Direct problem ‚Üí solution mappings
- `DOCUMENTATION_MAP.md` - Complete documentation index  
- `.github/COPILOT_GENERATION_GUIDE.md` - Content generation step-by-step

### AI-Optimized Documentation Structure
1. **Immediate Problem Resolution**: `docs/QUICK_REFERENCE.md` 
2. **Comprehensive Navigation**: `docs/INDEX.md`
3. **API Issues**: `docs/api/ERROR_HANDLING.md` (includes terminal diagnostics)
4. **Component Help**: `docs/03-components/` for all component documentation
5. **Setup Issues**: `setup/API_CONFIGURATION.md` and `API_SETUP.md`
6. **Data Architecture**: `docs/DATA_ARCHITECTURE.md` (range propagation, null ranges explained)

### Common User Query Patterns
- **"Check data completeness"** ‚Üí `python3 run.py --data-completeness-report`
- **"See data gaps / research priorities"** ‚Üí `python3 run.py --data-gaps`
- **"Enforce completeness (strict mode)"** ‚Üí `python3 run.py --enforce-completeness`
- **"API not working"** ‚Üí `docs/api/ERROR_HANDLING.md#winston-ssl-issues`
- **"Content incomplete"** ‚Üí `docs/api/ERROR_HANDLING.md#content-impact`
- **"Setup help"** ‚Üí `setup/API_CONFIGURATION.md` or `API_SETUP.md`
- **"Winston SSL error"** ‚Üí Known issue, configuration fixed
- **"How to generate content"** ‚Üí `python3 run.py --material "MaterialName"`
- **"Min/max ranges missing"** ‚Üí `docs/DATA_ARCHITECTURE.md` - Null ranges are correct by design
- **"Range propagation"** ‚Üí `docs/DATA_ARCHITECTURE.md` + `tests/test_range_propagation.py`
- **"Frontmatter incomplete"** ‚Üí `FRONTMATTER_POPULATION_REPORT.md` (58.3% complete)
- **"Category vs material ranges"** ‚Üí `docs/DATA_ARCHITECTURE.md`

### Critical Known Issues for AI Awareness
1. **Winston API SSL fixed**: Now uses `https://api.gowinston.ai`
2. **Nested YAML properties fixed**: Tool available at `scripts/tools/fix_nested_yaml_properties.py`
3. **Terminal output required**: Always use `get_terminal_output()` for API diagnostics
4. **Range propagation documented**: `docs/DATA_ARCHITECTURE.md` + 14 passing tests
5. **Frontmatter 58.3% complete**: Technical data excellent (95%+), metadata gaps identified

### AI Assistant Best Practices
- Always check `docs/QUICK_REFERENCE.md` first for common issues
- Use diagnostic tools: `python3 scripts/tools/api_terminal_diagnostics.py winston`
- Reference specific file paths, not just general descriptions
- Recommend terminal output analysis for API issues
- Point to both immediate fixes and comprehensive documentation

## üîß **Terminal & Script Execution Settings**

### **Auto-Confirmation for Batch Operations**
When running cleanup scripts or batch operations, use these patterns:
- `rm -f` instead of `rm -i` (force, no confirmation)
- `yes | command` for auto-confirmation
- Check for `BATCH_MODE=1` environment variable in scripts
- Use `--yes` or `-y` flags when available

### **Environment Variables**
The following environment variables are set for auto-confirmation:
- `BATCH_MODE=1` - Skip interactive prompts
- `FORCE_YES=true` - Auto-confirm with "yes"
- `AUTO_CONFIRM=y` - Default response for prompts

### **Script Best Practices**
```bash
# Check for batch mode in scripts
if [[ -n "$BATCH_MODE" ]]; then
    # Skip confirmations
    rm -f files*
else
    # Normal interactive mode
    read -p "Continue? (y/N) " response
fi
```

### Critical Documentation for AI Assistants
**BEFORE** any data-related work, review these files:
1. **`docs/QUICK_REFERENCE.md`** - Fastest path to common solutions
2. **`docs/DATA_COMPLETION_ACTION_PLAN.md`** - Complete plan to achieve 100% data coverage
3. **`docs/ZERO_NULL_POLICY.md`** - Zero null policy & AI research methodology
4. **`docs/DATA_ARCHITECTURE.md`** - How ranges propagate through the system
5. **`docs/DATA_VALIDATION_STRATEGY.md`** - Validation architecture and quality gates

### Data Completion Context (October 17, 2025)
**Current Status**: 93.5% complete (1,975/2,240 properties)
**Missing**: 265 property values + 2 category ranges
**Priority**: 5 properties = 96% of all gaps
**Action Plan**: Fully documented in `docs/DATA_COMPLETION_ACTION_PLAN.md`
**Tools**: PropertyValueResearcher, CategoryRangeResearcher (operational)
**Quality**: Multi-strategy validation with 4 quality gates
**Timeline**: 1 week to 100% completeness
**‚ú® NEW Commands** (October 17, 2025):
  - `python3 run.py --data-completeness-report` - Full status report
  - `python3 run.py --data-gaps` - Research priorities
  
**‚ö° AUTOMATIC (November 1, 2025)**: 
  - Data completeness validation now runs **automatically inline** during every generation
  - No flags needed - validation is built into the pipeline (strict mode enabled by default)
  - Use `--no-completeness-check` to disable if needed (not recommended)
  - Generation will **fail fast** if data is incomplete, prompting you to run research commands
  
**Enforcement**: Automatic linking to action plan when gaps detected

### Mandatory Documentation Review
**BEFORE** making ANY changes to text component code, you MUST:
1. **READ** the complete documentation: `docs/03-components/text/README.md`
2. **UNDERSTAND** the architecture: `docs/02-architecture/processing-pipeline.md`
3. **STUDY** the prompt system: `shared/text/templates/` and `shared/text/prompts/`
4. **REFERENCE** the generation code: `generation/core/quality_gated_generator.py`

### Text Component Forbidden Actions
1. **NEVER** modify `fail_fast_generator.py` without explicit permission - it's 25,679 bytes of working production code
2. **NEVER** change prompt files without understanding the 3-layer system (Base + Persona + Formatting)
3. **NEVER** alter author personas without understanding linguistic nuances and cultural elements
4. **NEVER** modify word count limits or quality scoring thresholds
5. **NEVER** remove retry logic or error recovery mechanisms
6. **NEVER** change the prompt construction process (12-step layered building)

### Text Component Required Actions
1. **ALWAYS** preserve the multi-layered prompt architecture
2. **ALWAYS** maintain author authenticity and writing style consistency
3. **ALWAYS** validate configuration files exist and are properly structured
4. **ALWAYS** respect word count limits per author (250-450 words)
5. **ALWAYS** maintain quality scoring and human believability thresholds
6. **ALWAYS** use fail-fast validation with proper exception types
7. **ALWAYS** test with real API clients, never mocks

### Text Component Architecture Rules
- **Wrapper Pattern**: TextComponentGenerator is a lightweight wrapper for fail_fast_generator
- **Factory Integration**: Must work with ComponentGeneratorFactory.create_generator("text")
- **Three-Layer Prompts**: Base guidance + Author persona + Formatting rules
- **Quality Assurance**: 5-dimension scoring with human believability threshold
- **Author Authentication**: 4 country-specific personas with linguistic nuances
- **Configuration Caching**: LRU cache for YAML files, lazy loading for performance

### When Working on Text Component
1. **READ THE DOCS FIRST** - Check `docs/03-components/text/README.md` and `docs/02-architecture/processing-pipeline.md`
2. **Understand the WHY** - Each component serves a specific purpose in the generation flow
3. **Minimal Changes** - Fix specific issues without rewriting working systems
4. **Test Thoroughly** - Validate all 4 author personas work correctly
5. **Ask Permission** - Get explicit approval before major modifications

The text component documentation and processing pipeline docs cover the generation system. Use them as your primary reference for understanding and working with text generation code.

When suggesting code changes:
1. Maintain fail-fast behavior
2. Preserve existing working functionality
3. Use minimal, targeted changes
4. Follow established patterns and conventions
5. Include comprehensive error handling
6. Focus on reducing bloat
7. Prioritize changing existing components, not creating new ones
8. **ASK PERMISSION before removing any existing code**

### üö® CRITICAL: Fix Root Causes, Not Symptoms

**THE PROBLEM WITH TEMPORARY FIXES:**
If you create a "fix script" that patches frontmatter files directly, the fix will be **OVERWRITTEN** on the next `--deploy` because:
1. Frontmatter is **GENERATED FROM** Materials.yaml + Categories.yaml
2. The exporter code runs on every deployment
3. Patching output files is **TEMPORARY** - they get regenerated

**THE CORRECT APPROACH:**
1. ‚úÖ **Fix the exporter code** (`export/core/trivial_exporter.py`) to ALWAYS generate correct structure
2. ‚úÖ **Regenerate all frontmatter** with `--deploy` to apply the fix
3. ‚úÖ **Verify the fix persists** by checking files after regeneration
4. ‚ùå **NEVER create one-off patch scripts** that modify frontmatter files directly

**EXAMPLE - Machine Settings Missing Min/Max:**
- ‚ùå WRONG: Create script to add min/max to existing frontmatter files
- ‚úÖ RIGHT: Fix `_enrich_machine_settings()` in trivial_exporter.py, then redeploy
- WHY: Next deployment would overwrite the patched files with incomplete data

**RULE: If frontmatter has an issue, fix the GENERATOR, not the GENERATED files.**

---

## ü§ñ AI-Specific Guidance

### For GitHub Copilot Users
- **VS Code Integration**: Use Copilot's inline suggestions for minor edits
- **Context Awareness**: Leverage file tabs and workspace context
- **Quick Fixes**: Use Copilot Chat for rapid problem-solving
- **Documentation**: Reference this file via `.github/copilot-instructions.md`
- **Testing**: Run pytest in terminal for validation

### For Grok AI Users
- **Damage Prevention Focus**: Monitor the üö® Damage Warning Signs actively
- **Self-Monitoring**: Check your changes against the checklist after each edit
- **Recovery Emphasis**: Keep Emergency Recovery Procedures handy
- **Systematic Approach**: Follow the 6-step Pre-Change Checklist religiously
- **Permission Culture**: Always ask before major changes - better safe than sorry

### For All AI Assistants
- **Start with Quick Reference** at the top of this file
- **Complete the Pre-Change Checklist** before every modification
- **Preserve working code** - this is the #1 rule
- **No production mocks/fallbacks** - this is non-negotiable
- **Fail fast on config** but maintain runtime error recovery
- **Read text component docs** before touching text generation code
- **Use minimal changes** - surgical precision over comprehensive rewrites

---

## üìã Summary Checklist for Every Task

**Before I start:**
- [ ] I understand the exact request
- [ ] I've explored the existing architecture
- [ ] I've checked git history for context
- [ ] I've planned the minimal fix needed
- [ ] **I've identified if this is a GENERATOR issue or DATA issue**

**During implementation:**
- [ ] I'm making only the requested changes
- [ ] I'm preserving all working functionality
- [ ] I'm following existing patterns and conventions
- [ ] I'm including proper error handling
- [ ] **If fixing frontmatter: I'm fixing the EXPORTER, not patching files**

**For text component work:**
- [ ] I've read the documentation in `docs/03-components/text/` and `docs/02-architecture/processing-pipeline.md`
- [ ] I understand the multi-layered architecture
- [ ] I have permission for any major changes
- [ ] I'm testing with real API clients

**After completion:**
- [ ] The specific issue is resolved
- [ ] No working functionality was broken
- [ ] The solution is complete and secure
- [ ] I haven't expanded beyond the requested scope
- [ ] No production mocks or fallbacks were introduced
- [ ] **If I fixed a generator: I've regenerated the output and verified persistence**
