# ğŸš¨ GROK QUICK REFERENCE - READ BEFORE EVERY CODE CHANGE

## ğŸ“˜ PRIMARY DOCUMENTATION
**â¡ï¸ MAIN REFERENCE: [.github/copilot-instructions.md](/.github/copilot-instructions.md)**

This file provides **quick lookup** for tier priorities and decision trees.  
For complete rules, policies, and guardrails, see **copilot-instructions.md**.

---

## âš¡ CRITICAL RULES (Full details in copilot-instructions.md)

### ğŸ”´ TIER 1: SYSTEM-BREAKING (Will cause failures)
1. âŒ **NO mocks/fallbacks in production code** (tests OK) - [Rule 2](#)
2. âŒ **NO hardcoded values/defaults** (use config/dynamic calc) - [Rule 3](#)
3. âŒ **NO rewriting working code** (minimal surgical fixes only) - [Rule 1](#)

### ğŸŸ¡ TIER 2: QUALITY-CRITICAL (Will cause bugs)
4. âŒ **NO expanding scope** (fix X means fix ONLY X) - [Rule 5](#)
5. âŒ **NO skipping validation** (must test before claiming success) - [Step 6](#)
6. âœ… **ALWAYS fail-fast on config** (throw exceptions, no silent degradation) - [Rule 3](#)
7. âœ… **ALWAYS preserve runtime recovery** (API retries are correct) - [ADR-002](#)
8. âœ… **ALWAYS log to terminal** (all generation attempts, scores, feedback) - See Terminal Output Policy below

### ğŸŸ¢ TIER 3: EVIDENCE & HONESTY (Will lose trust)
9. âœ… **ALWAYS provide evidence** (test output, counts, commits) - [Protocol](#)
10. âœ… **ALWAYS be honest** (acknowledge what remains broken) - [Protocol](#)
11. âœ… **ASK before major changes** (get permission for improvements) - [Rule 1](#)
12. âœ… **VERIFY before claiming violations** (check config files, confirm pattern exists) ğŸ”¥ **NEW (Nov 20, 2025)**

**ğŸš¨ CRITICAL: Claiming violations without verification is a TIER 3 violation itself.**

---

## ğŸ“‹ TERMINAL OUTPUT LOGGING POLICY ğŸ”¥ **NEW (Nov 18, 2025)**

**ALL generation operations MUST stream comprehensive output to terminal in real-time.**

**Logging Requirements**:
1. **Stream to stdout/stderr ONLY** - No log files created or saved
2. **Real-time output** - User sees progress as it happens
3. **Attempt Progress** - Every retry with attempt number (e.g., "Attempt 2/5")
4. **Quality Checks** - Winston score, Realism score, thresholds, pass/fail
5. **Feedback Application** - Parameter adjustments between attempts
6. **Learning Activity** - Prompt optimization, pattern learning
7. **Final Report** - Complete generation report (see GROK_INSTRUCTIONS.md)

**Example Streaming Output**:
```
Attempt 2/5
ğŸŒ¡ï¸  Temperature: 0.750 â†’ 0.825
ğŸ“‰ Frequency penalty: 0.20 â†’ 0.30
Winston Score: 98.6% human âœ… PASS
Realism Score: 5.0/10 (threshold: 5.5) âŒ FAIL
âœ… [REALISM FEEDBACK] Parameter adjustments calculated
```

**Implementation**:
- Use `print()` for terminal output (not `logger.info()` to files)
- All subprocess calls inherit stdout/stderr (no capture)
- Batch tests stream directly (no tee to log files)

**Purpose**: User visibility, debugging, transparency, verification
**Anti-Patterns**: 
- âŒ Silent failures, hidden retries, opaque processing
- âŒ Log files in /tmp/ or elsewhere
- âŒ Capturing output without displaying it

---

## ğŸš¦ DECISION TREES

### Decision: Should I use a default value?
```
Is this a config/setup issue?
â”œâ”€ YES â†’ âŒ FAIL FAST (throw ConfigurationError)
â””â”€ NO â†’ Is this a runtime/transient issue?
    â”œâ”€ YES â†’ âœ… RETRY with backoff (API timeout, network error)
    â””â”€ NO â†’ Is this a quality check iteration?
        â”œâ”€ YES â†’ âœ… ITERATE (adjust parameters based on feedback)
        â””â”€ NO â†’ âŒ FAIL FAST (programming error)
```

### Decision: Should I rewrite this code?
```
Does the code work correctly?
â”œâ”€ YES â†’ âŒ NO REWRITE (integrate around it, add method, minimal fix)
â””â”€ NO â†’ Is it a small targeted fix?
    â”œâ”€ YES â†’ âœ… FIX ONLY broken part
    â””â”€ NO â†’ âš ï¸ ASK PERMISSION (explain why rewrite needed)
```

### Decision: Is this a mock/fallback violation?
```
What type of code is this?
â”œâ”€ PRODUCTION CODE â†’ âŒ NO mocks/fallbacks/defaults (ZERO TOLERANCE)
â””â”€ TEST CODE â†’ âœ… Mocks/fallbacks ALLOWED (testing infrastructure)
```

### Decision: Should I report a violation? ğŸ”¥ **NEW (Nov 20, 2025)**
```
Have I found suspect code pattern (.get with default)?
â”œâ”€ NO â†’ Not a violation
â””â”€ YES â†’ Does the key exist in config?
    â”œâ”€ YES â†’ âœ… NOT A VIOLATION (valid fallback for optional key)
    â””â”€ NO â†’ Is this key supposed to be required?
        â”œâ”€ UNCLEAR â†’ âš ï¸ ASK USER (don't assume violation)
        â””â”€ YES â†’ âœ… VIOLATION (should fail-fast if missing)

CRITICAL: grep -r "key_name" config.yaml BEFORE reporting violation
Example: .get('intensity', 5) â†’ Check if 'intensity' exists in config
```

### Decision: Should I claim "fixed" or "working"?
```
Have I run comprehensive tests?
â”œâ”€ NO â†’ âŒ DON'T CLAIM (validate first)
â””â”€ YES â†’ Do I have evidence?
    â”œâ”€ NO â†’ âŒ DON'T CLAIM (capture output)
    â””â”€ YES â†’ Have I counted total vs passing?
        â”œâ”€ NO â†’ âŒ DON'T CLAIM (be specific)
        â””â”€ YES â†’ Have I acknowledged what remains broken?
            â”œâ”€ NO â†’ âŒ INCOMPLETE (mention limitations)
            â””â”€ YES â†’ âœ… CAN CLAIM with evidence

Example: "Fixed 11/11 requested failures (23/23 passing). 
Note: 10 other test files still have import errors."
```

---

## ğŸ“‹ MANDATORY PRE-CHANGE CHECKLIST

**â±ï¸ PHASE 1-3: Complete research BEFORE coding (7-11 minutes prevents hours of fixing)**

**Phase 1: Verification (2-3 minutes)**
- [ ] Read request word-by-word (what EXACTLY is requested?)
- [ ] Check for assumptions (am I assuming anything not stated?)
- [ ] Verify file paths (do all referenced files exist?)
- [ ] Check config keys (do claimed violations exist in config files?)
- [ ] Search for existing solutions (does DynamicConfig/helper already solve this?)

**Phase 2: Research (3-5 minutes)**
- [ ] grep_search for patterns (how does system currently handle this?)
- [ ] Read relevant code (understand current implementation)
- [ ] Check git history (was this tried before? why changed?)
- [ ] Review docs/ (is there policy documentation?)
- [ ] Check ADRs (is there an architectural decision?)

**Phase 3: Planning (2-3 minutes)**
- [ ] Identify exact change needed (one sentence description)
- [ ] Confirm minimal scope (am I fixing ONLY what was requested?)
- [ ] Check for side effects (what else might this affect?)
- [ ] Plan validation (how will I prove it works?)
- [ ] Get permission if major (ask before removing/rewriting code)

**Before committing:**
- [ ] Ran comprehensive tests (not just 1 example)
- [ ] Captured evidence (test output, file counts)
- [ ] Verified no regressions (nothing else broke)
- [ ] Checked for violations (no mocks in production, no hardcoded values)
- [ ] Honest assessment (acknowledged limitations)
- [ ] Graded my work (A/B/C/F with evidence)

---

## ğŸš¨ WHEN UNCERTAIN - STOP SIGNALS

**IF YOU'RE NOT SURE:**
1. ğŸ›‘ **STOP coding immediately**
2. ğŸ“– **READ** relevant copilot-instructions.md section
3. ğŸ¤” **CHECK** decision tree above
4. â“ **ASK** user for clarification
5. ğŸ“š **REFERENCE** specific ADR or doc section

**ğŸš¨ STOP SIGNALS - When to ASK instead of CODE:**
- â“ Not 100% certain about the requirement
- â“ Can't find the config key/file/pattern being referenced
- â“ Fixing this requires changing more than 3 files
- â“ About to add hardcoded value without finding dynamic config first
- â“ Request conflicts with existing architecture
- â“ Tests failing and don't understand why

**NEVER assume or guess when uncertain.**

---

## ğŸ† SELF-ASSESSMENT (Grade Before Reporting)

**Grade A (90-100): Excellence**
- âœ… All changes work + comprehensive evidence + honest about limitations
- âœ… Zero violations introduced + zero scope creep
- âœ… Verification completed before claiming violations

**Grade B (80-89): Good**
- âœ… Changes work + some evidence + minor issues remain

**Grade C (70-79): Needs Improvement**
- âš ï¸ Partial success + missing evidence + significant issues

**Grade F (<70): Unacceptable** 
- âŒ Made things worse + no evidence + false claims
- âŒ Reported violations without verification

**Example A-grade report:**
```
âœ… Fixed 3/3 requested violations
ğŸ“Š Evidence: 24/24 tests passing (see output below)
âœ… Commit: abc123def
âœ… Verified: grep confirms no config keys missing
âš ï¸ Note: 2 TODO comments remain (documented as future work)
ğŸ† Grade: A (95/100)
```

---

## ğŸ“‚ QUICK FILE REFERENCE

- **Full instructions**: `GROK_INSTRUCTIONS.md`
- **Fail-fast vs retry**: `docs/decisions/ADR-002-fail-fast-vs-runtime-recovery.md`
- **Content instructions**: `docs/prompts/CONTENT_INSTRUCTION_POLICY.md`
- **Data storage**: `docs/data/DATA_STORAGE_POLICY.md`
- **System interactions**: `docs/SYSTEM_INTERACTIONS.md`
- **Quick answers**: `docs/QUICK_REFERENCE.md`

---

**ğŸ¯ REMEMBER: Validate before claiming success. Provide evidence with every claim. Be honest about limitations.**
